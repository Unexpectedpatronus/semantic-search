"""–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞"""

import os
from pathlib import Path

from semantic_search.core.doc2vec_trainer import Doc2VecTrainer
from semantic_search.core.search_engine import SemanticSearchEngine
from semantic_search.evaluation.baselines import (
    Doc2VecSearchAdapter,
    OpenAISearchBaseline,
)
from semantic_search.evaluation.comparison import QueryTestCase, SearchComparison


def create_demo_test_cases():
    """–°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤"""

    test_cases = [
        QueryTestCase(
            query="–ü–æ–Ω—è—Ç–∏–µ –≥–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–µ",
            relevant_docs={
                "–¢—Ä–∞–Ω—Å–ª–∏–≥–≤–∏–∑–º/-1.pdf",
                "SALMAN RUSHDIE/Language is assumed by many to be a stable medium of communication.docx",
                "–ì–ª–æ–±–∞–ª–∏–∑–∞—Ü–∏—è –∏ –≥–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è/glokalizatsiya-i-vozvrat-etnichnosti-v-vek-globalizatsii.pdf",
            },
            relevance_scores={
                "–¢—Ä–∞–Ω—Å–ª–∏–≥–≤–∏–∑–º/-1.pdf": 3,
                "SALMAN RUSHDIE/Language is assumed by many to be a stable medium of communication.docx": 3,
                "–ì–ª–æ–±–∞–ª–∏–∑–∞—Ü–∏—è –∏ –≥–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è/glokalizatsiya-i-vozvrat-etnichnosti-v-vek-globalizatsii.pdf": 2,
                "–õ–∏–Ω–≥–≤–æ–∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å/Linguistic_Creativity_Cognitive_And_Communicative_.pdf": 1,
            },
            description="–ü–æ–Ω—è—Ç–∏–µ –≥–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–µ",
        ),
        QueryTestCase(
            query="–¢—Ä–∞–Ω—Å–ª–∏–Ω–≥–≤–∏–∑–º –∏ —Ç—Ä–∞–Ω—Å–ª–∏–Ω–≥–≤–∞–ª—å–Ω–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞",
            relevant_docs={
                "–¢—Ä–∞–Ω—Å–ª–∏–≥–≤–∏–∑–º/-1.pdf",
                "SALMAN RUSHDIE/rushdie-1997-notes-on-writing-and-the-nation.pdf",
                "glocal_strategy.pdf",
            },
            relevance_scores={
                "–¢—Ä–∞–Ω—Å–ª–∏–≥–≤–∏–∑–º/-1.pdf": 3,
                "SALMAN RUSHDIE/rushdie-1997-notes-on-writing-and-the-nation.pdf": 3,
                "SALMAN RUSHDIE/Hybridization_Heteroglossia_and_the_engl.doc": 2,
            },
            description="–¢—Ä–∞–Ω—Å–ª–∏–Ω–≥–≤–∏–∑–º –∏ —Ç—Ä–∞–Ω—Å–ª–∏–Ω–≥–≤–∞–ª—å–Ω–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞",
        ),
        QueryTestCase(
            query="–ì–µ—Ç–µ—Ä–æ–ª–æ–≥–∏—è –∏ –¥–∏–∞–ª–æ–≥–∏–∑–º",
            relevant_docs={"cultural_marketing.pdf", "cross_cultural_comm.pdf"},
            relevance_scores={
                "–¢—Ä–∞–Ω—Å–ª–∏–≥–≤–∏–∑–º/-1.pdf": 3,
                "SALMAN RUSHDIE/12.docx": 3,
                " –ë–∞—Ö—Ç–∏–Ω/Zebroski-MikhailBakhtinQuestion-1992.pdf": 1,
            },
            description="–ì–µ—Ç–µ—Ä–æ–ª–æ–≥–∏—è –∏ –¥–∏–∞–ª–æ–≥–∏–∑–º",
        ),
    ]

    return test_cases


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    print("=" * 80)
    print("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –°–†–ê–í–ù–ï–ù–ò–Ø DOC2VEC –ò OPENAI EMBEDDINGS")
    print("=" * 80)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ API –∫–ª—é—á–∞
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("\n‚ùå –û–®–ò–ë–ö–ê: OpenAI API key –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è OPENAI_API_KEY")
        print("–ù–∞–ø—Ä–∏–º–µ—Ä: set OPENAI_API_KEY=sk-...")
        return

    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Doc2Vec
    print("\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Doc2Vec...")
    model_name = "doc2vec_model"  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –∏–º—è –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏

    trainer = Doc2VecTrainer()
    model = trainer.load_model(model_name)

    if not model:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å '{model_name}'")
        print("–°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å –∫–æ–º–∞–Ω–¥–æ–π:")
        print("poetry run semantic-search-cli train -d /path/to/documents")
        return

    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {len(model.dv)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞
    search_engine = SemanticSearchEngine(model, trainer.corpus_info)

    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤
    print("\nüß™ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤...")
    test_cases = create_demo_test_cases()
    print(f"   –°–æ–∑–¥–∞–Ω–æ {len(test_cases)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")

    # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    comparison = SearchComparison(test_cases)

    # –°–æ–∑–¥–∞–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ –º–µ—Ç–æ–¥–æ–≤
    print("\nüîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞...")

    # Doc2Vec –∞–¥–∞–ø—Ç–µ—Ä
    doc2vec_adapter = Doc2VecSearchAdapter(search_engine, trainer.corpus_info)
    print("‚úÖ Doc2Vec –∞–¥–∞–ø—Ç–µ—Ä –≥–æ—Ç–æ–≤")

    # OpenAI baseline
    try:
        openai_baseline = OpenAISearchBaseline(api_key=api_key)
        print("‚úÖ OpenAI baseline –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ OpenAI: {e}")
        return

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è OpenAI
    print("\nüìö –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è OpenAI...")
    print("   (–î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 20 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")

    # –ë–µ—Ä–µ–º –ø–æ–¥–≤—ã–±–æ—Ä–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    demo_documents = []
    for i, (tokens, doc_id, metadata) in enumerate(trainer.corpus_info[:20]):
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Ç–æ–∫–µ–Ω–æ–≤
        text = " ".join(tokens[:300])  # –ü–µ—Ä–≤—ã–µ 300 —Ç–æ–∫–µ–Ω–æ–≤
        demo_documents.append((doc_id, text, metadata))

    try:
        openai_baseline.index(demo_documents)
        print(f"‚úÖ –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ {len(demo_documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")
        return

    # –û—Ü–µ–Ω–∫–∞ –º–µ—Ç–æ–¥–æ–≤
    print("\nüìä –û–¶–ï–ù–ö–ê –ú–ï–¢–û–î–û–í")
    print("-" * 80)

    # Doc2Vec
    print("\n1Ô∏è‚É£ –û—Ü–µ–Ω–∫–∞ Doc2Vec...")
    doc2vec_results = comparison.evaluate_method(
        doc2vec_adapter, top_k=10, verbose=True
    )

    # OpenAI
    print("\n2Ô∏è‚É£ –û—Ü–µ–Ω–∫–∞ OpenAI embeddings...")
    openai_results = comparison.evaluate_method(openai_baseline, top_k=10, verbose=True)

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\nüìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–†–ê–í–ù–ï–ù–ò–Ø")
    print("=" * 80)

    # –°–æ–∑–¥–∞–µ–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
    df_comparison = comparison.compare_methods(
        [doc2vec_adapter, openai_baseline], save_results=True
    )

    print("\n–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞:")
    print(df_comparison.to_string(index=False))

    # –û—Å–Ω–æ–≤–Ω—ã–µ –≤—ã–≤–æ–¥—ã
    doc2vec_map = doc2vec_results["aggregated"]["MAP"]
    openai_map = openai_results["aggregated"]["MAP"]

    print("\nüéØ –û–°–ù–û–í–ù–´–ï –í–´–í–û–î–´:")
    print("-" * 80)

    if doc2vec_map > openai_map:
        improvement = ((doc2vec_map - openai_map) / openai_map) * 100
        print(f"‚úÖ Doc2Vec –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç OpenAI –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –ø–æ–∏—Å–∫–∞ –Ω–∞ {improvement:.1f}%")
    else:
        improvement = ((openai_map - doc2vec_map) / doc2vec_map) * 100
        print(f"‚ùå OpenAI –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Doc2Vec –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –ø–æ–∏—Å–∫–∞ –Ω–∞ {improvement:.1f}%")

    # –°–∫–æ—Ä–æ—Å—Ç—å
    doc2vec_time = doc2vec_results["aggregated"]["avg_query_time"]
    openai_time = openai_results["aggregated"]["avg_query_time"]
    speed_ratio = openai_time / doc2vec_time

    print(f"\n‚úÖ Doc2Vec —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ {speed_ratio:.1f} —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ OpenAI")
    print(f"   Doc2Vec: {doc2vec_time:.3f}—Å –Ω–∞ –∑–∞–ø—Ä–æ—Å")
    print(f"   OpenAI:  {openai_time:.3f}—Å –Ω–∞ –∑–∞–ø—Ä–æ—Å")

    # –≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
    print("\nüí∞ –≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å:")
    yearly_cost = 1000 * 50 / 1000 * 0.0001 * 365  # –ü—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞—Å—á–µ—Ç
    print(f"   –ü—Ä–∏ 1000 –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –¥–µ–Ω—å —ç–∫–æ–Ω–æ–º–∏—è —Å–æ—Å—Ç–∞–≤–∏—Ç ~${yearly_cost:.0f} –≤ –≥–æ–¥")

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
    print("\nüìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤...")
    try:
        comparison.plot_comparison(save_plots=True)
        print("‚úÖ –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ data/evaluation_results/plots/")
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏: {e}")

    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    print("\nüìÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞...")
    report_path = Path("data/evaluation_results/comparison_report.txt")
    report = comparison.generate_report(report_path)

    print("\n‚úÖ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: data/evaluation_results/")
    print("\n" + "=" * 80)


if __name__ == "__main__":
    main()
