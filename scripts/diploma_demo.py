"""
–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –¥–∏–ø–ª–æ–º–Ω–æ–π —Ä–∞–±–æ—Ç—ã
–°—Ä–∞–≤–Ω–µ–Ω–∏–µ Doc2Vec —Å –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –ø–æ–∏—Å–∫–∞ (TF-IDF –∏ BM25)
"""

import sys
from pathlib import Path
from typing import Any, Dict, List

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from loguru import logger

from semantic_search.core.doc2vec_trainer import Doc2VecTrainer
from semantic_search.core.search_engine import SemanticSearchEngine
from semantic_search.evaluation.baselines import (
    BM25SearchBaseline,
    Doc2VecSearchAdapter,
    TFIDFSearchBaseline,
)
from semantic_search.evaluation.comparison import QueryTestCase, SearchComparison


def create_test_cases_for_diploma() -> List[QueryTestCase]:
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –≤ –¥–∏–ø–ª–æ–º–Ω–æ–π —Ä–∞–±–æ—Ç–µ"""

    # –°–æ–∑–¥–∞–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ —Å–ª—É—á–∞–∏, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–∏–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
    test_cases = [
        # 1. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å (—Å–∏–Ω–æ–Ω–∏–º—ã)
        QueryTestCase(
            query="–ì–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ª–æ–∫–∞–ª—å–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –±—Ä–µ–Ω–¥–æ–≤",
            relevant_docs={
                "–ì–ª–æ–±–∞–ª–∏–∑–∞—Ü–∏—è –∏ –≥–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è/glokalizatsiya-i-vozvrat-etnichnosti-v-vek-globalizatsii.pdf",
                "glocal_strategy.pdf",
                "cultural_marketing.pdf",
            },
            relevance_scores={
                "–ì–ª–æ–±–∞–ª–∏–∑–∞—Ü–∏—è –∏ –≥–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è/glokalizatsiya-i-vozvrat-etnichnosti-v-vek-globalizatsii.pdf": 3,
                "glocal_strategy.pdf": 3,
                "cultural_marketing.pdf": 2,
            },
            description="–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏",
        ),
        # 2. –ö–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        QueryTestCase(
            query="–Ø–∑—ã–∫–æ–≤–∞—è –≥–∏–±—Ä–∏–¥–Ω–æ—Å—Ç—å –≤ –º—É–ª—å—Ç–∏–∫—É–ª—å—Ç—É—Ä–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–µ",
            relevant_docs={
                "SALMAN RUSHDIE/Hybridization_Heteroglossia_and_the_engl.doc",
                "–¢—Ä–∞–Ω—Å–ª–∏–≥–≤–∏–∑–º/-1.pdf",
                "SALMAN RUSHDIE/Language is assumed by many to be a stable medium of communication.docx",
            },
            relevance_scores={
                "SALMAN RUSHDIE/Hybridization_Heteroglossia_and_the_engl.doc": 3,
                "–¢—Ä–∞–Ω—Å–ª–∏–≥–≤–∏–∑–º/-1.pdf": 3,
                "SALMAN RUSHDIE/Language is assumed by many to be a stable medium of communication.docx": 2,
            },
            description="–ö–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å",
        ),
        # 3. –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        QueryTestCase(
            query="–î–∏–∞–ª–æ–≥–∏–∑–º –ë–∞—Ö—Ç–∏–Ω–∞ –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–µ",
            relevant_docs={
                " –ë–∞—Ö—Ç–∏–Ω/Zebroski-MikhailBakhtinQuestion-1992.pdf",
                "SALMAN RUSHDIE/12.docx",
                "–¢—Ä–∞–Ω—Å–ª–∏–≥–≤–∏–∑–º/-1.pdf",
            },
            relevance_scores={
                " –ë–∞—Ö—Ç–∏–Ω/Zebroski-MikhailBakhtinQuestion-1992.pdf": 3,
                "SALMAN RUSHDIE/12.docx": 2,
                "–¢—Ä–∞–Ω—Å–ª–∏–≥–≤–∏–∑–º/-1.pdf": 2,
            },
            description="–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∑–∞–≤–∏—Å–∏–º—ã–π –∑–∞–ø—Ä–æ—Å",
        ),
        # 4. –ú–µ–∂–¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞—Ä–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        QueryTestCase(
            query="–ö—É–ª—å—Ç—É—Ä–Ω–∞—è –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å –≤ —ç–ø–æ—Ö—É –≥–ª–æ–±–∞–ª–∏–∑–∞—Ü–∏–∏",
            relevant_docs={
                "–ì–ª–æ–±–∞–ª–∏–∑–∞—Ü–∏—è –∏ –≥–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è/glokalizatsiya-i-vozvrat-etnichnosti-v-vek-globalizatsii.pdf",
                "SALMAN RUSHDIE/rushdie-1997-notes-on-writing-and-the-nation.pdf",
                "cultural_marketing.pdf",
            },
            relevance_scores={
                "–ì–ª–æ–±–∞–ª–∏–∑–∞—Ü–∏—è –∏ –≥–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è/glokalizatsiya-i-vozvrat-etnichnosti-v-vek-globalizatsii.pdf": 3,
                "SALMAN RUSHDIE/rushdie-1997-notes-on-writing-and-the-nation.pdf": 3,
                "cultural_marketing.pdf": 2,
            },
            description="–ú–µ–∂–¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞—Ä–Ω—ã–π –∑–∞–ø—Ä–æ—Å",
        ),
        # 5. –ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        QueryTestCase(
            query="–õ–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ —è–∑—ã–∫–æ–≤—ã–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏",
            relevant_docs={
                "–õ–∏–Ω–≥–≤–æ–∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å/Linguistic_Creativity_Cognitive_And_Communicative_.pdf",
                "–¢—Ä–∞–Ω—Å–ª–∏–≥–≤–∏–∑–º/-1.pdf",
                "SALMAN RUSHDIE/Language is assumed by many to be a stable medium of communication.docx",
            },
            relevance_scores={
                "–õ–∏–Ω–≥–≤–æ–∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å/Linguistic_Creativity_Cognitive_And_Communicative_.pdf": 3,
                "–¢—Ä–∞–Ω—Å–ª–∏–≥–≤–∏–∑–º/-1.pdf": 2,
                "SALMAN RUSHDIE/Language is assumed by many to be a stable medium of communication.docx": 2,
            },
            description="–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å",
        ),
    ]

    return test_cases


def prepare_documents_for_baselines(
    corpus_info: List, max_docs: int = 100
) -> List[tuple]:
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤ baseline –º–µ—Ç–æ–¥–∞—Ö

    Args:
        corpus_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ—Ä–ø—É—Å–µ –∏–∑ Doc2Vec
        max_docs: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏

    Returns:
        –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (doc_id, text, metadata)
    """
    documents = []

    for i, (tokens, doc_id, metadata) in enumerate(corpus_info[:max_docs]):
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Ç–æ–∫–µ–Ω–æ–≤
        # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
        text = " ".join(tokens[:1000])  # –£–≤–µ–ª–∏—á–∏–ª–∏ –¥–æ 1000 —Ç–æ–∫–µ–Ω–æ–≤
        documents.append((doc_id, text, metadata))

    return documents


def create_comparison_plots(results: Dict[str, Any], output_dir: Path):
    """–°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤ –¥–ª—è –¥–∏–ø–ª–æ–º–Ω–æ–π —Ä–∞–±–æ—Ç—ã"""

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å—Ç–∏–ª—å –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–π
    plt.style.use("seaborn-v0_8-paper")
    sns.set_palette("husl")

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —à—Ä–∏—Ñ—Ç–æ–≤ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
    plt.rcParams["font.family"] = "DejaVu Sans"
    plt.rcParams["font.size"] = 12

    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
    plots_dir = output_dir / "diploma_plots"
    plots_dir.mkdir(exist_ok=True, parents=True)

    # 1. –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

    methods = list(results.keys())
    metrics_data = {
        "MAP": [results[m]["aggregated"]["MAP"] for m in methods],
        "MRR": [results[m]["aggregated"]["MRR"] for m in methods],
        "P@10": [results[m]["aggregated"]["avg_precision@10"] for m in methods],
        "R@10": [results[m]["aggregated"]["avg_recall@10"] for m in methods],
    }

    # –ì—Ä–∞—Ñ–∏–∫ 1: –°—Ç–æ–ª–±—á–∞—Ç–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ –º–µ—Ç—Ä–∏–∫
    df_metrics = pd.DataFrame(metrics_data, index=methods)
    df_metrics.plot(kind="bar", ax=ax1, width=0.8)
    ax1.set_title("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞", fontsize=16, fontweight="bold")
    ax1.set_xlabel("–ú–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞", fontsize=14)
    ax1.set_ylabel("–ó–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏", fontsize=14)
    ax1.set_ylim(0, 1.0)
    ax1.legend(title="–ú–µ—Ç—Ä–∏–∫–∏", bbox_to_anchor=(1.05, 1), loc="upper left")
    ax1.grid(True, alpha=0.3)

    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for container in ax1.containers:
        ax1.bar_label(container, fmt="%.3f", padding=3)

    # –ì—Ä–∞—Ñ–∏–∫ 2: –†–∞–¥–∞—Ä–Ω–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞
    from math import pi

    categories = ["MAP", "MRR", "P@10", "R@10"]
    angles = [n / len(categories) * 2 * pi for n in range(len(categories))]
    angles += angles[:1]

    ax2 = plt.subplot(122, projection="polar")

    for method in methods:
        values = [
            results[method]["aggregated"]["MAP"],
            results[method]["aggregated"]["MRR"],
            results[method]["aggregated"]["avg_precision@10"],
            results[method]["aggregated"]["avg_recall@10"],
        ]
        values += values[:1]

        ax2.plot(angles, values, "o-", linewidth=2, label=method, markersize=8)
        ax2.fill(angles, values, alpha=0.15)

    ax2.set_xticks(angles[:-1])
    ax2.set_xticklabels(categories, fontsize=12)
    ax2.set_ylim(0, 1.0)
    ax2.set_title("–†–∞–¥–∞—Ä–Ω–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ –º–µ—Ç—Ä–∏–∫", fontsize=16, fontweight="bold", pad=20)
    ax2.legend(loc="upper right", bbox_to_anchor=(1.3, 1.1))
    ax2.grid(True)

    plt.tight_layout()
    plt.savefig(
        plots_dir / "quality_metrics_comparison.png", dpi=300, bbox_inches="tight"
    )
    plt.close()

    # 2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

    # –í—Ä–µ–º—è –ø–æ–∏—Å–∫–∞
    query_times = [results[m]["aggregated"]["avg_query_time"] for m in methods]
    colors = ["#1f77b4", "#ff7f0e", "#2ca02c"]

    bars1 = ax1.bar(methods, query_times, color=colors, alpha=0.8, edgecolor="black")
    ax1.set_title("–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞", fontsize=16, fontweight="bold")
    ax1.set_xlabel("–ú–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞", fontsize=14)
    ax1.set_ylabel("–í—Ä–µ–º—è (—Å–µ–∫—É–Ω–¥—ã)", fontsize=14)
    ax1.grid(True, alpha=0.3, axis="y")

    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for bar, time in zip(bars1, query_times):
        height = bar.get_height()
        ax1.text(
            bar.get_x() + bar.get_width() / 2.0,
            height,
            f"{time:.4f}s",
            ha="center",
            va="bottom",
            fontsize=12,
        )

    # –í—Ä–µ–º—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
    index_times = [results[m]["method_stats"]["index_time"] for m in methods]

    bars2 = ax2.bar(methods, index_times, color=colors, alpha=0.8, edgecolor="black")
    ax2.set_title("–í—Ä–µ–º—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∫–æ—Ä–ø—É—Å–∞", fontsize=16, fontweight="bold")
    ax2.set_xlabel("–ú–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞", fontsize=14)
    ax2.set_ylabel("–í—Ä–µ–º—è (—Å–µ–∫—É–Ω–¥—ã)", fontsize=14)
    ax2.grid(True, alpha=0.3, axis="y")

    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for bar, time in zip(bars2, index_times):
        height = bar.get_height()
        ax2.text(
            bar.get_x() + bar.get_width() / 2.0,
            height,
            f"{time:.2f}s",
            ha="center",
            va="bottom",
            fontsize=12,
        )

    plt.tight_layout()
    plt.savefig(plots_dir / "performance_comparison.png", dpi=300, bbox_inches="tight")
    plt.close()

    # 3. –î–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º –∑–∞–ø—Ä–æ—Å–æ–≤
    fig, ax = plt.subplots(figsize=(12, 8))

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ —Ç–∏–ø–∞–º –∑–∞–ø—Ä–æ—Å–æ–≤
    query_types = []
    for method in methods:
        for detail in results[method]["detailed"]:
            query_types.append(
                {
                    "–ú–µ—Ç–æ–¥": method,
                    "–¢–∏–ø –∑–∞–ø—Ä–æ—Å–∞": detail["query"][:30] + "...",
                    "AP": detail["average_precision"],
                }
            )

    df_queries = pd.DataFrame(query_types)
    pivot_df = df_queries.pivot(index="–¢–∏–ø –∑–∞–ø—Ä–æ—Å–∞", columns="–ú–µ—Ç–æ–¥", values="AP")

    # –°–æ–∑–¥–∞–µ–º —Ç–µ–ø–ª–æ–≤—É—é –∫–∞—Ä—Ç—É
    sns.heatmap(
        pivot_df,
        annot=True,
        fmt=".3f",
        cmap="YlOrRd",
        cbar_kws={"label": "Average Precision"},
        vmin=0,
        vmax=1,
    )
    ax.set_title(
        "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–æ–≤ –ø–æ —Ç–∏–ø–∞–º –∑–∞–ø—Ä–æ—Å–æ–≤", fontsize=16, fontweight="bold"
    )
    ax.set_xlabel("–ú–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞", fontsize=14)
    ax.set_ylabel("–¢–∏–ø –∑–∞–ø—Ä–æ—Å–∞", fontsize=14)

    plt.tight_layout()
    plt.savefig(plots_dir / "query_types_heatmap.png", dpi=300, bbox_inches="tight")
    plt.close()

    # 4. –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
    fig, ax = plt.subplots(figsize=(10, 8))

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    efficiency_data = []

    for method in methods:
        # –ö–∞—á–µ—Å—Ç–≤–æ (MAP) - —á–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ
        quality = results[method]["aggregated"]["MAP"]

        # –°–∫–æ—Ä–æ—Å—Ç—å (–æ–±—Ä–∞—Ç–Ω–∞—è –≤–µ–ª–∏—á–∏–Ω–∞ –≤—Ä–µ–º–µ–Ω–∏) - —á–µ–º –±—ã—Å—Ç—Ä–µ–µ, —Ç–µ–º –ª—É—á—à–µ
        speed = 1 / (results[method]["aggregated"]["avg_query_time"] + 0.001)

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É [0, 1]
        max_speed = max(
            [1 / (results[m]["aggregated"]["avg_query_time"] + 0.001) for m in methods]
        )
        speed_normalized = speed / max_speed

        efficiency_data.append(
            {
                "–ú–µ—Ç–æ–¥": method,
                "–ö–∞—á–µ—Å—Ç–≤–æ (MAP)": quality,
                "–°–∫–æ—Ä–æ—Å—Ç—å (–Ω–æ—Ä–º.)": speed_normalized,
                "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å": (quality + speed_normalized) / 2,  # –°—Ä–µ–¥–Ω–µ–µ
            }
        )

    df_efficiency = pd.DataFrame(efficiency_data)

    # Scatter plot
    for i, row in df_efficiency.iterrows():
        ax.scatter(
            row["–°–∫–æ—Ä–æ—Å—Ç—å (–Ω–æ—Ä–º.)"],
            row["–ö–∞—á–µ—Å—Ç–≤–æ (MAP)"],
            s=500,
            alpha=0.7,
            label=row["–ú–µ—Ç–æ–¥"],
        )
        ax.annotate(
            row["–ú–µ—Ç–æ–¥"],
            (row["–°–∫–æ—Ä–æ—Å—Ç—å (–Ω–æ—Ä–º.)"], row["–ö–∞—á–µ—Å—Ç–≤–æ (MAP)"]),
            xytext=(5, 5),
            textcoords="offset points",
            fontsize=12,
        )

    ax.set_xlabel("–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å", fontsize=14)
    ax.set_ylabel("–ö–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ (MAP)", fontsize=14)
    ax.set_title(
        "–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞", fontsize=16, fontweight="bold"
    )
    ax.grid(True, alpha=0.3)
    ax.set_xlim(-0.1, 1.1)
    ax.set_ylim(-0.1, 1.1)

    # –î–æ–±–∞–≤–ª—è–µ–º –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω—É—é –ª–∏–Ω–∏—é
    ax.plot([0, 1], [0, 1], "k--", alpha=0.3, label="–ë–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–æ/—Å–∫–æ—Ä–æ—Å—Ç—å")

    ax.legend()

    plt.tight_layout()
    plt.savefig(plots_dir / "efficiency_scatter.png", dpi=300, bbox_inches="tight")
    plt.close()

    logger.info(f"–ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {plots_dir}")


def generate_diploma_report(results: Dict[str, Any], output_path: Path) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –¥–ª—è –¥–∏–ø–ª–æ–º–Ω–æ–π —Ä–∞–±–æ—Ç—ã"""

    report = []
    report.append("=" * 80)
    report.append("–û–¢–ß–ï–¢ –û –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–û–ú –ê–ù–ê–õ–ò–ó–ï –ú–ï–¢–û–î–û–í –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–û–ì–û –ü–û–ò–°–ö–ê")
    report.append("–¥–ª—è –¥–∏–ø–ª–æ–º–Ω–æ–π —Ä–∞–±–æ—Ç—ã")
    report.append("=" * 80)
    report.append("")

    # –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è
    report.append("–ê–ù–ù–û–¢–ê–¶–ò–Ø")
    report.append("-" * 40)
    report.append("–í –¥–∞–Ω–Ω–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –ø—Ä–æ–≤–µ–¥–µ–Ω —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ—Ö –º–µ—Ç–æ–¥–æ–≤")
    report.append("–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:")
    report.append("1. Doc2Vec - –º–µ—Ç–æ–¥ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π")
    report.append("2. TF-IDF - –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –º–µ—Ç–æ–¥")
    report.append("3. BM25 - —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è TF-IDF, —Å—Ç–∞–Ω–¥–∞—Ä—Ç –≤ –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö")
    report.append("")

    # –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è
    report.append("–ú–ï–¢–û–î–û–õ–û–ì–ò–Ø –û–¶–ï–ù–ö–ò")
    report.append("-" * 40)
    report.append("–î–ª—è –æ—Ü–µ–Ω–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Å–ª–µ–¥—É—é—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏:")
    report.append("- MAP (Mean Average Precision) - —Å—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ –≤—Å–µ–º –∑–∞–ø—Ä–æ—Å–∞–º")
    report.append("- MRR (Mean Reciprocal Rank) - —Å—Ä–µ–¥–Ω–∏–π –æ–±—Ä–∞—Ç–Ω—ã–π —Ä–∞–Ω–≥")
    report.append("- Precision@k - —Ç–æ—á–Ω–æ—Å—Ç—å –≤ —Ç–æ–ø-k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö")
    report.append("- Recall@k - –ø–æ–ª–Ω–æ—Ç–∞ –≤ —Ç–æ–ø-k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö")
    report.append("")

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    report.append("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê")
    report.append("-" * 40)

    # –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    report.append("\n–¢–∞–±–ª–∏—Ü–∞ 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞")
    report.append("-" * 60)
    report.append(f"{'–ú–µ—Ç–æ–¥':<15} {'MAP':<10} {'MRR':<10} {'P@10':<10} {'R@10':<10}")
    report.append("-" * 60)

    for method in results:
        agg = results[method]["aggregated"]
        report.append(
            f"{method:<15} "
            f"{agg['MAP']:<10.3f} "
            f"{agg['MRR']:<10.3f} "
            f"{agg['avg_precision@10']:<10.3f} "
            f"{agg['avg_recall@10']:<10.3f}"
        )

    report.append("")

    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    report.append("\n–¢–∞–±–ª–∏—Ü–∞ 2. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
    report.append("-" * 60)
    report.append(
        f"{'–ú–µ—Ç–æ–¥':<15} {'–í—Ä–µ–º—è –∑–∞–ø—Ä–æ—Å–∞ (—Å)':<20} {'–í—Ä–µ–º—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (—Å)':<20}"
    )
    report.append("-" * 60)

    for method in results:
        query_time = results[method]["aggregated"]["avg_query_time"]
        index_time = results[method]["method_stats"]["index_time"]
        report.append(f"{method:<15} {query_time:<20.4f} {index_time:<20.2f}")

    report.append("")

    # –í—ã–≤–æ–¥—ã
    report.append("–û–°–ù–û–í–ù–´–ï –í–´–í–û–î–´")
    report.append("-" * 40)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à–∏–π –º–µ—Ç–æ–¥ –ø–æ MAP
    best_method = max(results.items(), key=lambda x: x[1]["aggregated"]["MAP"])[0]
    best_map = max(results.items(), key=lambda x: x[1]["aggregated"]["MAP"])[1][
        "aggregated"
    ]["MAP"]

    report.append("1. –ö–ê–ß–ï–°–¢–í–û –ü–û–ò–°–ö–ê:")
    report.append(
        f"   –ù–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª –º–µ—Ç–æ–¥ {best_method} —Å MAP = {best_map:.3f}"
    )

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Doc2Vec —Å –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏
    doc2vec_map = results["Doc2Vec"]["aggregated"]["MAP"]
    tfidf_map = results["TF-IDF"]["aggregated"]["MAP"]
    bm25_map = results["BM25"]["aggregated"]["MAP"]

    improvement_tfidf = ((doc2vec_map - tfidf_map) / tfidf_map) * 100
    improvement_bm25 = ((doc2vec_map - bm25_map) / bm25_map) * 100

    report.append(f"\n   Doc2Vec –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç TF-IDF –Ω–∞ {improvement_tfidf:.1f}%")
    report.append(f"   Doc2Vec –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç BM25 –Ω–∞ {improvement_bm25:.1f}%")

    # –ê–Ω–∞–ª–∏–∑ —Å–∫–æ—Ä–æ—Å—Ç–∏
    report.append("\n2. –°–ö–û–†–û–°–¢–¨ –†–ê–ë–û–¢–´:")
    fastest_method = min(
        results.items(), key=lambda x: x[1]["aggregated"]["avg_query_time"]
    )[0]

    doc2vec_time = results["Doc2Vec"]["aggregated"]["avg_query_time"]
    tfidf_time = results["TF-IDF"]["aggregated"]["avg_query_time"]
    bm25_time = results["BM25"]["aggregated"]["avg_query_time"]

    report.append(f"   –°–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π –º–µ—Ç–æ–¥: {fastest_method}")
    report.append(f"   Doc2Vec –º–µ–¥–ª–µ–Ω–Ω–µ–µ TF-IDF –≤ {doc2vec_time / tfidf_time:.1f} —Ä–∞–∑")
    report.append(f"   Doc2Vec –º–µ–¥–ª–µ–Ω–Ω–µ–µ BM25 –≤ {doc2vec_time / bm25_time:.1f} —Ä–∞–∑")

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    report.append("\n3. –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
    report.append("   - –î–ª—è –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞,")
    report.append("     —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Doc2Vec")
    report.append("   - –î–ª—è –≤—ã—Å–æ–∫–æ–Ω–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —Å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏ –∫ —Å–∫–æ—Ä–æ—Å—Ç–∏")
    report.append("     –º–æ–∂–Ω–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å BM25 –∫–∞–∫ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ")
    report.append("   - TF-IDF –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞–∏—Ö—É–¥—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è")
    report.append("     –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º")

    # –ó–∞–∫–ª—é—á–µ–Ω–∏–µ
    report.append("\n–ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï")
    report.append("-" * 40)
    report.append("–ü—Ä–æ–≤–µ–¥–µ–Ω–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ—Ç–æ–¥–∞ Doc2Vec")
    report.append("–Ω–∞–¥ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–º–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –ø–æ–∏—Å–∫–∞. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞")
    report.append("–±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã, Doc2Vec –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç")
    report.append("–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ –∑–∞ —Å—á–µ—Ç —É—á–µ—Ç–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö")
    report.append("—Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏.")

    report.append("\n" + "=" * 80)

    report_text = "\n".join(report)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(report_text)

    return report_text


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –≤ –¥–∏–ø–ª–æ–º–Ω–æ–π —Ä–∞–±–æ—Ç–µ"""
    print("=" * 80)
    print("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –î–õ–Ø –î–ò–ü–õ–û–ú–ù–û–ô –†–ê–ë–û–¢–´")
    print("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ Doc2Vec —Å –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏")
    print("=" * 80)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Doc2Vec
    print("\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ Doc2Vec...")
    model_name = "doc2vec_model"

    trainer = Doc2VecTrainer()
    model = trainer.load_model(model_name)

    if not model:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å '{model_name}'")
        print("–°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å –∫–æ–º–∞–Ω–¥–æ–π:")
        print("poetry run semantic-search-cli train -d /path/to/documents")
        return

    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {len(model.dv)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤: {model.vector_size}")
    print(f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(model.wv.key_to_index)} —Å–ª–æ–≤")

    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞
    search_engine = SemanticSearchEngine(model, trainer.corpus_info)

    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤
    print("\nüß™ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤...")
    test_cases = create_test_cases_for_diploma()
    print(f"   –°–æ–∑–¥–∞–Ω–æ {len(test_cases)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è baseline –º–µ—Ç–æ–¥–æ–≤
    print("\nüìö –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...")
    documents = prepare_documents_for_baselines(
        trainer.corpus_info, max_docs=len(trainer.corpus_info)
    )
    print(f"   –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    comparison = SearchComparison(test_cases)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞
    print("\nüîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞...")

    # 1. Doc2Vec
    doc2vec_adapter = Doc2VecSearchAdapter(search_engine, trainer.corpus_info)
    print("‚úÖ Doc2Vec –∞–¥–∞–ø—Ç–µ—Ä –≥–æ—Ç–æ–≤")

    # 2. TF-IDF
    tfidf_baseline = TFIDFSearchBaseline()
    print("‚úÖ TF-IDF baseline –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    # 3. BM25
    bm25_baseline = BM25SearchBaseline()
    print("‚úÖ BM25 baseline –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–ª—è baseline –º–µ—Ç–æ–¥–æ–≤
    print("\nüìä –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è baseline –º–µ—Ç–æ–¥–æ–≤...")

    print("   –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è TF-IDF...")
    tfidf_baseline.index(documents)

    print("   –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è BM25...")
    bm25_baseline.index(documents)

    # –û—Ü–µ–Ω–∫–∞ –º–µ—Ç–æ–¥–æ–≤
    print("\nüìà –û–¶–ï–ù–ö–ê –ú–ï–¢–û–î–û–í")
    print("-" * 80)

    all_results = {}

    # 1. Doc2Vec
    print("\n1Ô∏è‚É£ –û—Ü–µ–Ω–∫–∞ Doc2Vec...")
    doc2vec_results = comparison.evaluate_method(
        doc2vec_adapter, top_k=10, verbose=True
    )
    all_results["Doc2Vec"] = doc2vec_results

    # 2. TF-IDF
    print("\n2Ô∏è‚É£ –û—Ü–µ–Ω–∫–∞ TF-IDF...")
    tfidf_results = comparison.evaluate_method(tfidf_baseline, top_k=10, verbose=True)
    all_results["TF-IDF"] = tfidf_results

    # 3. BM25
    print("\n3Ô∏è‚É£ –û—Ü–µ–Ω–∫–∞ BM25...")
    bm25_results = comparison.evaluate_method(bm25_baseline, top_k=10, verbose=True)
    all_results["BM25"] = bm25_results

    # –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    print("\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–†–ê–í–ù–ï–ù–ò–Ø")
    print("=" * 80)

    # –°–æ–∑–¥–∞–µ–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
    df_comparison = comparison.compare_methods(
        [doc2vec_adapter, tfidf_baseline, bm25_baseline], save_results=True
    )

    print("\n–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –º–µ—Ç—Ä–∏–∫:")
    print(df_comparison.to_string(index=False))

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –¥–ª—è –¥–∏–ø–ª–æ–º–Ω–æ–π —Ä–∞–±–æ—Ç—ã
    print("\nüìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤...")
    output_dir = Path("data/evaluation_results")
    output_dir.mkdir(exist_ok=True, parents=True)

    create_comparison_plots(all_results, output_dir)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
    print("\nüìÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –¥–ª—è –¥–∏–ø–ª–æ–º–Ω–æ–π —Ä–∞–±–æ—Ç—ã...")
    report_path = output_dir / "diploma_comparison_report.txt"
    report = generate_diploma_report(all_results, report_path)

    # –í—ã–≤–æ–¥–∏–º –∫–ª—é—á–µ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\nüéØ –ö–õ–Æ–ß–ï–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –î–õ–Ø –î–ò–ü–õ–û–ú–ù–û–ô –†–ê–ë–û–¢–´:")
    print("=" * 80)

    doc2vec_map = all_results["Doc2Vec"]["aggregated"]["MAP"]
    tfidf_map = all_results["TF-IDF"]["aggregated"]["MAP"]
    bm25_map = all_results["BM25"]["aggregated"]["MAP"]

    print("\nüìä –ö–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ (MAP):")
    print(f"   Doc2Vec: {doc2vec_map:.3f} ‚≠ê")
    print(f"   BM25:    {bm25_map:.3f}")
    print(f"   TF-IDF:  {tfidf_map:.3f}")

    improvement_tfidf = ((doc2vec_map - tfidf_map) / tfidf_map) * 100
    improvement_bm25 = ((doc2vec_map - bm25_map) / bm25_map) * 100

    print("\nüìà –ü—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ Doc2Vec:")
    print(f"   –ù–∞–¥ TF-IDF: +{improvement_tfidf:.1f}%")
    print(f"   –ù–∞–¥ BM25:   +{improvement_bm25:.1f}%")

    print("\n‚úÖ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: data/evaluation_results/")
    print("   üìä diploma_plots/ - –≥—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏")
    print("   üìÑ diploma_comparison_report.txt - –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç")
    print("   üìà comparison_results.csv - —Ç–∞–±–ª–∏—Ü–∞ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏")

    print("\n" + "=" * 80)
    print("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
    print("=" * 80)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(0)
    except Exception as e:
        logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
        print(f"\n‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        print("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")
        sys.exit(1)
