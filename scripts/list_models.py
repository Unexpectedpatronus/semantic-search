#!/usr/bin/env python
"""–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""

import json
import sys
from datetime import datetime
from pathlib import Path

from semantic_search.config import MODELS_DIR
from semantic_search.core.doc2vec_trainer import Doc2VecTrainer

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src –≤ sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))


def format_file_size(size_bytes):
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞"""
    for unit in ["B", "KB", "MB", "GB"]:
        if size_bytes < 1024.0:
            return f"{size_bytes:.1f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.1f} TB"


def list_models():
    """–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    print("üìö –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ Doc2Vec")
    print("=" * 80)

    if not MODELS_DIR.exists():
        print("‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –º–æ–¥–µ–ª–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
        return

    model_files = list(MODELS_DIR.glob("*.model"))

    if not model_files:
        print("‚ÑπÔ∏è  –ú–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        print(f"   –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –º–æ–¥–µ–ª–µ–π: {MODELS_DIR}")
        print("\nüí° –°–æ–∑–¥–∞–π—Ç–µ –º–æ–¥–µ–ª—å –∫–æ–º–∞–Ω–¥–æ–π:")
        print("   poetry run semantic-search-cli train -d /path/to/documents")
        return

    print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –º–æ–¥–µ–ª–µ–π: {MODELS_DIR}")
    print(f"üî¢ –ù–∞–π–¥–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(model_files)}\n")

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–µ)
    model_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)

    for i, model_file in enumerate(model_files, 1):
        model_name = model_file.stem
        file_size = model_file.stat().st_size
        modified_time = datetime.fromtimestamp(model_file.stat().st_mtime)

        print(f"{i}. üß† {model_name}")
        print(f"   üìè –†–∞–∑–º–µ—Ä: {format_file_size(file_size)}")
        print(f"   üìÖ –ò–∑–º–µ–Ω–µ–Ω: {modified_time.strftime('%Y-%m-%d %H:%M:%S')}")

        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata_file = MODELS_DIR / f"{model_name}_metadata.json"
        if metadata_file.exists():
            try:
                with open(metadata_file, "r", encoding="utf-8") as f:
                    metadata = json.load(f)

                if "corpus_size" in metadata:
                    print(f"   üìÑ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {metadata['corpus_size']}")
                if "vector_size" in metadata:
                    print(f"   üéØ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {metadata['vector_size']}")
                if "epochs" in metadata:
                    print(f"   üîÑ –≠–ø–æ—Ö: {metadata['epochs']}")
                if "training_time_formatted" in metadata:
                    print(
                        f"   ‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {metadata['training_time_formatted']}"
                    )
                if "documents_base_path" in metadata:
                    base_path = Path(metadata["documents_base_path"])
                    print(f"   üìÇ –ë–∞–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {base_path.name}")

            except Exception as e:
                print(f"   ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {e}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ corpus_info
        corpus_info_file = MODELS_DIR / f"{model_name}_corpus_info.pkl"
        if corpus_info_file.exists():
            corpus_size = format_file_size(corpus_info_file.stat().st_size)
            print(f"   üíæ Corpus info: {corpus_size}")

        print()  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏


def show_model_details(model_name: str):
    """–ü–æ–∫–∞–∑–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏"""
    print(f"\nüìã –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏: {model_name}")
    print("=" * 80)

    trainer = Doc2VecTrainer()
    model = trainer.load_model(model_name)

    if model is None:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å '{model_name}'")
        return

    info = trainer.get_model_info()

    print(f"‚úÖ –°—Ç–∞—Ç—É—Å: {info['status']}")
    print(f"üìè –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤: {info['vector_size']}")
    print(f"üìö –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {info['vocabulary_size']:,} —Å–ª–æ–≤")
    print(f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –º–æ–¥–µ–ª–∏: {info['documents_count']}")
    print(f"üîç –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞: {info['window']}")
    print(f"üìä –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞: {info['min_count']}")
    print(f"üîÑ –≠–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è: {info['epochs']}")

    if info["dm"] == 1:
        print("üîß –†–µ–∂–∏–º: Distributed Memory (DM)")
    else:
        print("üîß –†–µ–∂–∏–º: Distributed Bag of Words (DBOW)")

    print(f"‚ûñ Negative sampling: {info['negative']}")
    print(f"üå≥ Hierarchical Softmax: {'–î–∞' if info['hs'] == 1 else '–ù–µ—Ç'}")
    print(f"üìâ Sample threshold: {info['sample']}")

    if "training_time_formatted" in info:
        print(f"\n‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {info['training_time_formatted']}")
    if "training_date" in info:
        print(f"üìÖ –î–∞—Ç–∞ –æ–±—É—á–µ–Ω–∏—è: {info['training_date']}")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    if trainer.corpus_info:
        print("\nüìë –ü—Ä–∏–º–µ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –º–æ–¥–µ–ª–∏:")
        for i, (tokens, doc_id, metadata) in enumerate(trainer.corpus_info[:5]):
            print(f"   {i + 1}. {doc_id}")
            if "tokens_count" in metadata:
                print(f"      –¢–æ–∫–µ–Ω–æ–≤: {metadata['tokens_count']}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse

    parser = argparse.ArgumentParser(description="–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—è–º–∏ Doc2Vec")
    parser.add_argument(
        "--details", "-d", help="–ü–æ–∫–∞–∑–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏"
    )

    args = parser.parse_args()

    if args.details:
        show_model_details(args.details)
    else:
        list_models()


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  –û–ø–µ—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞.")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        sys.exit(1)
