"""Ð¢Ð¾Ñ‡ÐºÐ° Ð²Ñ…Ð¾Ð´Ð° Ð² Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ"""

import sys
from pathlib import Path

import click
from loguru import logger

from semantic_search.config import GUI_CONFIG
from semantic_search.core.doc2vec_trainer import Doc2VecTrainer
from semantic_search.core.document_processor import DocumentProcessor
from semantic_search.core.search_engine import SemanticSearchEngine
from semantic_search.utils.logging_config import setup_logging


def main():
    """Ð“Ð»Ð°Ð²Ð½Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ"""

    setup_logging()
    logger.info("Ð—Ð°Ð¿ÑƒÑÐº Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ Semantic Document Search")

    try:
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾ÑÑ‚ÑŒ PyQt6
        from PyQt6.QtCore import Qt
        from PyQt6.QtWidgets import QApplication

        # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ Qt
        app = QApplication(sys.argv)
        app.setApplicationName(GUI_CONFIG["window_title"])
        app.setOrganizationName("Semantic Search")

        # Ð£ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ ÑÑ‚Ð¸Ð»ÑŒ
        app.setStyle("Fusion")  # Ð¡Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¹ ÑÑ‚Ð¸Ð»ÑŒ

        # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¸ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð³Ð»Ð°Ð²Ð½Ð¾Ð³Ð¾ Ð¾ÐºÐ½Ð°
        from semantic_search.gui.main_window import MainWindow

        main_window = MainWindow()
        main_window.show()

        logger.info("Ð“Ð»Ð°Ð²Ð½Ð¾Ðµ Ð¾ÐºÐ½Ð¾ ÑÐ¾Ð·Ð´Ð°Ð½Ð¾ Ð¸ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¾")

        # Ð—Ð°Ð¿ÑƒÑÐº Ñ†Ð¸ÐºÐ»Ð° ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹
        exit_code = app.exec()
        logger.info(f"ÐŸÑ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾ Ñ ÐºÐ¾Ð´Ð¾Ð¼: {exit_code}")
        sys.exit(exit_code)

    except ImportError as e:
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð°: {e}")
        print("Ð£Ð±ÐµÐ´Ð¸Ñ‚ÐµÑÑŒ, Ñ‡Ñ‚Ð¾ Ð²ÑÐµ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ñ‹:")
        print("poetry install")
        sys.exit(1)

    except Exception as e:
        logger.error(f"ÐšÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


@click.group()
def cli():
    """Semantic Document Search CLI"""
    setup_logging()


@cli.command()
@click.option("--documents", "-d", required=True, help="ÐŸÑƒÑ‚ÑŒ Ðº Ð¿Ð°Ð¿ÐºÐµ Ñ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ð¼Ð¸")
@click.option("--model", "-m", default="doc2vec_model", help="Ð˜Ð¼Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸")
@click.option("--vector-size", default=150, help="Ð Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð¾Ð²")
@click.option("--epochs", default=40, help="ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÐ¿Ð¾Ñ… Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ")
def train(documents, model, vector_size, epochs):
    """ÐžÐ±ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ"""

    documents_path = Path(documents)
    if not documents_path.exists():
        logger.error(f"ÐŸÐ°Ð¿ÐºÐ° Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°: {documents_path}")
        return

    logger.info("Ð ÐµÐ¶Ð¸Ð¼ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸")

    # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²
    processor = DocumentProcessor()
    corpus = processor.prepare_corpus_for_doc2vec(documents_path)

    if not corpus:
        logger.error("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²Ð¸Ñ‚ÑŒ ÐºÐ¾Ñ€Ð¿ÑƒÑ")
        return

    # ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸
    trainer = Doc2VecTrainer()
    trained_model = trainer.train_model(corpus, vector_size=vector_size, epochs=epochs)

    if trained_model:
        trainer.save_model(trained_model, model)
        logger.info("âœ… ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð° Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð°")

        # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ
        stats = processor.get_processing_statistics(documents_path)
        click.echo("\nðŸ“Š Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ:")
        click.echo(f"ðŸ“ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²: {stats['processed_files']}")
        click.echo(f"ðŸ”¤ ÐžÐ±Ñ‰ÐµÐµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²: {stats['total_tokens']}")
        click.echo(f"ðŸ“„ Ð¡Ñ€ÐµÐ´Ð½ÐµÐµ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð½Ð° Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚: {stats['avg_tokens_per_doc']:.1f}")
        click.echo(f"ðŸ“‹ Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ñ‹ Ñ„Ð°Ð¹Ð»Ð¾Ð²: {stats['extensions_count']}")
    else:
        logger.error("âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸")


@cli.command()
@click.option("--documents", "-d", required=True, help="ÐŸÑƒÑ‚ÑŒ Ðº Ð¿Ð°Ð¿ÐºÐµ Ñ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ð¼Ð¸")
@click.option("--query", "-q", required=True, help="ÐŸÐ¾Ð¸ÑÐºÐ¾Ð²Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ")
@click.option("--model", "-m", default="doc2vec_model", help="Ð˜Ð¼Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸")
@click.option("--top-k", default=10, help="ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²")
def search(documents, query, model, top_k):
    """ÐŸÐ¾Ð¸ÑÐº Ð¿Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ð¼"""
    logger.info(f"Ð ÐµÐ¶Ð¸Ð¼ Ð¿Ð¾Ð¸ÑÐºÐ°: {query}")

    # Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸
    trainer = Doc2VecTrainer()
    loaded_model = trainer.load_model(model)

    if loaded_model is None:
        logger.error("âŒ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ")
        click.echo("Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¾Ð±ÑƒÑ‡Ð¸Ñ‚Ðµ Ð¼Ð¾Ð´ÐµÐ»ÑŒ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð¾Ð¹: train")
        return

    # ÐŸÐ¾Ð¸ÑÐº
    search_engine = SemanticSearchEngine(loaded_model, trainer.corpus_info)
    results = search_engine.search(query, top_k=top_k)

    if results:
        click.echo(f"\nðŸ” Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾Ð¸ÑÐºÐ° Ð´Ð»Ñ '{query}':")
        click.echo("=" * 50)
        for i, result in enumerate(results, 1):
            click.echo(f"{i}. {result.doc_id}")
            click.echo(f"   ðŸ“Š Ð¡Ñ…Ð¾Ð´ÑÑ‚Ð²Ð¾: {result.similarity:.3f}")
            if result.metadata:
                tokens_count = result.metadata.get("tokens_count", "N/A")
                file_size = result.metadata.get("file_size", 0)
                click.echo(f"   ðŸ“ Ð¢Ð¾ÐºÐµÐ½Ð¾Ð²: {tokens_count}, Ð Ð°Ð·Ð¼ÐµÑ€: {file_size} Ð±Ð°Ð¹Ñ‚")
            click.echo()
    else:
        click.echo(f"âŒ Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ð´Ð»Ñ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° '{query}'")


@cli.command()
@click.option("--documents", "-d", required=True, help="ÐŸÑƒÑ‚ÑŒ Ðº Ð¿Ð°Ð¿ÐºÐµ Ñ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²")
@click.option("--model", "-m", default="doc2vec_model", help="Ð˜Ð¼Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸")
def stats(documents, model):
    """ÐŸÐ¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸ ÐºÐ¾Ñ€Ð¿ÑƒÑÐ°"""
    documents_path = Path(documents)

    # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° ÐºÐ¾Ñ€Ð¿ÑƒÑÐ°
    if documents_path.exists():
        processor = DocumentProcessor()
        corpus_stats = processor.get_processing_statistics(documents_path)

        click.echo("ðŸ“ Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° ÐºÐ¾Ñ€Ð¿ÑƒÑÐ°:")
        click.echo(f"  Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²: {corpus_stats['processed_files']}")
        click.echo(f"  Ð¢Ð¾ÐºÐµÐ½Ð¾Ð²: {corpus_stats['total_tokens']}")
        click.echo(
            f"  Ð¡Ñ€ÐµÐ´Ð½ÐµÐµ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²/Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚: {corpus_stats['avg_tokens_per_doc']:.1f}"
        )
        click.echo(f"  Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ñ‹: {corpus_stats['extensions_count']}")

        if corpus_stats["largest_doc"]:
            click.echo(
                f"  Ð¡Ð°Ð¼Ñ‹Ð¹ Ð±Ð¾Ð»ÑŒÑˆÐ¾Ð¹: {corpus_stats['largest_doc']['path']} ({corpus_stats['largest_doc']['tokens']} Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²)"
            )
        if corpus_stats["smallest_doc"]:
            click.echo(
                f"  Ð¡Ð°Ð¼Ñ‹Ð¹ Ð¼Ð°Ð»ÐµÐ½ÑŒÐºÐ¸Ð¹: {corpus_stats['smallest_doc']['path']} ({corpus_stats['smallest_doc']['tokens']} Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²)"
            )

    # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸
    trainer = Doc2VecTrainer()
    if trainer.load_model(model):
        model_info = trainer.get_model_info()

        click.echo(f"\nðŸ§  Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸ '{model}':")
        click.echo(f"  Ð¡Ñ‚Ð°Ñ‚ÑƒÑ: {model_info['status']}")
        click.echo(f"  Ð Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð¾Ð²: {model_info['vector_size']}")
        click.echo(f"  Ð Ð°Ð·Ð¼ÐµÑ€ ÑÐ»Ð¾Ð²Ð°Ñ€Ñ: {model_info['vocabulary_size']}")
        click.echo(f"  Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð² Ð¼Ð¾Ð´ÐµÐ»Ð¸: {model_info['documents_count']}")
        click.echo(f"  ÐžÐºÐ½Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°: {model_info['window']}")
        click.echo(f"  ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ð°: {model_info['min_count']}")
        click.echo(f"  Ð­Ð¿Ð¾Ñ… Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ: {model_info['epochs']}")
    else:
        click.echo(f"\nâŒ ÐœÐ¾Ð´ÐµÐ»ÑŒ '{model}' Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°")


def cli_mode():
    """Ð—Ð°Ð¿ÑƒÑÐº CLI Ñ€ÐµÐ¶Ð¸Ð¼Ð°"""
    cli()


if __name__ == "__main__":
    if len(sys.argv) > 1:
        cli_mode()
    else:
        main()
