"""–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""

from collections import Counter
from typing import Any, Dict, List

from semantic_search.core.document_processor import ProcessedDocument


def calculate_statistics_from_processed_docs(
    docs_data: List[ProcessedDocument],
) -> Dict[str, Any]:
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–∑ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

    Args:
        docs_data: –°–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π:
        - processed_files: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        - total_tokens: –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
        - avg_tokens_per_doc: —Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç
        - extensions_count: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º
        - largest_doc: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∞–º–æ–º –±–æ–ª—å—à–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ
        - smallest_doc: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∞–º–æ–º –º–∞–ª–µ–Ω—å–∫–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ
        - total_chars: –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤
        - avg_chars_per_doc: —Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç
    """
    if not docs_data:
        return {
            "processed_files": 0,
            "total_tokens": 0,
            "avg_tokens_per_doc": 0.0,
            "extensions_count": {},
            "largest_doc": None,
            "smallest_doc": None,
            "total_chars": 0,
            "avg_chars_per_doc": 0.0,
        }

    # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = {
        "processed_files": len(docs_data),
        "total_tokens": sum(doc.metadata["tokens_count"] for doc in docs_data),
        "total_chars": sum(doc.metadata["text_length"] for doc in docs_data),
        "extensions_count": dict(
            Counter(doc.metadata["extension"] for doc in docs_data)
        ),
    }

    # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    stats["avg_tokens_per_doc"] = stats["total_tokens"] / stats["processed_files"]
    stats["avg_chars_per_doc"] = stats["total_chars"] / stats["processed_files"]

    # –°–∞–º—ã–π –±–æ–ª—å—à–æ–π –∏ –º–∞–ª–µ–Ω—å–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç—ã
    docs_by_tokens = sorted(docs_data, key=lambda x: x.metadata["tokens_count"])
    stats["smallest_doc"] = {
        "path": docs_by_tokens[0].relative_path,
        "tokens": docs_by_tokens[0].metadata["tokens_count"],
        "chars": docs_by_tokens[0].metadata["text_length"],
    }
    stats["largest_doc"] = {
        "path": docs_by_tokens[-1].relative_path,
        "tokens": docs_by_tokens[-1].metadata["tokens_count"],
        "chars": docs_by_tokens[-1].metadata["text_length"],
    }

    return stats


def format_statistics_for_display(stats: Dict[str, Any]) -> str:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞

    Args:
        stats: –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –∏–∑ calculate_statistics_from_processed_docs

    Returns:
        –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
    """
    if stats["processed_files"] == 0:
        return "‚ùå –ù–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"

    lines = [
        "üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ—Ä–ø—É—Å–∞:",
        f"üìÅ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {stats['processed_files']}",
        f"üî§ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {stats['total_tokens']:,}",
        f"üìÑ –°—Ä–µ–¥–Ω–µ–µ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç: {stats['avg_tokens_per_doc']:.1f}",
        f"üìù –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤: {stats['total_chars']:,}",
        f"üìñ –°—Ä–µ–¥–Ω–µ–µ —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç: {stats['avg_chars_per_doc']:.1f}",
        f"üìã –§–æ—Ä–º–∞—Ç—ã —Ñ–∞–π–ª–æ–≤: {stats['extensions_count']}",
    ]

    if stats["largest_doc"]:
        lines.extend(
            [
                "üìà –°–∞–º—ã–π –±–æ–ª—å—à–æ–π –¥–æ–∫—É–º–µ–Ω—Ç:",
                f"   üìÑ {stats['largest_doc']['path']}",
                f"   üî§ {stats['largest_doc']['tokens']} —Ç–æ–∫–µ–Ω–æ–≤, {stats['largest_doc']['chars']} —Å–∏–º–≤–æ–ª–æ–≤",
            ]
        )

    if stats["smallest_doc"]:
        lines.extend(
            [
                "üìâ –°–∞–º—ã–π –º–∞–ª–µ–Ω—å–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç:",
                f"   üìÑ {stats['smallest_doc']['path']}",
                f"   üî§ {stats['smallest_doc']['tokens']} —Ç–æ–∫–µ–Ω–æ–≤, {stats['smallest_doc']['chars']} —Å–∏–º–≤–æ–ª–æ–≤",
            ]
        )

    return "\n".join(lines)


def calculate_model_statistics(model_info: Dict[str, Any]) -> str:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –º–æ–¥–µ–ª–∏ –¥–ª—è –≤—ã–≤–æ–¥–∞

    Args:
        model_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏ –∏–∑ Doc2VecTrainer.get_model_info()

    Returns:
        –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –º–æ–¥–µ–ª–∏
    """
    if model_info.get("status") != "loaded":
        return f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {model_info.get('status', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}"

    lines = [
        "üß† –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–æ–¥–µ–ª–∏:",
        f"‚úÖ –°—Ç–∞—Ç—É—Å: {model_info['status']}",
        f"üìè –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤: {model_info['vector_size']}",
        f"üìö –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {model_info['vocabulary_size']:,} —Å–ª–æ–≤",
        f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –º–æ–¥–µ–ª–∏: {model_info['documents_count']}",
        f"üîç –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {model_info['window']}",
        f"üìä –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤–∞: {model_info['min_count']}",
        f"üîÑ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è: {model_info['epochs']}",
    ]

    return "\n".join(lines)
