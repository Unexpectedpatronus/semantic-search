"""Ð£Ñ‚Ð¸Ð»Ð¸Ñ‚Ñ‹ Ð´Ð»Ñ Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ñ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸"""

from collections import Counter
from typing import Any, Dict, List

from semantic_search.core.document_processor import ProcessedDocument


def calculate_statistics_from_processed_docs(
    docs_data: List[ProcessedDocument],
) -> Dict[str, Any]:
    """
    Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ðµ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸ Ð¸Ð· ÑƒÐ¶Ðµ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²

    Args:
        docs_data: Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²

    Returns:
        Ð¡Ð»Ð¾Ð²Ð°Ñ€ÑŒ ÑÐ¾ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¾Ð¹:
        - processed_files: ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ñ‹Ñ… Ñ„Ð°Ð¹Ð»Ð¾Ð²
        - total_tokens: Ð¾Ð±Ñ‰ÐµÐµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²
        - avg_tokens_per_doc: ÑÑ€ÐµÐ´Ð½ÐµÐµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð½Ð° Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚
        - extensions_count: ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð¿Ð¾ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð¸ÑÐ¼
        - largest_doc: Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð¾ ÑÐ°Ð¼Ð¾Ð¼ Ð±Ð¾Ð»ÑŒÑˆÐ¾Ð¼ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ðµ
        - smallest_doc: Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð¾ ÑÐ°Ð¼Ð¾Ð¼ Ð¼Ð°Ð»ÐµÐ½ÑŒÐºÐ¾Ð¼ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ðµ
        - total_chars: Ð¾Ð±Ñ‰ÐµÐµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²
        - avg_chars_per_doc: ÑÑ€ÐµÐ´Ð½ÐµÐµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð² Ð½Ð° Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚
    """
    if not docs_data:
        return {
            "processed_files": 0,
            "total_tokens": 0,
            "avg_tokens_per_doc": 0.0,
            "extensions_count": {},
            "largest_doc": None,
            "smallest_doc": None,
            "total_chars": 0,
            "avg_chars_per_doc": 0.0,
        }

    # ÐžÑÐ½Ð¾Ð²Ð½Ð°Ñ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°
    stats = {
        "processed_files": len(docs_data),
        "total_tokens": sum(doc.metadata["tokens_count"] for doc in docs_data),
        "total_chars": sum(doc.metadata["text_length"] for doc in docs_data),
        "extensions_count": dict(
            Counter(doc.metadata["extension"] for doc in docs_data)
        ),
    }

    # Ð¡Ñ€ÐµÐ´Ð½Ð¸Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ
    stats["avg_tokens_per_doc"] = stats["total_tokens"] / stats["processed_files"]
    stats["avg_chars_per_doc"] = stats["total_chars"] / stats["processed_files"]

    # Ð¡Ð°Ð¼Ñ‹Ð¹ Ð±Ð¾Ð»ÑŒÑˆÐ¾Ð¹ Ð¸ Ð¼Ð°Ð»ÐµÐ½ÑŒÐºÐ¸Ð¹ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹
    docs_by_tokens = sorted(docs_data, key=lambda x: x.metadata["tokens_count"])
    stats["smallest_doc"] = {
        "path": docs_by_tokens[0].relative_path,
        "tokens": docs_by_tokens[0].metadata["tokens_count"],
        "chars": docs_by_tokens[0].metadata["text_length"],
    }
    stats["largest_doc"] = {
        "path": docs_by_tokens[-1].relative_path,
        "tokens": docs_by_tokens[-1].metadata["tokens_count"],
        "chars": docs_by_tokens[-1].metadata["text_length"],
    }

    return stats


def format_statistics_for_display(stats: Dict[str, Any]) -> str:
    """
    Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸ Ð´Ð»Ñ ÐºÑ€Ð°ÑÐ¸Ð²Ð¾Ð³Ð¾ Ð²Ñ‹Ð²Ð¾Ð´Ð°

    Args:
        stats: Ð¡Ð»Ð¾Ð²Ð°Ñ€ÑŒ ÑÐ¾ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¾Ð¹ Ð¸Ð· calculate_statistics_from_processed_docs

    Returns:
        ÐžÑ‚Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ ÑÑ‚Ñ€Ð¾ÐºÐ° ÑÐ¾ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¾Ð¹
    """
    if stats["processed_files"] == 0:
        return "âŒ ÐÐµÑ‚ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²"

    lines = [
        "ðŸ“Š Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° ÐºÐ¾Ñ€Ð¿ÑƒÑÐ°:",
        f"ðŸ“ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²: {stats['processed_files']}",
        f"ðŸ”¤ ÐžÐ±Ñ‰ÐµÐµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²: {stats['total_tokens']:,}",
        f"ðŸ“„ Ð¡Ñ€ÐµÐ´Ð½ÐµÐµ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð½Ð° Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚: {stats['avg_tokens_per_doc']:.1f}",
        f"ðŸ“ ÐžÐ±Ñ‰ÐµÐµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²: {stats['total_chars']:,}",
        f"ðŸ“– Ð¡Ñ€ÐµÐ´Ð½ÐµÐµ ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð² Ð½Ð° Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚: {stats['avg_chars_per_doc']:.1f}",
        f"ðŸ“‹ Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ñ‹ Ñ„Ð°Ð¹Ð»Ð¾Ð²: {stats['extensions_count']}",
    ]

    if stats["largest_doc"]:
        lines.extend(
            [
                "ðŸ“ˆ Ð¡Ð°Ð¼Ñ‹Ð¹ Ð±Ð¾Ð»ÑŒÑˆÐ¾Ð¹ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚:",
                f"   ðŸ“„ {stats['largest_doc']['path']}",
                f"   ðŸ”¤ {stats['largest_doc']['tokens']} Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð², {stats['largest_doc']['chars']} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²",
            ]
        )

    if stats["smallest_doc"]:
        lines.extend(
            [
                "ðŸ“‰ Ð¡Ð°Ð¼Ñ‹Ð¹ Ð¼Ð°Ð»ÐµÐ½ÑŒÐºÐ¸Ð¹ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚:",
                f"   ðŸ“„ {stats['smallest_doc']['path']}",
                f"   ðŸ”¤ {stats['smallest_doc']['tokens']} Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð², {stats['smallest_doc']['chars']} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²",
            ]
        )

    return "\n".join(lines)


def calculate_model_statistics(model_info: Dict[str, Any]) -> str:
    """
    Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð´Ð»Ñ Ð²Ñ‹Ð²Ð¾Ð´Ð°

    Args:
        model_info: Ð˜Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð¾ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸Ð· Doc2VecTrainer.get_model_info()

    Returns:
        ÐžÑ‚Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ ÑÑ‚Ñ€Ð¾ÐºÐ° ÑÐ¾ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸
    """
    if model_info.get("status") != "loaded":
        return f"âŒ ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð°: {model_info.get('status', 'Ð½ÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð¾')}"

    lines = [
        "ðŸ§  Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸:",
        f"âœ… Ð¡Ñ‚Ð°Ñ‚ÑƒÑ: {model_info['status']}",
        f"ðŸ“ Ð Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð¾Ð²: {model_info['vector_size']}",
        f"ðŸ“š Ð Ð°Ð·Ð¼ÐµÑ€ ÑÐ»Ð¾Ð²Ð°Ñ€Ñ: {model_info['vocabulary_size']:,} ÑÐ»Ð¾Ð²",
        f"ðŸ“„ Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð² Ð¼Ð¾Ð´ÐµÐ»Ð¸: {model_info['documents_count']}",
        f"ðŸ” Ð Ð°Ð·Ð¼ÐµÑ€ Ð¾ÐºÐ½Ð° ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°: {model_info['window']}",
        f"ðŸ“Š ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ð° ÑÐ»Ð¾Ð²Ð°: {model_info['min_count']}",
        f"ðŸ”„ ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÐ¿Ð¾Ñ… Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ: {model_info['epochs']}",
    ]

    # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ
    if "training_time_formatted" in model_info:
        lines.append(f"â±ï¸ Ð’Ñ€ÐµÐ¼Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ: {model_info['training_time_formatted']}")

    if "training_date" in model_info:
        lines.append(f"ðŸ“… Ð”Ð°Ñ‚Ð° Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ: {model_info['training_date']}")

    if "corpus_size" in model_info and model_info["corpus_size"] > 0:
        lines.append(
            f"ðŸ“‘ Ð Ð°Ð·Ð¼ÐµÑ€ ÐºÐ¾Ñ€Ð¿ÑƒÑÐ° Ð¿Ñ€Ð¸ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ð¸: {model_info['corpus_size']} Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²"
        )

    # Ð ÐµÐ¶Ð¸Ð¼ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ
    if model_info.get("dm") == 1:
        lines.append("ðŸ”§ Ð ÐµÐ¶Ð¸Ð¼: Distributed Memory (DM)")
    else:
        lines.append("ðŸ”§ Ð ÐµÐ¶Ð¸Ð¼: Distributed Bag of Words (DBOW)")

    # Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹
    if model_info.get("negative", 0) > 0:
        lines.append(f"âž– Negative sampling: {model_info['negative']}")
    if model_info.get("hs") == 1:
        lines.append("ðŸŒ³ Hierarchical Softmax: Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½")

    return "\n".join(lines)
