================================================================================
SEMANTIC SEARCH PROJECT EXPORT
Date: 2025-06-22 19:35:43
Root: C:\Users\evgen\Evgeny\Dev_projects\Dev_Python\diplom\semantic-search
================================================================================

PROJECT STRUCTURE:
----------------------------------------

[root]
  - pyproject.toml
  - README.md
  - LICENSE
  - .gitignore

src\semantic_search/
  - __init__.py
  - config.py
  - main.py

src\semantic_search\core/
  - __init__.py
  - doc2vec_trainer.py
  - document_processor.py
  - search_engine.py
  - text_summarizer.py

src\semantic_search\gui/
  - __init__.py
  - evaluation_widget.py
  - main_window.py

src\semantic_search\utils/
  - __init__.py
  - cache_manager.py
  - file_utils.py
  - logging_config.py
  - notification_system.py
  - performance_monitor.py
  - statistics.py
  - task_manager.py
  - text_utils.py
  - validators.py

src\semantic_search\evaluation/
  - __init__.py
  - baselines.py
  - comparison.py
  - metrics.py

scripts/
  - build.py
  - cache_clear.py
  - check_dependencies.py
  - cleanup.py
  - demo_evaluation.py
  - export_project.py
  - inspect_corpus.py
  - list_models.py
  - print_project_tree.py
  - run_semantic_search.bat
  - run_semantic_search.sh
  - setup_spacy.py

tests/
  - __init__.py
  - test_core_functionality.py

config/
  - app_config.json

================================================================================
FILE CONTENTS:
================================================================================


========================================
FILE: pyproject.toml
========================================
[project]
name = "semantic-search"
version = "0.1.0"
description = "Десктопное приложение для семантического поиска по документам с использованием Doc2Vec"
authors = [
    {name = "Evgeny Odintsov",email = "ev1genial@gmail.com"}
]
readme = "README.md"
requires-python = ">=3.10,<3.13"
dependencies = [
    "pymupdf (>=1.26.0,<2.0.0)",
    "pywin32 (>=310,<311); sys_platform == 'win32'",
    "python-docx (>=1.1.2,<2.0.0)",
    "spacy (>=3.8.7,<4.0.0)",
    "gensim (>=4.3.3,<5.0.0)",
    "pyqt6 (>=6.9.1,<7.0.0)",
    "loguru (>=0.7.3,<0.8.0)",
    "click (>=8.2.1,<9.0.0)",
    "scikit-learn (>=1.7.0,<2.0.0)",
    "psutil (>=7.0.0,<8.0.0)",
    "matplotlib (>=3.10.3,<4.0.0)",
    "seaborn (>=0.13.2,<0.14.0)"
]

[project.optional-dependencies]
openai = ["openai (>=1.88.0,<2.0.0)"]

[project.scripts]
semantic-search = "semantic_search.main:main"
semantic-search-cli = "semantic_search.main:cli_mode"

[tool.poetry]
packages = [{include = "semantic_search", from = "src"}]

[tool.poetry.group.dev.dependencies]
ruff = "^0.11.13"
pytest = "^8.4.0"
pytest-qt = "^4.4.0"
pyinstaller = "^6.14.1"
pytest-cov = "^6.2.1"
pytest-benchmark = "^5.1.0"
mypy = "^1.16.1"
pre-commit = "^4.2.0"

[build-system]
requires = ["poetry-core>=2.0.0,<3.0.0"]
build-backend = "poetry.core.masonry.api"

[tool.pytest.ini_options]
testpaths = ["tests"]


========================================
FILE: README.md
========================================
# Semantic Document Search

Десктопное приложение для интеллектуального семантического поиска по локальной базе документов с использованием модели Doc2Vec.

## 🚀 Возможности

- 🔍 **Семантический поиск** - находит документы по смыслу, а не только по ключевым словам
- 📄 **Поддержка популярных форматов** - PDF, DOCX, DOC
- 📝 **Автоматическое создание выжимок** - экстрактивная суммаризация документов
- 🧠 **Обучение собственных моделей** - создавайте модели на ваших документах
- 💻 **Удобный графический интерфейс** - современный и интуитивный
- 🚀 **Высокая производительность** - многопоточная обработка и кэширование

## 📋 Системные требования

- Python 3.10 - 3.12
- Windows 10/11 (для поддержки .doc файлов)
- Минимум 4 ГБ ОЗУ
- 500 МБ свободного места на диске

## 🛠️ Установка

### 1. Клонирование репозитория
```bash
git clone https://github.com/yourusername/semantic-search.git
cd semantic-search
```

### 2. Установка Poetry (если не установлен)
```bash
# Windows (PowerShell)
(Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | py -

# Linux/MacOS
curl -sSL https://install.python-poetry.org | python3 -
```

### 3. Установка зависимостей
```bash
# Установка всех зависимостей через Poetry
poetry install
```

### 4. Установка языковой модели SpaCy
```bash
# Автоматическая установка
poetry run python scripts/setup_spacy.py

# Или вручную
poetry run python -m spacy download ru_core_news_sm
```

## 🚀 Запуск приложения

### Графический интерфейс
```bash
poetry run semantic-search
```

### Командная строка
```bash
poetry run semantic-search-cli --help
```

## 📖 Использование

### Графический интерфейс

1. **Обучение модели**
   - Перейдите на вкладку "Обучение"
   - Выберите папку с документами
   - Настройте параметры модели
   - Нажмите "Начать обучение"

2. **Поиск документов**
   - Выберите обученную модель из списка
   - Введите поисковый запрос
   - Нажмите "Поиск"
   - Кликните на результат для просмотра

3. **Создание выжимок**
   - Перейдите на вкладку "Суммаризация"
   - Выберите файл документа
   - Укажите количество предложений
   - Нажмите "Создать выжимку"

### Командная строка

#### Обучение модели
```bash
# Базовое обучение
poetry run semantic-search-cli train -d /path/to/documents

# С настройкой параметров
poetry run semantic-search-cli train -d /path/to/documents \
    --model my_model \
    --vector-size 200 \
    --epochs 50
```

#### Поиск документов
```bash
poetry run semantic-search-cli search -d /path/to/documents \
    -q "машинное обучение и нейронные сети" \
    --top-k 5
```

#### Создание выжимок
```bash
# Для одного файла
poetry run semantic-search-cli summarize-file -f document.pdf -s 5

# Для всех документов в папке
poetry run semantic-search-cli summarize-batch -d /path/to/documents \
    -s 3 -o /path/to/summaries
```

#### Просмотр статистики
```bash
poetry run semantic-search-cli stats -d /path/to/documents -m my_model
```

#### Команды конфига
```bash
# Показать текущую конфигурацию
poetry run semantic-search-cli config --show

# Установить максимальную длину текста
poetry run semantic-search-cli config --set text_processing.max_text_length 5000000

# Перезагрузить конфигурацию
poetry run semantic-search-cli config --reload
```

## 🔬 Оценка и сравнение с OpenAI

Приложение включает модуль для количественного сравнения качества поиска между обученной моделью Doc2Vec и OpenAI embeddings.

### Установка дополнительных зависимостей

```bash
# Установка с поддержкой OpenAI
poetry install -E openai

# Или отдельно
pip install openai pandas
```

### Настройка OpenAI API

```bash
# Windows
setx OPENAI_API_KEY "your_api_key_here"

# Linux/MacOS
export OPENAI_API_KEY="your_api_key_here"
```

### Запуск сравнения

#### Через GUI

1. Откройте вкладку "📊 Оценка методов"
2. Введите OpenAI API ключ
3. Выберите набор тестов
4. Нажмите "Запустить сравнение"

#### Через CLI

```bash
# Быстрая оценка (3 теста)
poetry run semantic-search-cli evaluate --test-cases quick

# Стандартная оценка (5 тестов)
poetry run semantic-search-cli evaluate --test-cases standard

# Расширенная оценка (10 тестов)
poetry run semantic-search-cli evaluate --test-cases extended

# С указанием модели и директории результатов
poetry run semantic-search-cli evaluate \
    --model my_model \
    --test-cases standard \
    --output-dir ./evaluation_results
```

#### Демонстрационный скрипт

```bash
# Запуск демонстрации
poetry run python scripts/demo_evaluation.py
```

### Метрики оценки

Система оценивает следующие метрики:

- **MAP (Mean Average Precision)** - основная метрика качества ранжирования
- **MRR (Mean Reciprocal Rank)** - позиция первого релевантного результата
- **Precision@k** - точность в топ-k результатах
- **Recall@k** - полнота в топ-k результатах
- **NDCG@k** - нормализованный дисконтированный накопленный выигрыш
- **Скорость поиска** - среднее время на запрос
- **Экономическая эффективность** - стоимость использования

### Результаты оценки

После выполнения оценки создаются:

```
data/evaluation_results/
├── comparison_results.csv      # Таблица с метриками
├── comparison_report.txt       # Текстовый отчет
├── detailed_results.json       # Детальные результаты
└── plots/
    ├── comparison_plots.png    # Сравнительные графики
    └── *_detailed.png          # Детальные графики по методам
```

### Интерпретация результатов

#### Преимущества Doc2Vec:
- ✅ **Скорость**: В 10-100 раз быстрее OpenAI
- ✅ **Автономность**: Работает без интернета
- ✅ **Стоимость**: Бесплатно после обучения
- ✅ **Конфиденциальность**: Данные остаются локальными
- ✅ **Специализация**: Лучше для узкоспециализированных корпусов

#### Преимущества OpenAI:
- ✅ **Универсальность**: Работает без предварительного обучения
- ✅ **Многоязычность**: Поддержка 100+ языков
- ✅ **Обновления**: Постоянные улучшения модели

### Рекомендации по выбору метода

| Сценарий | Рекомендация |
|----------|--------------|
| Специализированный корпус документов | Doc2Vec |
| Высокая нагрузка (>1000 запросов/день) | Doc2Vec |
| Конфиденциальные данные | Doc2Vec |
| Работа без интернета | Doc2Vec |
| Универсальный поиск | OpenAI |
| Малый объем данных | OpenAI |
| Многоязычные документы | OpenAI |

### Экономическое обоснование

При типичной нагрузке в 1000 запросов в день:
- **OpenAI**: ~$1.50/месяц ($18/год)
- **Doc2Vec**: $0 (после единоразового обучения)
- **Экономия**: $18/год на минимальной нагрузке

При корпоративном использовании (10,000+ запросов/день) экономия может достигать сотен долларов в месяц.

## 🏗️ Сборка в исполняемый файл

```bash
# Установка PyInstaller (если не установлен)
poetry add --group dev pyinstaller

# Сборка в один файл
poetry run python scripts/build.py --onefile --windowed

# Сборка в папку
poetry run python scripts/build.py
```

## 🧪 Тестирование

```bash
# Запуск всех тестов
poetry run pytest

# С покрытием кода
poetry run pytest --cov=semantic_search

# Только юнит-тесты
poetry run pytest tests/test_core_functionality.py

# Бенчмарки производительности
poetry run pytest tests/ -k "performance" --benchmark-only
```

## 📁 Структура проекта

```
semantic-search/
├── README.md                            # Информация о проекте
├── config/                              # Сохраненные настройки
│   └── app_config.json
├── data/                                # Данные и модели
│   ├── cache/                           # Кэш
│   ├── models/                          # Обученные модели
│   └── temp/                            # Временные файлы
├── logs/                                # Логи
├── pyproject.toml                       # Конфигурация Poetry
├── scripts/                             # Вспомогательные скрипты
│   ├── build.py
│   ├── print_project_tree.py
│   └── setup_spacy.py
├── src/
│   └── semantic_search/
│       ├── config.py                    # Настройки
│       ├── core/                        # Основная логика
│       │   ├── doc2vec_trainer.py
│       │   ├── document_processor.py
│       │   ├── search_engine.py
│       │   └── text_summarizer.py
│       ├── gui/                         # Графический интерфейс
│       │   └── main_window.py
│       ├── main.py                      # Точка входа
│       └── utils/                       # Вспомогательные модули
│           ├── cache_manager.py
│           ├── file_utils.py
│           ├── logging_config.py
│           ├── notification_system.py
│           ├── performance_monitor.py
│           ├── statistics.py
│           ├── task_manager.py
│           ├── text_utils.py
│           └── validators.py
└── tests/                               # Тесты
    └── test_core_functionality.py
```

## ⚙️ Конфигурация

Приложение автоматически создает файл конфигурации `config/app_config.json` при первом запуске.

### Основные параметры:

```json
{
  "text_processing": {
    "min_text_length": 100,
    "max_text_length": 5000000,
    "min_tokens_count": 10,
    "min_token_length": 2,
    "min_sentence_length": 10,
    "remove_stop_words": true,
    "lemmatize": true,
    "max_file_size_mb": 100,
    "chunk_size": 800000,
    "spacy_max_length": 3000000
  },
  "doc2vec": {
    "vector_size": 150,
    "window": 10,
    "min_count": 2,
    "epochs": 40,
    "workers": 15,
    "seed": 42,
    "dm": 1,
    "negative": 5,
    "hs": 0,
    "sample": 0.0001
  },
  "search": {
    "default_top_k": 10,
    "max_top_k": 100,
    "similarity_threshold": 0.1,
    "enable_caching": true,
    "cache_size": 1000,
    "enable_filtering": true
  }
}
```

## 🔧 Решение проблем

### SpaCy модель не найдена
```bash
# Переустановите модель
poetry run python -m spacy download ru_core_news_sm --force
```

### Ошибка при обработке .doc файлов
- Убедитесь, что установлен Microsoft Word
- Или конвертируйте .doc файлы в .docx

### Недостаточно памяти при обучении
- Уменьшите размер батча в конфигурации
- Используйте меньшую размерность векторов
- Обрабатывайте документы частями

## 🤝 Вклад в проект

1. Форкните репозиторий
2. Создайте ветку для функции (`git checkout -b feature/AmazingFeature`)
3. Закоммитьте изменения (`git commit -m 'Add some AmazingFeature'`)
4. Запушьте в ветку (`git push origin feature/AmazingFeature`)
5. Откройте Pull Request

## 📝 Лицензия

Этот проект лицензирован под MIT License - см. файл LICENSE для деталей.

## 👤 Автор

**Evgeny Odintsov**
- Email: ev1genial@gmail.com
- GitHub: [@unexpectedpatronus](https://github.com/Unexpectedpatronus)

## 🙏 Благодарности

- [Gensim](https://radimrehurek.com/gensim/) - за отличную реализацию Doc2Vec
- [SpaCy](https://spacy.io/) - за мощные инструменты NLP
- [PyQt6](https://www.riverbankcomputing.com/software/pyqt/) - за возможность создания GUI

## 📈 Roadmap

- [ ] Поддержка английского языка
- [ ] Экспорт результатов поиска
- [ ] Интеграция с облачными хранилищами
- [ ] Веб-интерфейс
- [ ] Поддержка других форматов (TXT, RTF, ODT)
- [ ] Улучшенная визуализация результатов
- [ ] Пакетная обработка запросов
- [ ] API для интеграции с другими приложениями


========================================
FILE: LICENSE
========================================
[Binary file - 1093 bytes]


========================================
FILE: .gitignore
========================================
[Binary file - 4564 bytes]


========================================
FILE: src\semantic_search\__init__.py
========================================
"""Semantic Search - Интеллектуальный поиск по документам"""

__version__ = "1.0.0"
__author__ = "Evgeny Odintsov"
__email__ = "ev1genial@gmail.com"

from semantic_search.core.doc2vec_trainer import Doc2VecTrainer
from semantic_search.core.document_processor import DocumentProcessor
from semantic_search.core.search_engine import SearchResult, SemanticSearchEngine
from semantic_search.core.text_summarizer import TextSummarizer

__all__ = [
    "Doc2VecTrainer",
    "DocumentProcessor",
    "SemanticSearchEngine",
    "SearchResult",
    "TextSummarizer",
]


========================================
FILE: src\semantic_search\config.py
========================================
"""Расширенная конфигурация приложения"""

import json
import multiprocessing
from dataclasses import asdict, dataclass, field
from pathlib import Path
from typing import Any, Dict, Optional

from loguru import logger

# Базовые пути
PROJECT_ROOT = Path(__file__).parent.parent.parent
SRC_DIR = PROJECT_ROOT / "src"
DATA_DIR = PROJECT_ROOT / "data"
MODELS_DIR = DATA_DIR / "models"
TEMP_DIR = DATA_DIR / "temp"
LOGS_DIR = PROJECT_ROOT / "logs"
CACHE_DIR = DATA_DIR / "cache"
CONFIG_DIR = PROJECT_ROOT / "config"
EVALUATION_RESULTS_DIR = DATA_DIR / "evaluation_results"

# Создаем директории
for dir_path in [
    DATA_DIR,
    MODELS_DIR,
    TEMP_DIR,
    LOGS_DIR,
    CACHE_DIR,
    CONFIG_DIR,
    EVALUATION_RESULTS_DIR,
]:
    dir_path.mkdir(exist_ok=True, parents=True)


@dataclass
class AppConfig:
    """Главная конфигурация приложения"""

    # Обработка текста
    text_processing: Dict[str, Any] = field(default_factory=dict)

    # Модель Doc2Vec
    doc2vec: Dict[str, Any] = field(default_factory=dict)

    # Альтернативные конфигурации Doc2Vec
    doc2vec_presets: Dict[str, Dict[str, Any]] = field(default_factory=dict)

    # Поиск
    search: Dict[str, Any] = field(default_factory=dict)

    # GUI
    gui: Dict[str, Any] = field(default_factory=dict)

    # Суммаризация
    summarization: Dict[str, Any] = field(default_factory=dict)

    # Производительность
    performance: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        if not self.text_processing:
            self.text_processing = {
                "min_text_length": 100,
                "max_text_length": 5_000_000,
                "min_tokens_count": 10,
                "min_token_length": 2,
                "min_sentence_length": 10,
                "remove_stop_words": True,
                "lemmatize": True,
                "max_file_size_mb": 100,
                "chunk_size": 800_000,
                "spacy_max_length": 3_000_000,
                # НОВОЕ: Модели SpaCy для разных языков
                "spacy_models": {
                    "ru": "ru_core_news_sm",  # Русская модель
                    "en": "en_core_web_sm",  # Английская модель
                },
            }

        if not self.doc2vec:
            # ОБНОВЛЕННЫЕ ПАРАМЕТРЫ для многоязычных документов
            self.doc2vec = {
                "vector_size": 300,  # Увеличено для многоязычности
                "window": 15,  # Увеличено для длинных документов
                "min_count": 3,  # Увеличено для фильтрации шума
                "epochs": 30,  # Оптимизировано для баланса скорость/качество
                "workers": max(1, multiprocessing.cpu_count() - 1),
                "seed": 42,
                "dm": 1,  # Distributed Memory
                "dm_concat": 0,  # Усреднение (экономия памяти)
                "dm_mean": 1,  # Усреднение векторов слов
                "negative": 10,  # Увеличено для лучшего качества
                "hs": 0,  # Hierarchical Softmax отключен
                "sample": 1e-5,  # Уменьшено для сохранения терминов
                "alpha": 0.025,  # Начальная скорость обучения
                "min_alpha": 0.0001,  # Конечная скорость обучения
            }

        if not self.doc2vec_presets:
            # Альтернативные конфигурации
            self.doc2vec_presets = {
                "fast": {
                    "vector_size": 200,
                    "window": 10,
                    "min_count": 5,
                    "epochs": 15,
                    "negative": 5,
                    "sample": 1e-4,
                },
                "balanced": {
                    "vector_size": 300,
                    "window": 15,
                    "min_count": 3,
                    "epochs": 30,
                    "negative": 10,
                    "sample": 1e-5,
                },
                "quality": {
                    "vector_size": 400,
                    "window": 20,
                    "min_count": 2,
                    "epochs": 50,
                    "negative": 15,
                    "sample": 1e-5,
                },
            }

        if not self.search:
            self.search = {
                "default_top_k": 10,
                "max_top_k": 100,
                "similarity_threshold": 0.1,
                "enable_caching": True,
                "cache_size": 1000,
                "enable_filtering": True,
            }

        if not self.gui:
            self.gui = {
                "window_title": "Semantic Document Search",
                "window_size": (1400, 900),
                "theme": "default",
                "font_size": 10,
                "enable_dark_theme": False,
                "auto_save_settings": True,
            }

        if not self.summarization:
            self.summarization = {
                "default_sentences_count": 5,
                "max_sentences_count": 20,
                "min_sentence_length": 15,
                "use_textrank": True,
                "damping_factor": 0.85,
                "max_iterations": 100,
                "filter_short_sentences": True,  # Включить фильтрацию коротких предложений
                "max_digit_ratio": 0.5,  # Максимальная доля цифр в предложении
                "min_meaningful_words": 2,  # Минимум значимых слов (длиннее 3 символов)
            }

        if not self.performance:
            self.performance = {
                "enable_monitoring": True,
                "log_slow_operations": True,
                "slow_operation_threshold": 5.0,  # секунды
                "memory_warning_threshold": 80,  # процент
                "enable_profiling": False,
                # Новые параметры для управления параллелизмом
                "task_manager_workers": min(8, multiprocessing.cpu_count()),
                "cpu_task_manager_workers": max(1, multiprocessing.cpu_count() - 2),
                "adaptive_workers": True,  # Автоматическая адаптация
                "min_workers_per_task": 2,  # Минимум воркеров на задачу
            }


class ConfigManager:
    """Менеджер конфигурации"""

    def __init__(self, config_file: Optional[Path] = None):
        self.config_file = config_file or (CONFIG_DIR / "app_config.json")
        self._config = None

    @property
    def config(self) -> AppConfig:
        """Получение конфигурации с ленивой загрузкой"""
        if self._config is None:
            self._config = self.load_config()
        return self._config

    def load_config(self) -> AppConfig:
        """Загрузка конфигурации из файла"""
        if self.config_file.exists():
            try:
                with open(self.config_file, "r", encoding="utf-8") as f:
                    config_data = json.load(f)

                logger.info(f"Конфигурация загружена из {self.config_file}")
                return AppConfig(**config_data)

            except Exception as e:
                logger.warning(
                    f"Ошибка загрузки конфигурации: {e}. Используется конфигурация по умолчанию"
                )

        # Создаем конфигурацию по умолчанию
        default_config = AppConfig()
        self.save_config(default_config)
        return default_config

    def save_config(self, config: AppConfig) -> bool:
        """Сохранение конфигурации в файл"""
        try:
            with open(self.config_file, "w", encoding="utf-8") as f:
                json.dump(asdict(config), f, indent=2, ensure_ascii=False)

            logger.info(f"Конфигурация сохранена в {self.config_file}")
            return True

        except Exception as e:
            logger.error(f"Ошибка сохранения конфигурации: {e}")
            return False

    def reload_config(self) -> AppConfig:
        """Принудительная перезагрузка конфигурации"""
        self._config = None
        return self.config

    def reset_to_defaults(self) -> AppConfig:
        """Сброс конфигурации к значениям по умолчанию"""
        self._config = AppConfig()
        self.save_config(self._config)
        logger.info("Конфигурация сброшена к значениям по умолчанию")
        return self._config

    def update_config(self, **kwargs) -> bool:
        """Обновление конфигурации"""
        try:
            config_dict = asdict(self.config)

            for key, value in kwargs.items():
                if key in config_dict:
                    if isinstance(config_dict[key], dict) and isinstance(value, dict):
                        config_dict[key].update(value)
                    else:
                        config_dict[key] = value

            self._config = AppConfig(**config_dict)
            return self.save_config(self._config)

        except Exception as e:
            logger.error(f"Ошибка обновления конфигурации: {e}")
            return False


# Глобальный менеджер конфигурации
config_manager = ConfigManager()

# Экспортируем для обратной совместимости
SUPPORTED_EXTENSIONS = {".pdf", ".docx", ".doc"}

# ОБНОВЛЕНО: Теперь модели SpaCy берутся из конфигурации
SPACY_MODELS: dict = config_manager.config.text_processing.get(
    "spacy_models", {"ru": "ru_core_news_sm", "en": "en_core_web_sm"}
)

# Для обратной совместимости оставляем SPACY_MODEL как русскую модель по умолчанию
SPACY_MODEL = SPACY_MODELS.get("ru", "ru_core_news_sm")

TEXT_PROCESSING_CONFIG = config_manager.config.text_processing
DOC2VEC_CONFIG = config_manager.config.doc2vec
DOC2VEC_PRESETS = config_manager.config.doc2vec_presets
SEARCH_CONFIG = config_manager.config.search
GUI_CONFIG = config_manager.config.gui
SUMMARIZATION_CONFIG = config_manager.config.summarization
PERFORMANCE_CONFIG = config_manager.config.performance


========================================
FILE: src\semantic_search\main.py
========================================
"""Точка входа в приложение"""

import sys
import time
from pathlib import Path
from typing import Optional

import click
from loguru import logger

from semantic_search.config import GUI_CONFIG, SPACY_MODEL
from semantic_search.core.doc2vec_trainer import Doc2VecTrainer
from semantic_search.core.document_processor import DocumentProcessor
from semantic_search.core.search_engine import SemanticSearchEngine
from semantic_search.core.text_summarizer import TextSummarizer
from semantic_search.utils.file_utils import FileExtractor
from semantic_search.utils.logging_config import setup_logging
from semantic_search.utils.notification_system import notification_manager
from semantic_search.utils.performance_monitor import PerformanceMonitor
from semantic_search.utils.statistics import (
    calculate_model_statistics,
    calculate_statistics_from_processed_docs,
    format_statistics_for_display,
)
from semantic_search.utils.task_manager import task_manager
from semantic_search.utils.text_utils import check_spacy_model_availability
from semantic_search.utils.validators import DataValidator, FileValidator

performance_monitor = PerformanceMonitor()


def main():
    """Главная функция приложения"""

    notification_manager.start()

    try:
        setup_logging()

        # Проверка SpaCy с уведомлением
        spacy_info = check_spacy_model_availability()

        # Проверяем установку моделей
        if not spacy_info["spacy_installed"]:
            notification_manager.error(
                "SpaCy не установлен",
                "Установите SpaCy для работы с текстом",
                "Используйте: pip install spacy",
            )
        else:
            # Проверяем наличие хотя бы одной модели
            if not (spacy_info["ru_model_loadable"] or spacy_info["en_model_loadable"]):
                notification_manager.warning(
                    "SpaCy модели не найдены",
                    "Ни одна языковая модель не установлена",
                    "Используйте: poetry run python scripts/setup_spacy.py",
                )
            else:
                # Информируем о доступных моделях
                models_status = []
                if spacy_info["ru_model_loadable"]:
                    models_status.append("русская")
                if spacy_info["en_model_loadable"]:
                    models_status.append("английская")

                notification_manager.success(
                    "SpaCy готов", f"Загружены модели: {', '.join(models_status)}"
                )

        # Проверяем доступность PyQt6
        try:
            from PyQt6.QtWidgets import QApplication

            from semantic_search.gui.main_window import MainWindow
        except ImportError as e:
            logger.error(f"PyQt6 не установлен: {e}")
            notification_manager.error(
                "Ошибка импорта",
                "PyQt6 не установлен",
                "Установите зависимости: poetry install",
            )
            print("\n❌ PyQt6 не установлен!")
            print("Установите зависимости командой: poetry install")
            print("\nВы можете использовать CLI режим:")
            print("poetry run semantic-search-cli --help")
            sys.exit(1)

        # Создание приложения Qt
        app = QApplication(sys.argv)
        app.setApplicationName(GUI_CONFIG["window_title"])
        app.setOrganizationName("Semantic Search")

        # Устанавливаем стиль
        app.setStyle("Fusion")  # Современный стиль

        # Создание и отображение главного окна
        main_window = MainWindow()
        main_window.show()

        logger.info("Главное окно создано и отображено")

        # Запуск цикла событий
        exit_code = app.exec()
        logger.info(f"Приложение завершено с кодом: {exit_code}")
        sys.exit(exit_code)

    except Exception as e:
        notification_manager.error(
            "Критическая ошибка", "Ошибка при запуске приложения", str(e)
        )
        logger.error(f"Критическая ошибка: {e}", exc_info=True)
        print(f"\n❌ Критическая ошибка: {e}")
        print("\nПроверьте логи для деталей")
        sys.exit(1)
    finally:
        notification_manager.stop()
        task_manager.shutdown()


@click.group()
def cli():
    """Semantic Document Search CLI"""
    setup_logging()


@cli.command()
@click.option("--documents", "-d", required=True, help="Путь к папке с документами")
@click.option("--model", "-m", default="doc2vec_model", help="Имя модели")
@click.option("--vector-size", default=150, help="Размерность векторов")
@click.option("--epochs", default=40, help="Количество эпох обучения")
@click.option("--async-mode", "-a", is_flag=True, help="Асинхронное выполнение")
def train(documents: str, model: str, vector_size: int, epochs: int, async_mode: bool):
    """
    Обучить модель Doc2Vec на корпусе документов

    Args:
        documents: Путь к папке с документами
        model: Имя модели для сохранения
        vector_size: Размерность векторов
        epochs: Количество эпох обучения
    """
    try:
        # Валидация параметров
        documents_path = DataValidator.validate_directory_path(Path(documents))
        model_params = DataValidator.validate_model_params(
            vector_size=vector_size, epochs=epochs
        )

        logger.info("Валидация прошла успешно")

    except Exception as e:
        click.echo(f"❌ Ошибка валидации: {e}")
        return

    def train_task(progress_tracker=None):
        """Задача обучения модели"""

        # Начинаем отсчет общего времени
        start_time = time.time()

        with performance_monitor.measure_operation("document_processing"):
            # Обработка документов
            processor = DocumentProcessor()
            processed_docs = []

            file_extractor = FileExtractor()
            file_paths = file_extractor.find_documents(documents_path)

            if progress_tracker:
                progress_tracker.total_steps = len(file_paths) + epochs + 2
                progress_tracker.update(0, "Начинаем обработку документов...")

            for i, doc in enumerate(processor.process_documents(documents_path)):
                processed_docs.append(doc)
                if progress_tracker:
                    progress_tracker.update(
                        i + 1, f"Обработан документ: {doc.relative_path}"
                    )

            if not processed_docs:
                raise ValueError("Не удалось обработать документы")

            corpus = [
                (doc.tokens, doc.relative_path, doc.metadata) for doc in processed_docs
            ]

            if progress_tracker:
                progress_tracker.update(message="Подготовка к обучению модели...")

        with performance_monitor.measure_operation("model_training"):
            # Обучение модели
            trainer = Doc2VecTrainer()

            # Обучение модели
            trained_model = trainer.train_model(
                corpus, vector_size=vector_size, epochs=epochs
            )

            if trained_model:
                # Вычисляем общее время
                training_time = time.time() - start_time
                trainer.training_metadata["training_time_formatted"] = (
                    f"{training_time:.1f}с ({training_time / 60:.1f}м)"
                )
                trainer.training_metadata["training_date"] = time.strftime(
                    "%Y-%m-%d %H:%M:%S", time.localtime(start_time)
                )
                trainer.training_metadata["corpus_size"] = len(processed_docs)
                trainer.save_model(trained_model, model)

                if progress_tracker:
                    progress_tracker.finish(
                        f"Модель обучена за {training_time / 60:.1f} минут"
                    )

                # Возвращаем статистику
                stats = calculate_statistics_from_processed_docs(processed_docs)
                return {
                    "model_saved": True,
                    "documents_processed": len(processed_docs),
                    "vocabulary_size": len(trained_model.wv.key_to_index),
                    "training_time": training_time,
                    "statistics": stats,
                }
            else:
                raise ValueError("Не удалось обучить модель")

    if async_mode:
        # Асинхронное выполнение
        notification_manager.start()

        task_id = task_manager.submit_task(
            train_task,
            name=f"Обучение модели {model}",
            description=f"Обучение на документах из {documents_path}",
            track_progress=True,
            total_steps=100,  # Примерное количество шагов
        )

        click.echo(f"🔄 Задача обучения запущена (ID: {task_id})")
        click.echo("Используйте команду 'status' для проверки прогресса")

        # Подписка на уведомления для консоли
        def console_notification_handler(notification):
            if notification.type.value == "success":
                click.echo(f"✅ {notification.title}: {notification.message}")
            elif notification.type.value == "error":
                click.echo(f"❌ {notification.title}: {notification.message}")
            elif notification.type.value == "warning":
                click.echo(f"⚠️ {notification.title}: {notification.message}")

        notification_manager.subscribe(console_notification_handler)

    else:
        # Синхронное выполнение
        try:
            with performance_monitor.measure_operation("full_training"):
                result = train_task()

            click.echo("✅ Обучение завершено успешно!")
            click.echo(f"📊 Обработано документов: {result['documents_processed']}")
            click.echo(f"📚 Размер словаря: {result['vocabulary_size']:,}")

            # Выводим детальную статистику
            stats_display = format_statistics_for_display(result["statistics"])
            click.echo(f"\n{stats_display}")

        except Exception as e:
            click.echo(f"❌ Ошибка при обучении: {e}")
            logger.error(f"Ошибка обучения модели: {e}")


@cli.command()
@click.option("--documents", "-d", required=True, help="Путь к папке с документами")
@click.option("--query", "-q", required=True, help="Поисковый запрос")
@click.option("--model", "-m", default="doc2vec_model", help="Имя модели")
@click.option("--top-k", "-k", default=10, help="Количество результатов")
def search(documents: str, query: str, model: str, top_k: int):
    """
    Поиск по документам

    Args:
        documents: Путь к папке с документами
        query: Поисковый запрос
        model: Имя модели
        top_k: Количество результатов для вывода
    """
    logger.info(f"Режим поиска: {query}")

    # Загрузка модели
    trainer = Doc2VecTrainer()
    loaded_model = trainer.load_model(model)

    if loaded_model is None:
        logger.error("❌ Не удалось загрузить модель")
        click.echo("Сначала обучите модель командой: train")
        return

    # Поиск
    search_engine = SemanticSearchEngine(loaded_model, trainer.corpus_info)
    results = search_engine.search(query, top_k=top_k)

    if results:
        click.echo(f"\n🔍 Результаты поиска для '{query}':")
        click.echo("=" * 50)
        for i, result in enumerate(results, 1):
            click.echo(f"{i}. {result.doc_id}")
            click.echo(f"   📊 Сходство: {result.similarity:.3f}")
            if result.metadata:
                tokens_count = result.metadata.get("tokens_count", "N/A")
                file_size = result.metadata.get("file_size", 0)
                click.echo(f"   📝 Токенов: {tokens_count}, Размер: {file_size} байт")
            click.echo()
    else:
        click.echo(f"❌ Результатов не найдено для запроса '{query}'")


@cli.command()
@click.option("--documents", "-d", help="Путь к папке с документами")
@click.option("--model", "-m", default="doc2vec_model", help="Имя модели")
def stats(documents: Optional[str], model: str):
    """
    Показать статистику модели и корпуса

    Args:
        documents: Путь к папке с документами (опционально)
        model: Имя модели
    """
    # Статистика корпуса
    if documents:
        documents_path = Path(documents)
        if documents_path.exists():
            processor = DocumentProcessor()
            processed_docs = list(processor.process_documents(documents_path))

            if processed_docs:
                stats_data = calculate_statistics_from_processed_docs(processed_docs)
                click.echo(format_statistics_for_display(stats_data))
            else:
                click.echo("❌ Не удалось обработать документы")
        else:
            click.echo(f"❌ Папка не найдена: {documents_path}")

    # Статистика модели
    trainer = Doc2VecTrainer()
    if trainer.load_model(model):
        model_info = trainer.get_model_info()
        click.echo(f"\n{calculate_model_statistics(model_info)}")
    else:
        click.echo(f"\n❌ Модель '{model}' не найдена")


# КОМАНДЫ СУММАРИЗАЦИИ


@cli.command()
@click.option("--file", "-f", required=True, help="Путь к файлу для суммаризации")
@click.option("--model", "-m", default="doc2vec_model", help="Имя Doc2Vec модели")
@click.option("--sentences", "-s", default=5, help="Количество предложений в выжимке")
@click.option(
    "--min-length", "-l", default=15, help="Минимальная длина предложения в символах"
)
@click.option(
    "--min-words", "-w", default=5, help="Минимальное количество слов в предложении"
)
@click.option(
    "--no-filter", is_flag=True, help="Отключить фильтрацию коротких предложений"
)
@click.option("--output", "-o", help="Файл для сохранения выжимки")
def summarize_file(
    file: str,
    model: str,
    sentences: int,
    min_length: int,
    min_words: int,
    no_filter: bool,
    output: Optional[str],
) -> None:
    """
    Создать выжимку из файла с фильтрацией коротких предложений

    Args:
        file: Путь к файлу для суммаризации
        model: Имя Doc2Vec модели для улучшенной суммаризации
        sentences: Количество предложений в выжимке
        min_length: Минимальная длина предложения в символах
        min_words: Минимальное количество слов в предложении
        no_filter: Отключить фильтрацию коротких предложений
        output: Путь для сохранения выжимки (опционально)
    """
    file_path = Path(file)
    if not file_path.exists():
        click.echo(f"❌ Файл не найден: {file_path}")
        return

    # Загрузка модели Doc2Vec
    trainer = Doc2VecTrainer()
    loaded_model = trainer.load_model(model)

    if loaded_model is None:
        click.echo("⚠️ Модель Doc2Vec не найдена. Используется базовая суммаризация")
        summarizer = TextSummarizer()
    else:
        click.echo("✅ Используется продвинутая суммаризация с Doc2Vec")
        summarizer = TextSummarizer(loaded_model)

    # Настройка параметров фильтрации
    if not no_filter:
        summarizer.min_summary_sentence_length = min_length
        summarizer.min_words_in_sentence = min_words
        click.echo(f"📏 Фильтрация: минимум {min_length} символов и {min_words} слов")
    else:
        summarizer.min_summary_sentence_length = 1
        summarizer.min_words_in_sentence = 1
        click.echo("📋 Фильтрация коротких предложений отключена")

    logger.info(f"Создание выжимки файла: {file_path}")

    # Создание выжимки
    try:
        summary = summarizer.summarize_file(str(file_path), sentences_count=sentences)

        if not summary:
            click.echo(
                "❌ Не удалось создать выжимку. Возможные причины:\n"
                "   - Все предложения слишком короткие\n"
                "   - Файл не содержит текста\n"
                "   Попробуйте --no-filter или уменьшите --min-length"
            )
            return

        # Вывод результата в консоль
        click.echo(f"\n📄 Выжимка файла: {file_path.name}")
        click.echo("=" * 60)

        for i, sentence in enumerate(summary, 1):
            click.echo(f"{i}. {sentence.strip()}")
            click.echo()  # Пустая строка между предложениями

        # Статистика суммаризации
        try:
            from semantic_search.utils.file_utils import FileExtractor

            extractor = FileExtractor()
            original_text = extractor.extract_text(file_path)

            if original_text:
                stats = summarizer.get_summary_statistics(original_text, summary)

                click.echo("📊 Статистика суммаризации:")
                click.echo("-" * 30)
                click.echo(
                    f"  📑 Исходных предложений: {stats['original_sentences_count']}"
                )

                if "valid_original_sentences_count" in stats and not no_filter:
                    filtered = (
                        stats["original_sentences_count"]
                        - stats["valid_original_sentences_count"]
                    )
                    click.echo(f"  🔽 Отфильтровано коротких: {filtered}")
                    click.echo(
                        f"  ✅ Валидных предложений: {stats['valid_original_sentences_count']}"
                    )

                click.echo(
                    f"  📄 Предложений в выжимке: {stats['summary_sentences_count']}"
                )
                click.echo(f"  📉 Коэффициент сжатия: {stats['compression_ratio']:.1%}")
                click.echo(f"  🔤 Исходных символов: {stats['original_chars_count']:,}")
                click.echo(f"  ✂️ Символов в выжимке: {stats['summary_chars_count']:,}")
                click.echo(
                    f"  📊 Сокращение текста: {stats['chars_compression_ratio']:.1%}"
                )

                if "avg_sentence_length" in stats:
                    click.echo(
                        f"  📏 Средняя длина предложения: {stats['avg_sentence_length']:.1f} слов"
                    )

        except Exception as e:
            logger.error(f"Ошибка при расчете статистики: {e}")

        # Сохранение в файл
        if output:
            output_path = Path(output)
            try:
                with open(output_path, "w", encoding="utf-8") as f:
                    f.write(f"Выжимка файла: {file_path.name}\n")
                    f.write(f"Создано: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                    if not no_filter:
                        f.write(
                            f"Фильтрация: мин. {min_length} символов, {min_words} слов\n"
                        )
                    f.write("=" * 60 + "\n\n")

                    for i, sentence in enumerate(summary, 1):
                        f.write(f"{i}. {sentence.strip()}\n\n")

                    # Добавляем статистику в файл
                    if "stats" in locals():
                        f.write("\n" + "=" * 60 + "\n")
                        f.write("СТАТИСТИКА СУММАРИЗАЦИИ\n")
                        f.write("=" * 60 + "\n")
                        f.write(
                            f"Исходных предложений: {stats['original_sentences_count']}\n"
                        )

                        if "valid_original_sentences_count" in stats and not no_filter:
                            f.write(
                                f"Валидных предложений: {stats['valid_original_sentences_count']}\n"
                            )
                            f.write(
                                f"Отфильтровано: {stats['original_sentences_count'] - stats['valid_original_sentences_count']}\n"
                            )

                        f.write(
                            f"Предложений в выжимке: {stats['summary_sentences_count']}\n"
                        )
                        f.write(
                            f"Коэффициент сжатия: {stats['compression_ratio']:.1%}\n"
                        )
                        f.write(
                            f"Исходных символов: {stats['original_chars_count']:,}\n"
                        )
                        f.write(
                            f"Символов в выжимке: {stats['summary_chars_count']:,}\n"
                        )
                        f.write(
                            f"Сокращение текста: {stats['chars_compression_ratio']:.1%}\n"
                        )

                click.echo(f"💾 Выжимка сохранена в: {output_path}")

            except Exception as e:
                click.echo(f"❌ Ошибка при сохранении: {e}")
                logger.error(f"Ошибка при сохранении выжимки: {e}")

    except Exception as e:
        click.echo(f"❌ Ошибка при создании выжимки: {e}")
        logger.error(f"Ошибка суммаризации файла {file_path}: {e}")


@cli.command()
@click.option("--text", "-t", required=True, help="Текст для суммаризации")
@click.option("--model", "-m", default="doc2vec_model", help="Имя Doc2Vec модели")
@click.option("--sentences", "-s", default=5, help="Количество предложений в выжимке")
@click.option("--output", "-o", help="Файл для сохранения выжимки")
def summarize_text(
    text: str, model: str, sentences: int, output: Optional[str]
) -> None:
    """
    Создать выжимку из текста

    Args:
        text: Текст для суммаризации (строка)
        model: Имя Doc2Vec модели для улучшенной суммаризации
        sentences: Количество предложений в выжимке
        output: Путь для сохранения выжимки (опционально)
    """
    # Базовая валидация входного текста
    if not text or len(text.strip()) < 100:
        click.echo("❌ Текст слишком короткий для суммаризации (минимум 100 символов)")
        return

    # Проверяем количество предложений в исходном тексте
    temp_processor = TextSummarizer()
    original_sentences = temp_processor.text_processor.split_into_sentences(text)

    if len(original_sentences) <= sentences:
        click.echo(
            f"⚠️ В тексте всего {len(original_sentences)} предложений, что меньше или равно запрошенному количеству ({sentences})"
        )
        click.echo("Выводим весь текст:")
        for i, sentence in enumerate(original_sentences, 1):
            click.echo(f"{i}. {sentence.strip()}")
        return

    # Загрузка модели Doc2Vec
    trainer = Doc2VecTrainer()
    loaded_model = trainer.load_model(model)

    if loaded_model is None:
        click.echo("⚠️ Модель Doc2Vec не найдена. Используется базовая суммаризация")
        summarizer = TextSummarizer()
    else:
        click.echo("✅ Используется продвинутая суммаризация с Doc2Vec")
        summarizer = TextSummarizer(loaded_model)

    logger.info("Создание выжимки текста")

    try:
        # Создание выжимки
        summary = summarizer.summarize_text(text, sentences_count=sentences)

        if not summary:
            click.echo("❌ Не удалось создать выжимку")
            return

        # Вывод результата
        click.echo("\n📄 Выжимка текста:")
        click.echo("=" * 60)

        for i, sentence in enumerate(summary, 1):
            click.echo(f"{i}. {sentence.strip()}")
            click.echo()  # Пустая строка между предложениями

        # Статистика суммаризации
        stats = summarizer.get_summary_statistics(text, summary)

        click.echo("📊 Статистика суммаризации:")
        click.echo("-" * 30)
        click.echo(f"  📑 Исходных предложений: {stats['original_sentences_count']}")
        click.echo(f"  📄 Предложений в выжимке: {stats['summary_sentences_count']}")
        click.echo(
            f"  📉 Коэффициент сжатия предложений: {stats['compression_ratio']:.1%}"
        )
        click.echo(f"  🔤 Исходных символов: {stats['original_chars_count']:,}")
        click.echo(f"  ✂️ Символов в выжимке: {stats['summary_chars_count']:,}")
        click.echo(f"  📊 Сокращение текста: {stats['chars_compression_ratio']:.1%}")

        # Сохранение в файл
        if output:
            output_path = Path(output)
            try:
                with open(output_path, "w", encoding="utf-8") as f:
                    f.write("Выжимка текста\n")
                    f.write("=" * 60 + "\n\n")

                    for i, sentence in enumerate(summary, 1):
                        f.write(f"{i}. {sentence.strip()}\n\n")

                    # Добавляем статистику
                    f.write("\n" + "=" * 60 + "\n")
                    f.write("СТАТИСТИКА СУММАРИЗАЦИИ\n")
                    f.write("=" * 60 + "\n")
                    f.write(
                        f"Исходных предложений: {stats['original_sentences_count']}\n"
                    )
                    f.write(
                        f"Предложений в выжимке: {stats['summary_sentences_count']}\n"
                    )
                    f.write(
                        f"Коэффициент сжатия предложений: {stats['compression_ratio']:.1%}\n"
                    )
                    f.write(f"Исходных символов: {stats['original_chars_count']:,}\n")
                    f.write(f"Символов в выжимке: {stats['summary_chars_count']:,}\n")
                    f.write(
                        f"Сокращение текста: {stats['chars_compression_ratio']:.1%}\n"
                    )

                click.echo(f"💾 Выжимка сохранена в: {output_path}")

            except Exception as e:
                click.echo(f"❌ Ошибка при сохранении: {e}")
                logger.error(f"Ошибка при сохранении выжимки: {e}")

    except Exception as e:
        click.echo(f"❌ Ошибка при создании выжимки: {e}")
        logger.error(f"Ошибка суммаризации текста: {e}")


@cli.command()
@click.option("--documents", "-d", required=True, help="Путь к папке с документами")
@click.option("--model", "-m", default="doc2vec_model", help="Имя Doc2Vec модели")
@click.option(
    "--sentences",
    "-s",
    default=3,
    help="Количество предложений в выжимке каждого файла",
)
@click.option(
    "--min-length", "-l", default=15, help="Минимальная длина предложения в символах"
)
@click.option(
    "--min-words", "-w", default=5, help="Минимальное количество слов в предложении"
)
@click.option(
    "--no-filter", is_flag=True, help="Отключить фильтрацию коротких предложений"
)
@click.option("--output-dir", "-o", help="Папка для сохранения выжимок")
@click.option(
    "--extensions", default="pdf,docx,doc", help="Расширения файлов (через запятую)"
)
@click.option(
    "--max-files",
    default=0,
    help="Максимальное количество файлов для обработки (0 = все)",
)
def summarize_batch(
    documents: str,
    model: str,
    sentences: int,
    min_length: int,
    min_words: int,
    no_filter: bool,
    output_dir: Optional[str],
    extensions: str,
    max_files: int,
) -> None:
    """
    Создать выжимки для всех документов в папке с фильтрацией

    Args:
        documents: Путь к папке с документами
        model: Имя Doc2Vec модели
        sentences: Количество предложений в каждой выжимке
        min_length: Минимальная длина предложения
        min_words: Минимальное количество слов
        no_filter: Отключить фильтрацию
        output_dir: Папка для сохранения выжимок
        extensions: Обрабатываемые расширения файлов
        max_files: Максимальное количество файлов
    """
    documents_path = Path(documents)
    if not documents_path.exists():
        click.echo(f"❌ Папка не найдена: {documents_path}")
        return

    # Загрузка модели Doc2Vec
    trainer = Doc2VecTrainer()
    loaded_model = trainer.load_model(model)

    if loaded_model is None:
        click.echo("⚠️ Модель Doc2Vec не найдена. Используется базовая суммаризация")
        summarizer = TextSummarizer()
    else:
        click.echo("✅ Используется продвинутая суммаризация с Doc2Vec")
        summarizer = TextSummarizer(loaded_model)

    # Настройка фильтрации
    if not no_filter:
        summarizer.min_summary_sentence_length = min_length
        summarizer.min_words_in_sentence = min_words
        click.echo(f"📏 Фильтрация: минимум {min_length} символов и {min_words} слов")
    else:
        summarizer.min_summary_sentence_length = 1
        summarizer.min_words_in_sentence = 1
        click.echo("📋 Фильтрация отключена")

    # Подготовка расширений
    allowed_extensions = {f".{ext.strip().lower()}" for ext in extensions.split(",")}
    click.echo(f"🔍 Поиск файлов с расширениями: {allowed_extensions}")

    # Поиск файлов
    all_files = []
    for file_path in documents_path.rglob("*"):
        if file_path.is_file() and file_path.suffix.lower() in allowed_extensions:
            all_files.append(file_path)

    if not all_files:
        click.echo(f"❌ Файлы с расширениями {allowed_extensions} не найдены")
        return

    # Ограничение количества файлов
    if max_files > 0 and len(all_files) > max_files:
        all_files = all_files[:max_files]
        click.echo(f"📁 Ограничено до {max_files} файлов из найденных")

    click.echo(f"📁 Найдено файлов для обработки: {len(all_files)}")

    # Подготовка папки для выходных файлов
    if output_dir:
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True, parents=True)
        click.echo(f"💾 Выжимки будут сохранены в: {output_path}")
    else:
        click.echo("📺 Выжимки будут выведены только в консоль")

    successful = 0
    failed = 0
    filtered_out = 0

    # Обработка файлов
    for i, file_path in enumerate(all_files, 1):
        click.echo(f"\n🔄 Обработка {i}/{len(all_files)}: {file_path.name}")

        try:
            # Создание выжимки
            summary = summarizer.summarize_file(
                str(file_path), sentences_count=sentences
            )

            if not summary:
                click.echo(
                    "   ⚠️ Не удалось создать выжимку (возможно, все предложения слишком короткие)"
                )
                filtered_out += 1
                continue

            # Краткий вывод в консоль
            click.echo(f"   ✅ Создана выжимка: {len(summary)} предложений")

            # Показываем первое предложение как превью
            if summary:
                preview = (
                    summary[0][:100] + "..." if len(summary[0]) > 100 else summary[0]
                )
                click.echo(f"   👁️ Превью: {preview}")

            # Сохранение в файл
            if output_dir:
                summary_filename = f"{file_path.stem}_summary.txt"
                summary_path = output_path / summary_filename

                try:
                    with open(summary_path, "w", encoding="utf-8") as f:
                        f.write(f"Выжимка файла: {file_path.name}\n")
                        f.write(f"Исходный путь: {file_path}\n")
                        f.write(f"Количество предложений: {len(summary)}\n")
                        if not no_filter:
                            f.write(
                                f"Фильтрация: мин. {min_length} символов, {min_words} слов\n"
                            )
                        f.write("=" * 60 + "\n\n")

                        for j, sentence in enumerate(summary, 1):
                            f.write(f"{j}. {sentence.strip()}\n\n")

                    click.echo(f"   💾 Сохранено: {summary_filename}")
                except Exception as save_error:
                    click.echo(f"   ❌ Ошибка сохранения: {save_error}")
                    failed += 1
                    continue

            successful += 1

        except Exception as e:
            click.echo(f"   ❌ Ошибка при обработке: {e}")
            logger.error(f"Ошибка при обработке {file_path}: {e}")
            failed += 1

    # Итоговая статистика
    click.echo("\n📊 Итоговая статистика пакетной суммаризации:")
    click.echo("=" * 50)
    click.echo(f"  ✅ Успешно обработано: {successful}")
    click.echo(f"  ⚠️ Отфильтровано (короткие предложения): {filtered_out}")
    click.echo(f"  ❌ Ошибок: {failed}")
    click.echo(f"  📁 Всего файлов: {len(all_files)}")
    click.echo(f"  📈 Процент успеха: {(successful / len(all_files) * 100):.1f}%")

    if output_dir and successful > 0:
        click.echo(f"  💾 Выжимки сохранены в: {output_path}")

    if filtered_out > 0 and not no_filter:
        click.echo(
            "\n💡 Совет: Используйте --no-filter или уменьшите --min-length для обработки файлов с короткими предложениями"
        )


@cli.command()
def status():
    """Проверка статуса выполняющихся задач"""

    tasks = task_manager.get_all_tasks()

    if not tasks:
        click.echo("📭 Активных задач нет")
        return

    click.echo("📋 Статус задач:")
    click.echo("=" * 60)

    for task in tasks:
        status_icon = {
            "pending": "⏳",
            "running": "🔄",
            "completed": "✅",
            "failed": "❌",
            "cancelled": "⏹️",
        }.get(task.status.value, "❓")

        click.echo(f"{status_icon} {task.name}")
        click.echo(f"   ID: {task.id}")
        click.echo(f"   Статус: {task.status.value}")

        if task.progress > 0:
            progress_bar = "█" * int(task.progress * 20) + "░" * (
                20 - int(task.progress * 20)
            )
            click.echo(f"   Прогресс: [{progress_bar}] {task.progress:.1%}")

        if task.duration:
            click.echo(f"   Время: {task.duration:.1f}с")

        if task.error:
            click.echo(f"   Ошибка: {task.error}")

        click.echo()


@cli.command()
@click.argument("task_id")
def cancel(task_id: str):
    """Отмена задачи"""

    if task_manager.cancel_task(task_id):
        click.echo(f"✅ Задача {task_id} отменена")
    else:
        click.echo(f"❌ Не удалось отменить задачу {task_id}")


@cli.command()
@click.option(
    "--max-keep", default=50, help="Максимальное количество задач для хранения"
)
def cleanup(max_keep: int):
    """Очистка завершенных задач"""

    before_count = len(task_manager.get_all_tasks())
    task_manager.cleanup_finished_tasks(max_keep)
    after_count = len(task_manager.get_all_tasks())

    removed = before_count - after_count
    click.echo(f"🧹 Удалено {removed} завершенных задач")


@cli.command()
@click.option("--documents", "-d", help="Путь к папке с документами")
@click.option("--output", "-o", help="Файл для сохранения отчета")
def system_info(documents: Optional[str], output: Optional[str]):
    """Системная информация и диагностика"""

    info_lines = []

    # Системная информация
    system_info = performance_monitor.get_system_info()
    info_lines.extend(
        [
            "🖥️ Системная информация:",
            f"   CPU: {system_info['cpu_count']} ядер, загрузка {system_info['cpu_percent']}%",
            f"   ОЗУ: {system_info['memory_available']:.1f}/{system_info['memory_total']:.1f} ГБ свободно",
            f"   Диск: {100 - system_info['disk_usage']:.1f}% свободно",
            "",
        ]
    )

    # Статус SpaCy
    spacy_info = check_spacy_model_availability()
    spacy_status = "✅ Готов" if spacy_info["model_loadable"] else "❌ Не готов"
    info_lines.extend(
        [
            "🧠 Языковая модель:",
            f"   SpaCy: {spacy_status}",
            f"   Модель: {SPACY_MODEL}",
            "",
        ]
    )

    # Информация о документах
    if documents:
        try:
            docs_path = Path(documents)
            if docs_path.exists():
                file_extractor = FileExtractor()
                found_files = file_extractor.find_documents(docs_path)

                # Валидация файлов
                valid_files = 0
                invalid_files = 0
                total_size = 0

                for file_path in found_files:
                    validation = FileValidator.validate_document_file(file_path)
                    if validation["valid"]:
                        valid_files += 1
                        total_size += validation["file_info"]["size"]
                    else:
                        invalid_files += 1

                info_lines.extend(
                    [
                        "📁 Анализ документов:",
                        f"   Всего найдено: {len(found_files)}",
                        f"   Валидных: {valid_files}",
                        f"   С ошибками: {invalid_files}",
                        f"   Общий размер: {total_size / 1024 / 1024:.1f} МБ",
                        "",
                    ]
                )

        except Exception as e:
            info_lines.extend(["📁 Анализ документов:", f"   ❌ Ошибка: {e}", ""])

    # Производительность
    if performance_monitor.metrics:
        info_lines.extend(
            [
                "⚡ Последние операции:",
            ]
        )

        for op_name, metrics in list(performance_monitor.metrics.items())[-5:]:
            info_lines.append(f"   {op_name}: {metrics['duration']:.2f}с")

        info_lines.append("")

    # Активные задачи
    running_tasks = task_manager.get_running_tasks()
    if running_tasks:
        info_lines.extend(
            [
                "🔄 Активные задачи:",
            ]
        )

        for task in running_tasks:
            info_lines.append(f"   {task.name}: {task.progress:.1%}")

        info_lines.append("")

    report = "\n".join(info_lines)

    # Вывод в консоль
    click.echo(report)

    # Сохранение в файл
    if output:
        try:
            with open(output, "w", encoding="utf-8") as f:
                f.write(f"Системный отчет - {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("=" * 60 + "\n\n")
                f.write(report)

            click.echo(f"💾 Отчет сохранен в: {output}")

        except Exception as e:
            click.echo(f"❌ Ошибка сохранения отчета: {e}")


@cli.command()
@click.option("--show", is_flag=True, help="Показать текущую конфигурацию")
@click.option("--reset", is_flag=True, help="Сбросить к значениям по умолчанию")
@click.option("--reload", is_flag=True, help="Перезагрузить конфигурацию из файла")
@click.option(
    "--set", nargs=2, multiple=True, help="Установить параметр: --set key value"
)
def config(show: bool, reset: bool, reload: bool, set: tuple):
    """Управление конфигурацией приложения"""

    from semantic_search.config import config_manager

    if reset:
        if click.confirm(
            "Вы уверены, что хотите сбросить конфигурацию к значениям по умолчанию?"
        ):
            config_manager.reset_to_defaults()
            click.echo("✅ Конфигурация сброшена к значениям по умолчанию")
        else:
            click.echo("❌ Сброс отменен")
        return

    if reload:
        config_manager.reload_config()
        click.echo("✅ Конфигурация перезагружена из файла")

    if set:
        for key, value in set:
            # Пытаемся преобразовать значение в правильный тип
            try:
                # Числа
                if value.isdigit():
                    value = int(value)
                elif value.replace(".", "", 1).isdigit():
                    value = float(value)
                # Булевы значения
                elif value.lower() in ("true", "false"):
                    value = value.lower() == "true"
                # Числа с подчеркиваниями
                elif "_" in value and value.replace("_", "").isdigit():
                    value = int(value.replace("_", ""))

            except Exception:
                pass  # Оставляем как строку

            # Определяем секцию и параметр
            if "." in key:
                section, param = key.split(".", 1)
                config_manager.update_config(**{section: {param: value}})
                click.echo(f"✅ Установлено: {section}.{param} = {value}")
            else:
                click.echo("❌ Неверный формат ключа. Используйте: section.parameter")

    if show or (not reset and not reload and not set):
        # Показываем текущую конфигурацию
        current_config = config_manager.config

        click.echo("\n📋 Текущая конфигурация:")
        click.echo("=" * 60)

        # Обработка текста
        click.echo("\n📝 Обработка текста (text_processing):")
        for key, value in current_config.text_processing.items():
            if isinstance(value, int) and value > 1000:
                click.echo(f"  {key}: {value:,}")
            else:
                click.echo(f"  {key}: {value}")

        # Doc2Vec
        click.echo("\n🧠 Параметры Doc2Vec (doc2vec):")
        for key, value in current_config.doc2vec.items():
            click.echo(f"  {key}: {value}")

        # Поиск
        click.echo("\n🔍 Параметры поиска (search):")
        for key, value in current_config.search.items():
            click.echo(f"  {key}: {value}")

        # GUI
        click.echo("\n💻 Параметры интерфейса (gui):")
        for key, value in current_config.gui.items():
            click.echo(f"  {key}: {value}")

        # Суммаризация
        click.echo("\n📄 Параметры суммаризации (summarization):")
        for key, value in current_config.summarization.items():
            click.echo(f"  {key}: {value}")

        click.echo("\n💡 Примеры изменения параметров:")
        click.echo(
            "  semantic-search-cli config --set text_processing.max_text_length 10000000"
        )
        click.echo("  semantic-search-cli config --set doc2vec.vector_size 200")
        click.echo("  semantic-search-cli config --set search.default_top_k 20")


"""Добавить в main.py после других CLI команд"""


@cli.command()
@click.option("--model", "-m", default="doc2vec_model", help="Имя Doc2Vec модели")
@click.option("--openai-key", envvar="OPENAI_API_KEY", help="OpenAI API key")
@click.option(
    "--test-cases",
    type=click.Choice(["quick", "standard", "extended"]),
    default="standard",
    help="Набор тестовых случаев",
)
@click.option("--output-dir", "-o", help="Директория для сохранения результатов")
def evaluate(model: str, openai_key: str, test_cases: str, output_dir: Optional[str]):
    """
    Сравнение Doc2Vec с OpenAI embeddings

    Args:
        model: Имя Doc2Vec модели для оценки
        openai_key: API ключ OpenAI (или из OPENAI_API_KEY)
        test_cases: Размер набора тестов (quick/standard/extended)
        output_dir: Директория для результатов
    """
    if not openai_key:
        click.echo("❌ OpenAI API key не найден")
        click.echo(
            "Установите переменную окружения OPENAI_API_KEY или используйте --openai-key"
        )
        return

    # Загрузка модели
    click.echo(f"📂 Загрузка модели {model}...")
    trainer = Doc2VecTrainer()
    loaded_model = trainer.load_model(model)

    if not loaded_model:
        click.echo(f"❌ Не удалось загрузить модель '{model}'")
        return

    if not trainer.corpus_info:
        click.echo("❌ Информация о корпусе не найдена")
        return

    # Импорт модулей оценки
    from semantic_search.core.search_engine import SemanticSearchEngine
    from semantic_search.evaluation.baselines import (
        Doc2VecSearchAdapter,
        OpenAISearchBaseline,
    )
    from semantic_search.evaluation.comparison import SearchComparison

    # Создание поискового движка
    search_engine = SemanticSearchEngine(loaded_model, trainer.corpus_info)

    # Создание сравнения
    comparison = SearchComparison()

    # Получение тестовых случаев
    click.echo("🧪 Подготовка тестовых случаев...")
    default_cases = comparison.create_default_test_cases()

    if test_cases == "quick":
        test_cases_list = default_cases[:3]
        click.echo("   Быстрый тест: 3 запроса")
    elif test_cases == "extended":
        # Добавляем дополнительные тесты
        from semantic_search.evaluation.comparison import QueryTestCase

        extra_cases = [
            QueryTestCase(
                query="классификация изображений CNN",
                relevant_docs={"cnn_tutorial.pdf", "image_classification.pdf"},
                relevance_scores={"cnn_tutorial.pdf": 3, "image_classification.pdf": 3},
            ),
            QueryTestCase(
                query="регуляризация dropout L1 L2",
                relevant_docs={"regularization.pdf", "overfitting.pdf"},
                relevance_scores={"regularization.pdf": 3, "overfitting.pdf": 2},
            ),
            QueryTestCase(
                query="word embeddings word2vec GloVe",
                relevant_docs={"word2vec_paper.pdf", "embeddings_tutorial.pdf"},
                relevance_scores={
                    "word2vec_paper.pdf": 3,
                    "embeddings_tutorial.pdf": 3,
                },
            ),
            QueryTestCase(
                query="precision recall F1 score ROC AUC",
                relevant_docs={"ml_metrics.pdf", "evaluation_methods.pdf"},
                relevance_scores={"ml_metrics.pdf": 3, "evaluation_methods.pdf": 3},
            ),
            QueryTestCase(
                query="backpropagation gradient descent",
                relevant_docs={"backpropagation.pdf", "optimization.pdf"},
                relevance_scores={"backpropagation.pdf": 3, "optimization.pdf": 2},
            ),
        ]
        test_cases_list = default_cases + extra_cases
        click.echo("   Расширенный тест: 10 запросов")
    else:  # standard
        test_cases_list = default_cases
        click.echo("   Стандартный тест: 5 запросов")

    comparison.test_cases = test_cases_list

    # Создание адаптеров
    click.echo("\n🔧 Инициализация методов поиска...")
    doc2vec_adapter = Doc2VecSearchAdapter(search_engine, trainer.corpus_info)

    try:
        openai_baseline = OpenAISearchBaseline(api_key=openai_key)
        click.echo("✅ OpenAI baseline инициализирован")
    except Exception as e:
        click.echo(f"❌ Ошибка инициализации OpenAI: {e}")
        return

    # Подготовка документов для индексации
    click.echo("\n📚 Подготовка документов для OpenAI...")
    documents = []
    max_docs = min(50, len(trainer.corpus_info))  # Ограничиваем для экономии

    for i, (tokens, doc_id, metadata) in enumerate(trainer.corpus_info[:max_docs]):
        # Восстанавливаем текст из токенов
        text = " ".join(tokens[:500])  # Берем первые 500 токенов
        documents.append((doc_id, text, metadata))

    click.echo(f"   Подготовлено {len(documents)} документов")

    # Индексация для OpenAI
    click.echo("\n🔄 Индексация документов через OpenAI API...")
    click.echo("   (это может занять несколько минут)")

    try:
        with click.progressbar(length=100, label="Индексация") as bar:
            # Используем callback для обновления прогресса
            original_index = openai_baseline.index

            def index_with_progress(docs):
                result = original_index(docs)
                bar.update(100)
                return result

            openai_baseline.index = index_with_progress
            openai_baseline.index(documents)
            openai_baseline.index = original_index

        click.echo("✅ Индексация завершена")
    except Exception as e:
        click.echo(f"❌ Ошибка индексации: {e}")
        return

    # Оценка методов
    click.echo("\n📊 Оценка методов...")

    # Doc2Vec
    click.echo("\n1️⃣ Оценка Doc2Vec...")
    doc2vec_results = comparison.evaluate_method(
        doc2vec_adapter, top_k=10, verbose=False
    )
    click.echo(f"   MAP: {doc2vec_results['aggregated']['MAP']:.3f}")
    click.echo(f"   MRR: {doc2vec_results['aggregated']['MRR']:.3f}")
    click.echo(
        f"   Среднее время запроса: {doc2vec_results['aggregated']['avg_query_time']:.3f}с"
    )

    # OpenAI
    click.echo("\n2️⃣ Оценка OpenAI embeddings...")
    openai_results = comparison.evaluate_method(
        openai_baseline, top_k=10, verbose=False
    )
    click.echo(f"   MAP: {openai_results['aggregated']['MAP']:.3f}")
    click.echo(f"   MRR: {openai_results['aggregated']['MRR']:.3f}")
    click.echo(
        f"   Среднее время запроса: {openai_results['aggregated']['avg_query_time']:.3f}с"
    )

    # Сравнение результатов
    click.echo("\n📈 СРАВНЕНИЕ РЕЗУЛЬТАТОВ")
    click.echo("=" * 60)

    # MAP сравнение
    doc2vec_map = doc2vec_results["aggregated"]["MAP"]
    openai_map = openai_results["aggregated"]["MAP"]
    map_improvement = (
        ((doc2vec_map - openai_map) / openai_map * 100) if openai_map > 0 else 0
    )

    click.echo("\n📊 Mean Average Precision (MAP):")
    click.echo(f"   Doc2Vec: {doc2vec_map:.3f}")
    click.echo(f"   OpenAI:  {openai_map:.3f}")

    if map_improvement > 0:
        click.echo(f"   ✅ Doc2Vec превосходит OpenAI на {map_improvement:.1f}%")
    else:
        click.echo(f"   ❌ OpenAI превосходит Doc2Vec на {-map_improvement:.1f}%")

    # Скорость
    doc2vec_time = doc2vec_results["aggregated"]["avg_query_time"]
    openai_time = openai_results["aggregated"]["avg_query_time"]
    speed_ratio = openai_time / doc2vec_time if doc2vec_time > 0 else float("inf")

    click.echo("\n⚡ Скорость поиска:")
    click.echo(f"   Doc2Vec: {doc2vec_time:.3f}с на запрос")
    click.echo(f"   OpenAI:  {openai_time:.3f}с на запрос")
    click.echo(f"   ✅ Doc2Vec быстрее в {speed_ratio:.1f} раз")

    # Другие метрики
    click.echo("\n📏 Дополнительные метрики:")

    for metric in ["precision@10", "recall@10", "f1@10"]:
        doc2vec_val = doc2vec_results["aggregated"].get(f"avg_{metric}", 0)
        openai_val = openai_results["aggregated"].get(f"avg_{metric}", 0)
        click.echo(f"   {metric.upper()}:")
        click.echo(f"      Doc2Vec: {doc2vec_val:.3f}")
        click.echo(f"      OpenAI:  {openai_val:.3f}")

    # Экономическая эффективность
    click.echo("\n💰 Экономическая эффективность:")

    # Расчет примерной стоимости
    queries_per_day = 1000
    avg_tokens_per_query = 50
    openai_cost_per_1k_tokens = 0.0001  # $0.0001 за 1K токенов для ada-002

    daily_queries_cost = (
        queries_per_day * avg_tokens_per_query / 1000
    ) * openai_cost_per_1k_tokens
    # Стоимость индексации (примерно 200 токенов на документ)
    indexing_cost = (len(documents) * 200 / 1000) * openai_cost_per_1k_tokens

    monthly_cost = daily_queries_cost * 30 + indexing_cost
    yearly_cost = monthly_cost * 12

    click.echo(f"   При {queries_per_day} запросов в день:")
    click.echo(
        f"   - Стоимость OpenAI: ~${monthly_cost:.2f}/месяц (${yearly_cost:.2f}/год)"
    )
    click.echo("   - Стоимость Doc2Vec: $0 (после единоразового обучения)")
    click.echo(f"   - 💵 Экономия: ${yearly_cost:.2f} в год")

    # Генерация отчетов
    click.echo("\n📄 Генерация отчетов и графиков...")

    # Определяем директорию для результатов
    if output_dir:
        results_dir = Path(output_dir)
    else:
        from semantic_search.config import EVALUATION_RESULTS_DIR

        results_dir = EVALUATION_RESULTS_DIR

    results_dir.mkdir(exist_ok=True, parents=True)

    # Сравнительная таблица
    comparison.compare_methods([doc2vec_adapter, openai_baseline], save_results=True)

    # Генерация графиков
    try:
        comparison.plot_comparison(save_plots=True)
        click.echo("✅ Графики сохранены")
    except Exception as e:
        click.echo(f"⚠️ Не удалось создать графики: {e}")

    # Текстовый отчет
    report_path = results_dir / "comparison_report.txt"
    comparison.generate_report(report_path)

    click.echo(f"\n✅ Результаты сохранены в: {results_dir}")
    click.echo("   📊 comparison_results.csv - таблица с метриками")
    click.echo("   📝 comparison_report.txt - текстовый отчет")
    click.echo("   📈 plots/ - графики сравнения")
    click.echo("   🗂️ detailed_results.json - детальные результаты")

    # Основные выводы
    click.echo("\n🎯 ОСНОВНЫЕ ВЫВОДЫ:")
    click.echo("=" * 60)

    if map_improvement > 0:
        click.echo(
            f"✅ Doc2Vec показывает ЛУЧШЕЕ качество поиска (+{map_improvement:.1f}% MAP)"
        )
        click.echo("   на специализированном корпусе документов")
    else:
        click.echo(
            f"⚠️ OpenAI показывает лучшее качество поиска (+{-map_improvement:.1f}% MAP)"
        )
        click.echo("   Рекомендуется дообучить модель Doc2Vec")

    click.echo(f"\n✅ Doc2Vec работает ЗНАЧИТЕЛЬНО БЫСТРЕЕ (в {speed_ratio:.1f} раз)")
    click.echo("   и не требует интернет-соединения")

    click.echo(f"\n✅ Doc2Vec ЭКОНОМИЧЕСКИ ВЫГОДНЕЕ (экономия ${yearly_cost:.0f}/год)")
    click.echo("   при регулярном использовании")

    click.echo("\n📌 Рекомендация: Doc2Vec оптимален для:")
    click.echo("   • Специализированных корпусов документов")
    click.echo("   • Высокой нагрузки (много запросов)")
    click.echo("   • Работы без интернета")
    click.echo("   • Конфиденциальных данных")


def cli_mode():
    """Запуск CLI режима"""
    cli()


if __name__ == "__main__":
    if len(sys.argv) > 1:
        cli_mode()
    else:
        main()


========================================
FILE: src\semantic_search\core\__init__.py
========================================
"""Основные модули для работы с документами и поиском"""

from .doc2vec_trainer import Doc2VecTrainer
from .document_processor import DocumentProcessor, ProcessedDocument
from .search_engine import SearchResult, SemanticSearchEngine
from .text_summarizer import TextSummarizer

__all__ = [
    "Doc2VecTrainer",
    "DocumentProcessor",
    "ProcessedDocument",
    "SemanticSearchEngine",
    "SearchResult",
    "TextSummarizer",
]


========================================
FILE: src\semantic_search\core\doc2vec_trainer.py
========================================
"""Модуль для обучения модели Doc2Vec"""

from __future__ import annotations

import json
import pickle
from pathlib import Path
from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple

from loguru import logger

if TYPE_CHECKING:
    from gensim.models.doc2vec import Doc2Vec, TaggedDocument
try:
    from gensim.models.doc2vec import Doc2Vec, TaggedDocument

    GENSIM_AVAILABLE = True
except ImportError:
    logger.error("Gensim не установлен. Установите: pip install gensim")
    GENSIM_AVAILABLE = False

from semantic_search.config import DOC2VEC_CONFIG, MODELS_DIR


class Doc2VecTrainer:
    """Класс для обучения и управления моделью Doc2Vec"""

    def __init__(self):
        self.model: Optional[Doc2Vec] = None
        self.config = DOC2VEC_CONFIG
        self.corpus_info: Optional[List[Tuple[List[str], str, dict]]] = None
        self.training_metadata: Dict[str, Any] = {}
        self.documents_base_path: Optional[Path] = None

    def create_tagged_documents(
        self, corpus: List[Tuple[List[str], str, dict]]
    ) -> List[TaggedDocument]:
        """
        Создание TaggedDocument объектов для gensim

        Args:
            corpus: Список кортежей (tokens, doc_id, metadata)

        Returns:
            Список TaggedDocument объектов
        """
        if not GENSIM_AVAILABLE:
            raise ImportError("Gensim не доступен")

        tagged_docs = [
            TaggedDocument(words=tokens, tags=[doc_id]) for tokens, doc_id, _ in corpus
        ]
        logger.info(f"Создано {len(tagged_docs)} TaggedDocument объектов")
        return tagged_docs

    def _get_training_params(
        self,
        vector_size: Optional[int],
        window: Optional[int],
        min_count: Optional[int],
        epochs: Optional[int],
        workers: Optional[int],
        dm: Optional[int],
        negative: Optional[int],
        hs: Optional[int],
        sample: Optional[float],
        dm_concat: Optional[int] = None,
        dm_mean: Optional[int] = None,
        alpha: Optional[float] = None,
        min_alpha: Optional[float] = None,
    ) -> Dict[str, Any]:
        """Получить параметры для обучения с поддержкой новых параметров"""
        params = {
            "vector_size": vector_size or self.config["vector_size"],
            "window": window or self.config["window"],
            "min_count": min_count or self.config["min_count"],
            "epochs": epochs or self.config["epochs"],
            "workers": workers or self.config["workers"],
            "seed": self.config["seed"],
            "dm": dm if dm is not None else self.config.get("dm", 1),
            "negative": negative
            if negative is not None
            else self.config.get("negative", 10),
            "hs": hs if hs is not None else self.config.get("hs", 0),
            "sample": sample if sample is not None else self.config.get("sample", 1e-5),
        }

        # Новые параметры
        if dm_concat is not None:
            params["dm_concat"] = dm_concat
        elif "dm_concat" in self.config:
            params["dm_concat"] = self.config["dm_concat"]

        if dm_mean is not None:
            params["dm_mean"] = dm_mean
        elif "dm_mean" in self.config:
            params["dm_mean"] = self.config["dm_mean"]

        if alpha is not None:
            params["alpha"] = alpha
        elif "alpha" in self.config:
            params["alpha"] = self.config["alpha"]

        if min_alpha is not None:
            params["min_alpha"] = min_alpha
        elif "min_alpha" in self.config:
            params["min_alpha"] = self.config["min_alpha"]

        return params

    def _train_standard(
        self,
        corpus: List[Tuple[List[str], str, dict]],
        vector_size: Optional[int] = None,
        window: Optional[int] = None,
        min_count: Optional[int] = None,
        epochs: Optional[int] = None,
        workers: Optional[int] = None,
        dm: Optional[int] = None,
        negative: Optional[int] = None,
        hs: Optional[int] = None,
        sample: Optional[float] = None,
    ) -> Optional[Doc2Vec]:
        """
        Обучение модели Doc2Vec

        Args:
            corpus: Подготовленный корпус
            vector_size: Размерность векторов (по умолчанию из config)
            window: Размер окна контекста
            min_count: Минимальная частота слова
            epochs: Количество эпох обучения
            workers: Количество потоков
            dm: Distributed Memory (1) или Distributed Bag of Words (0)
            negative: Размер negative sampling (если hs=0)
            hs: Использовать Hierarchical Softmax (1) или negative sampling (0)
            sample: Порог для downsampling высокочастотных слов

        Returns:
            Обученная модель Doc2Vec или None при ошибке
        """
        if not GENSIM_AVAILABLE:
            logger.error("Gensim не доступен для обучения модели")
            return None
        if not corpus:
            logger.error("Корпус пуст, обучение невозможно")
            return None

        params = self._get_training_params(
            vector_size, window, min_count, epochs, workers, dm, negative, hs, sample
        )

        logger.info("Подготовка данных для обучения...")
        tagged_docs = self.create_tagged_documents(corpus)

        logger.info("Начинаем обучение модели Doc2Vec с параметрами:")
        for k, v in params.items():
            logger.info(f"  {k}: {v}")

        try:
            model = Doc2Vec(tagged_docs, **params)

            self.model = model
            self.corpus_info = corpus

            logger.info("Обучение модели завершено успешно!")
            logger.info(
                f"Словарь содержит {len(model.wv.key_to_index)} уникальных слов"
            )
            logger.info(f"Обучено векторов документов: {len(model.dv)}")

            return model

        except Exception as e:
            logger.error(f"Ошибка при обучении модели: {e}")
            return None

    def train_model(
        self,
        corpus: List[Tuple[List[str], str, dict]],
        vector_size: Optional[int] = None,
        window: Optional[int] = None,
        min_count: Optional[int] = None,
        epochs: Optional[int] = None,
        workers: Optional[int] = None,
        dm: Optional[int] = None,
        negative: Optional[int] = None,
        hs: Optional[int] = None,
        sample: Optional[float] = None,
        dm_concat: Optional[int] = None,
        dm_mean: Optional[int] = None,
        alpha: Optional[float] = None,
        min_alpha: Optional[float] = None,
        preset: Optional[str] = None,
    ) -> Optional[Doc2Vec]:
        """
        Обучение модели Doc2Vec с оптимизацией под объём корпуса.

        Для небольших корпусов используется стандартное обучение.
        Для больших корпусов (> 10 000 документов) применяется поэпоховое обучение.

        Args:
            corpus: Подготовленный корпус, где каждый элемент — кортеж (токены, тег, метаданные)
            vector_size: Размерность векторов (по умолчанию из self.config)
            window: Размер окна контекста (по умолчанию из self.config)
            min_count: Минимальная частота слова (по умолчанию из self.config)
            epochs: Количество эпох обучения (по умолчанию из self.config)
            workers: Количество потоков (по умолчанию из self.config)
            dm: Distributed Memory (1) или Distributed Bag of Words (0)
            negative: Размер negative sampling
            hs: Использовать Hierarchical Softmax
            sample: Порог для downsampling
            preset: Использовать пресет настроек ('fast', 'balanced', 'quality')

        Returns:
            Обученная модель Doc2Vec или None при ошибке
        """
        if not GENSIM_AVAILABLE:
            logger.error("Gensim не доступен для обучения модели")
            return None
        if not corpus:
            logger.error("Корпус пуст, обучение невозможно")
            return None

        if preset and preset in self.config.get("doc2vec_presets", {}):
            preset_config: Dict = self.config["doc2vec_presets"][preset]
            logger.info(f"Используется пресет '{preset}'")

            # Применяем настройки пресета (если параметр не указан явно)
            vector_size = vector_size or preset_config.get("vector_size")
            window = window or preset_config.get("window")
            min_count = min_count or preset_config.get("min_count")
            epochs = epochs or preset_config.get("epochs")
            negative = negative or preset_config.get("negative")
            sample = sample or preset_config.get("sample")

        if len(corpus) > 10000:
            # Для больших корпусов - поэпоховое обучение
            logger.info("Большой корпус обнаружен. Используем поэпоховое обучение...")

            params = self._get_training_params(
                vector_size,
                window,
                min_count,
                epochs,
                workers,
                dm,
                negative,
                hs,
                sample,
            )

            logger.info("Подготовка данных для обучения...")
            tagged_docs = self.create_tagged_documents(corpus)

            logger.info("Создание модели с параметрами:")
            for k, v in params.items():
                logger.info(f"  {k}: {v}")

            try:
                model = Doc2Vec(**params)
                model.build_vocab(tagged_docs)
                logger.info(f"Словарь построен: {len(model.wv.key_to_index)} слов")

                for epoch in range(params["epochs"]):
                    logger.info(f"Эпоха {epoch + 1}/{params['epochs']}...")
                    model.train(
                        tagged_docs, total_examples=model.corpus_count, epochs=1
                    )

                self.model = model
                self.corpus_info = corpus

                logger.info("Обучение завершено успешно!")
                logger.info(
                    f"Словарь содержит {len(model.wv.key_to_index)} уникальных слов"
                )
                logger.info(f"Обучено векторов документов: {len(model.dv)}")

                return model

            except Exception as e:
                logger.error(f"Ошибка при обучении модели: {e}")
                return None
        else:
            # Для небольших корпусов используем стандартное обучение
            return self._train_standard(
                corpus,
                vector_size=vector_size,
                window=window,
                min_count=min_count,
                epochs=epochs,
                workers=workers,
                dm=dm,
                negative=negative,
                hs=hs,
                sample=sample,
            )

    def save_model(
        self, model: Optional[Doc2Vec] = None, model_name: str = "doc2vec_model"
    ) -> bool:
        """
        Сохранение модели на диск

        Args:
            model: Модель для сохранения (по умолчанию self.model)
            model_name: Имя файла модели

        Returns:
            True если сохранение успешно
        """
        model_to_save = model or self.model

        if model_to_save is None:
            logger.error("Нет модели для сохранения")
            return False

        # Проверяем, что имя модели не пустое
        if not model_name or model_name.strip() == "":
            logger.error("Имя модели не может быть пустым")
            return False

        try:
            if model_name.endswith(".model"):
                model_name = model_name[:-6]
            model_path = MODELS_DIR / f"{model_name}.model"
            model_to_save.save(str(model_path))

            # Сохраняем также информацию о корпусе
            if self.corpus_info:
                corpus_path = MODELS_DIR / f"{model_name}_corpus_info.pkl"
                with open(corpus_path, "wb") as f:
                    pickle.dump(self.corpus_info, f)
                logger.info(f"Информация о корпусе сохранена: {corpus_path}")

            # Сохраняем метаданные обучения если они есть
            if self.training_metadata:
                metadata_path = MODELS_DIR / f"{model_name}_metadata.json"
                with open(metadata_path, "w", encoding="utf-8") as f:
                    json.dump(self.training_metadata, f, indent=2, ensure_ascii=False)
                logger.info(f"Метаданные обучения сохранены: {metadata_path}")

            logger.info(f"Модель сохранена: {model_path}")
            return True

        except Exception as e:
            logger.error(f"Ошибка при сохранении модели: {e}")
            return False

    def load_model(self, model_name: str = "doc2vec_model") -> Optional[Doc2Vec]:
        """
        Загрузка модели с диска

        Args:
            model_name: Имя файла модели

        Returns:
            Загруженная модель Doc2Vec или None при ошибке
        """
        if not GENSIM_AVAILABLE:
            logger.error("Gensim не доступен для загрузки модели")
            return None

        if not model_name or model_name.strip() == "":
            logger.error("Имя модели не может быть пустым")
            return None

        try:
            if model_name.endswith(".model"):
                model_name = model_name[:-6]

            model_path = MODELS_DIR / f"{model_name}.model"

            if not model_path.exists():
                logger.error(f"Файл модели не найден: {model_path}")
                return None

            model = Doc2Vec.load(str(model_path))
            self.model = model

            # Загружаем информацию о корпусе если есть
            corpus_path = MODELS_DIR / f"{model_name}_corpus_info.pkl"
            if corpus_path.exists():
                try:
                    with open(corpus_path, "rb") as f:
                        self.corpus_info = pickle.load(f)
                    logger.info("Информация о корпусе загружена")
                except Exception as e:
                    logger.warning(f"Не удалось загрузить информацию о корпусе: {e}")
                    self.corpus_info = []

            # Загружаем метаданные обучения если есть
            metadata_path = MODELS_DIR / f"{model_name}_metadata.json"
            if metadata_path.exists():
                try:
                    with open(metadata_path, "r", encoding="utf-8") as f:
                        self.training_metadata = json.load(f)
                    logger.info("Метаданные обучения загружены")

                    # Загружаем базовый путь документов
                    if "documents_base_path" in self.training_metadata:
                        self.documents_base_path = Path(
                            self.training_metadata["documents_base_path"]
                        )
                        logger.info(
                            f"Базовый путь документов: {self.documents_base_path}"
                        )

                        # Проверяем существование пути
                        if not self.documents_base_path.exists():
                            logger.warning(
                                f"Базовый путь не существует: {self.documents_base_path}"
                            )
                    else:
                        logger.warning("Базовый путь документов не найден в метаданных")

                except Exception as e:
                    logger.warning(f"Не удалось загрузить метаданные обучения: {e}")
                    self.training_metadata = {}

            logger.info(f"Модель загружена: {model_path}")
            logger.info(f"Векторов документов: {len(model.dv)}")
            logger.info(f"Размерность векторов: {model.vector_size}")

            return model

        except Exception as e:
            logger.error(f"Ошибка при загрузке модели: {e}")
            return None

    def get_model_info(self) -> dict:
        """
        Получение информации о текущей модели

        Returns:
            Словарь с информацией о модели
        """
        if self.model is None:
            return {"status": "no_model"}

        info = {
            "status": "loaded",
            "vector_size": self.model.vector_size,
            "vocabulary_size": len(self.model.wv.key_to_index),
            "documents_count": len(self.model.dv),
            "window": self.model.window,
            "min_count": self.model.min_count,
            "epochs": self.model.epochs,
            "dm": self.model.dm,
            "dm_mean": getattr(self.model, "dm_mean", None),
            "dm_concat": getattr(self.model, "dm_concat", None),
            "negative": self.model.negative,
            "hs": self.model.hs,
            "sample": self.model.sample,
            "workers": self.model.workers,
        }

        info["training_time_formatted"] = self.training_metadata.get(
            "training_time_formatted", "Неизвестно"
        )

        info["training_date"] = self.training_metadata.get(
            "training_date", "Неизвестно"
        )

        return info


========================================
FILE: src\semantic_search\core\document_processor.py
========================================
"""Основной модуль для обработки документов"""

from pathlib import Path
from typing import Generator, List, NamedTuple

from loguru import logger

from semantic_search.config import TEXT_PROCESSING_CONFIG
from semantic_search.utils.file_utils import FileExtractor
from semantic_search.utils.text_utils import TextProcessor


class ProcessedDocument(NamedTuple):
    """Структура для хранения обрабатываемого документа"""

    file_path: Path
    relative_path: str
    raw_text: str
    tokens: List[str]
    metadata: dict


class DocumentProcessor:
    """Главный класс для обработки коллекции документов"""

    def __init__(self):
        self.file_extractor = FileExtractor()
        self.text_processor = TextProcessor()
        self.config = TEXT_PROCESSING_CONFIG

    def process_documents(
        self, root_path: Path
    ) -> Generator[ProcessedDocument, None, None]:
        """
        Основная функция обработки документов

        Args:
            root_path: Путь к корневой директории с документами

        Yields:
            ProcessedDocument объекты
        """

        if not root_path.exists():
            raise FileNotFoundError(f"Директория не найдена: {root_path}")

        if not root_path.is_dir():
            raise NotADirectoryError(f"Путь не является директорией: {root_path}")

        logger.info(f"Начинаем обработку документов в: {root_path}")

        file_paths = self.file_extractor.find_documents(root_path)

        if not file_paths:
            logger.warning("Документы не найдены")
            return

        processed_count = 0
        skipped_count = 0

        for i, file_path in enumerate(file_paths, 1):
            logger.info(f"Обработка {i}/{len(file_paths)}: {file_path.name}")

            try:
                file_size = file_path.stat().st_size
                max_file_size_bytes = (
                    self.config.get("max_file_size_mb", 100) * 1024 * 1024
                )

                if file_size > max_file_size_bytes:
                    logger.warning(
                        f"Файл слишком большой ({file_size / 1024 / 1024:.1f}MB): {file_path}"
                    )
                    skipped_count += 1
                    continue

                raw_text = self.file_extractor.extract_text(file_path)

                if len(raw_text) < self.config["min_text_length"]:
                    logger.warning(
                        f"Текст слишком короткий ({len(raw_text)} символов): {file_path}"
                    )
                    skipped_count += 1
                    continue

                max_text_length = self.config.get("max_text_length", 5_000_000)

                if len(raw_text) > max_text_length:
                    logger.info(
                        f"Текст обрезан с {len(raw_text):,} до {max_text_length:,} символов"
                    )
                    raw_text = raw_text[:max_text_length]

                tokens = self.text_processor.preprocess_text(raw_text)

                if len(tokens) < self.config["min_tokens_count"]:
                    logger.warning(f"Слишком мало токенов ({len(tokens)}): {file_path}")
                    skipped_count += 1
                    continue

                relative_path = str(file_path.relative_to(root_path))
                relative_path = relative_path.replace("\\", "/")

                metadata = {
                    "file_size": file_path.stat().st_size,
                    "extension": file_path.suffix,
                    "tokens_count": len(tokens),
                    "text_length": len(raw_text),
                }

                processed_count += 1
                yield ProcessedDocument(
                    file_path=file_path,
                    relative_path=relative_path,
                    raw_text=raw_text,
                    tokens=tokens,
                    metadata=metadata,
                )

            except PermissionError:
                logger.error(f"Нет доступа к файлу: {file_path}")
                skipped_count += 1
                continue
            except Exception as e:
                logger.error(f"Ошибка при обработке {file_path}: {e}")
                skipped_count += 1
                continue

        logger.info(
            f"Обработка завершена. Успешно: {processed_count}, Пропущено: {skipped_count}"
        )


========================================
FILE: src\semantic_search\core\search_engine.py
========================================
"""Модуль поискового движка (ИСПРАВЛЕННАЯ ВЕРСИЯ)"""

from __future__ import annotations

import json
from pathlib import Path
from typing import TYPE_CHECKING, Any, Dict, List, Optional

if TYPE_CHECKING:
    from gensim.models.doc2vec import Doc2Vec
from loguru import logger

from semantic_search.config import CACHE_DIR, SEARCH_CONFIG
from semantic_search.utils.cache_manager import CacheManager
from semantic_search.utils.text_utils import TextProcessor


class SearchResult:
    """Класс для представления результата поиска"""

    def __init__(self, doc_id: str, similarity: float, metadata: Optional[Dict] = None):
        self.doc_id = doc_id
        self.similarity = similarity
        self.metadata = metadata or {}
        self.file_path = Path(doc_id)  # doc_id это относительный путь

    def __repr__(self):
        return f"SearchResult(doc_id='{self.doc_id}', similarity={self.similarity:.3f})"


class SemanticSearchEngine:
    """Класс для семантического поиска по документам"""

    def __init__(
        self,
        model: Optional[Doc2Vec] = None,
        corpus_info: Optional[List] = None,
        documents_base_path: Optional[Path] = None,
    ):
        self.model = model
        self.corpus_info = corpus_info or []
        self.documents_base_path = documents_base_path  # Сохраняем базовый путь
        self.text_processor = TextProcessor()
        self.config = SEARCH_CONFIG
        self.cache_manager = CacheManager(CACHE_DIR)

        # Создаем индекс метаданных для быстрого доступа
        self._metadata_index = dict()
        if self.corpus_info:
            for tokens, doc_id, metadata in self.corpus_info:
                self._metadata_index[doc_id] = metadata

        if self.documents_base_path:
            logger.info(
                f"SearchEngine инициализирован с базовым путем: {self.documents_base_path}"
            )

    def set_model(
        self,
        model: Doc2Vec,
        corpus_info: Optional[List] = None,
        documents_base_path: Optional[Path] = None,
    ):
        """
        Установка модели для поиска

        Args:
            model: Обученная модель Doc2Vec
            corpus_info: Информация о корпусе
            documents_base_path: Базовый путь документов
        """
        self.model = model
        if corpus_info:
            self.corpus_info = corpus_info
            self._metadata_index = dict()
            for tokens, doc_id, metadata in corpus_info:
                self._metadata_index[doc_id] = metadata

        if documents_base_path:
            self.documents_base_path = documents_base_path
            logger.info(f"Установлен базовый путь: {self.documents_base_path}")

        logger.info("Поисковая модель установлена")

    def _search_base(
        self,
        query: str,
        top_k: Optional[int] = None,
        similarity_threshold: Optional[float] = None,
    ) -> List[SearchResult]:
        """
        Базовая функция поиска

        Args:
            query: Поисковый запрос
            top_k: Количество результатов (по умолчанию из config)
            similarity_threshold: Минимальный порог схожести

        Returns:
            Список результатов поиска
        """
        if self.model is None:
            logger.error("Модель не загружена")
            return []

        if not query.strip():
            logger.warning("Пустой запрос")
            return []

        top_k = top_k or self.config["default_top_k"]
        similarity_threshold = (
            similarity_threshold or self.config["similarity_threshold"]
        )

        try:
            logger.info(f"Поиск по запросу: '{query}'")

            # Препроцессор запроса
            query_tokens = self.text_processor.preprocess_text(query)

            if not query_tokens:
                logger.warning("Запрос не содержит значимых токенов")
                return []

            logger.info(
                f"Токены запроса: {query_tokens[:10]}..."
            )  # Показываем первые 10

            # Получаем вектор для запроса
            query_vector = self.model.infer_vector(query_tokens)

            # Ищем похожие документы
            similar_docs = self.model.dv.most_similar([query_vector], topn=top_k)

            # Фильтруем по порогу схожести и создаем результаты
            results = []
            for doc_id, similarity in similar_docs:
                if similarity >= similarity_threshold:
                    metadata = self._metadata_index.get(doc_id, {})
                    results.append(SearchResult(doc_id, similarity, metadata))

            logger.info(f"Найдено результатов: {len(results)}")
            return results

        except Exception as e:
            logger.error(f"Ошибка при поиске: {e}")
            return []

    def search_with_filters(
        self,
        query: str,
        top_k: Optional[int] = None,
        file_extensions: Optional[set] = None,
        date_range: Optional[tuple] = None,
        min_file_size: Optional[int] = None,
        max_file_size: Optional[int] = None,
    ) -> List[SearchResult]:
        """Поиск с фильтрами"""

        # Базовый поиск
        results = self._search_base(query, top_k=top_k or 100)

        # Применяем фильтры
        filtered_results = []
        for result in results:
            metadata = result.metadata

            # Фильтр по расширению
            if file_extensions and metadata.get("extension") not in file_extensions:
                continue

            # Фильтр по размеру файла
            file_size = metadata.get("file_size", 0)
            if min_file_size and file_size < min_file_size:
                continue
            if max_file_size and file_size > max_file_size:
                continue

            filtered_results.append(result)

            if len(filtered_results) >= (top_k or self.config["default_top_k"]):
                break

        return filtered_results

    def make_cache_key(
        self,
        query: str,
        top_k: Optional[int],
        file_extensions: Optional[set],
        date_range: Optional[tuple],
        min_file_size: Optional[int],
        max_file_size: Optional[int],
    ) -> str:
        """Генерация стабильного кэш-ключа для поискового запроса"""
        key_data = {
            "query": query.strip().lower(),
            "top_k": top_k,
            "file_extensions": sorted(file_extensions) if file_extensions else None,
            "date_range": date_range,
            "min_file_size": min_file_size,
            "max_file_size": max_file_size,
        }
        return json.dumps(key_data, sort_keys=True, ensure_ascii=False)

    def search(
        self,
        query: str,
        top_k: Optional[int] = None,
        file_extensions: Optional[set] = None,
        date_range: Optional[tuple] = None,
        min_file_size: Optional[int] = None,
        max_file_size: Optional[int] = None,
    ) -> List[SearchResult]:
        """
        Поиск с кэшированием и поддержкой фильтров

        Args:
            query: Поисковый запрос
            top_k: Количество результатов
            file_extensions: Фильтр по расширениям файлов
            date_range: Фильтр по дате (не используется, но оставлено для совместимости)
            min_file_size: Минимальный размер файла
            max_file_size: Максимальный размер файла

        Returns:
            Список результатов поиска
        """
        if not self.config.get("enable_caching", True):
            return self.search_with_filters(
                query,
                top_k=top_k,
                file_extensions=file_extensions,
                date_range=date_range,
                min_file_size=min_file_size,
                max_file_size=max_file_size,
            )

        # Генерируем стабильный ключ
        raw_key = self.make_cache_key(
            query, top_k, file_extensions, date_range, min_file_size, max_file_size
        )
        cache_key = f"search:{raw_key}"

        # Проверяем кэш
        cached_result = self.cache_manager.get(cache_key)
        if cached_result:
            logger.info(f"Результат получен из кэша для запроса: {query}")
            return cached_result

        # Выполняем поиск
        results = self.search_with_filters(
            query,
            top_k=top_k,
            file_extensions=file_extensions,
            date_range=date_range,
            min_file_size=min_file_size,
            max_file_size=max_file_size,
        )

        # Сохраняем в кэш
        self.cache_manager.set(cache_key, results)

        return results

    def search_similar_to_document(
        self, doc_id: str, top_k: Optional[int] = None
    ) -> List[SearchResult]:
        """
        Поиск документов, похожих на указанный документ

        Args:
            doc_id: ID документа
            top_k: Количество результатов

        Returns:
            Список похожих документов
        """
        if self.model is None:
            logger.error("Модель не загружена")
            return []

        top_k = top_k or self.config["default_top_k"]

        try:
            if doc_id not in self.model.dv:
                logger.error(f"Документ не найден в модели: {doc_id}")
                return []

            # Получаем похожие документы
            similar_docs = self.model.dv.most_similar(
                doc_id, topn=top_k + 1
            )  # +1 чтобы исключить сам документ

            results = []
            for similar_doc_id, similarity in similar_docs:
                if similar_doc_id != doc_id:  # Исключаем сам документ
                    metadata = self._metadata_index.get(similar_doc_id, {})
                    results.append(SearchResult(similar_doc_id, similarity, metadata))

            return results[:top_k]  # Возвращаем только top_k результатов

        except Exception as e:
            logger.error(f"Ошибка при поиске похожих документов: {e}")
            return []

    def get_document_vector(self, doc_id: str) -> Optional[list]:
        """
        Получение вектора документа

        Args:
            doc_id: ID документа

        Returns:
            Вектор документа или None
        """
        if self.model is None or doc_id not in self.model.dv:
            return None

        return self.model.dv[doc_id].tolist()

    def get_search_statistics(self) -> Dict[str, Any]:
        """
        Получение статистики поисковой системы

        Returns:
            Словарь со статистикой
        """
        if self.model is None:
            return {"status": "no_model"}

        return {
            "status": "ready",
            "documents_count": len(self.model.dv),
            "vocabulary_size": len(self.model.wv.key_to_index),
            "vector_size": self.model.vector_size,
            "indexed_documents": list(self.model.dv.key_to_index.keys())[
                :10
            ],  # Первые 10
        }


========================================
FILE: src\semantic_search\core\text_summarizer.py
========================================
"""Модуль для суммаризации текстов"""

from __future__ import annotations

from pathlib import Path
from typing import TYPE_CHECKING, List, Optional, Tuple

if TYPE_CHECKING:
    from gensim.models.doc2vec import Doc2Vec

import numpy as np
from loguru import logger

from semantic_search.config import SUMMARIZATION_CONFIG, TEXT_PROCESSING_CONFIG
from semantic_search.utils.text_utils import TextProcessor

try:
    from sklearn.metrics.pairwise import cosine_similarity

    SKLEARN_AVAILABLE = True
except ImportError:
    logger.warning("scikit-learn не установлен. Суммаризация будет недоступна")
    SKLEARN_AVAILABLE = False


class TextSummarizer:
    """Класс для экстрактивной суммаризации текстов"""

    def __init__(self, doc2vec_model: Optional[Doc2Vec] = None):
        self.model = doc2vec_model
        self.text_processor = TextProcessor()
        self.config = SUMMARIZATION_CONFIG
        self.chunk_size = TEXT_PROCESSING_CONFIG.get("chunk_size", 500_000)

        # Минимальная длина предложения для включения в выжимку
        self.min_summary_sentence_length = self.config.get("min_sentence_length", 15)
        # Минимальное количество слов в предложении
        self.min_words_in_sentence = self.config.get("min_words_in_sentence", 5)

    def set_model(self, model: Doc2Vec):
        """Установка модели Doc2Vec"""
        self.model = model
        logger.info("Модель для суммаризации установлена")

    def _filter_sentence(self, sentence: str) -> bool:
        """
        Проверка, подходит ли предложение для включения в выжимку

        Args:
            sentence: Предложение для проверки

        Returns:
            True если предложение подходит, False если нужно отфильтровать
        """
        # Убираем лишние пробелы
        cleaned_sentence = sentence.strip()

        # Проверка минимальной длины в символах
        if len(cleaned_sentence) < self.min_summary_sentence_length:
            return False

        # Проверка минимального количества слов
        words = cleaned_sentence.split()
        if len(words) < self.min_words_in_sentence:
            return False

        # Проверка на наличие хотя бы одного значимого слова (не только предлоги/союзы)
        meaningful_words = [w for w in words if len(w) > 3]
        if len(meaningful_words) < 2:
            return False

        # Проверка на слишком много цифр (возможно, это таблица или список)
        digit_ratio = sum(c.isdigit() for c in cleaned_sentence) / len(cleaned_sentence)
        if digit_ratio > 0.5:
            return False

        # Проверка на повторяющиеся символы (например, "............")
        for char in cleaned_sentence:
            if cleaned_sentence.count(char * 5) > 0:  # 5 одинаковых символов подряд
                return False

        return True

    def _sentence_to_vector(self, sentence_tokens: List[str]) -> Optional[np.ndarray]:
        """
        Преобразование предложения в вектор

        Args:
            sentence_tokens: Токены предложения

        Returns:
            Векторное представление предложения
        """
        if self.model is None or not sentence_tokens:
            return None

        try:
            # Используем infer_vector для получения вектора предложения
            vector = self.model.infer_vector(sentence_tokens)
            return vector
        except Exception as e:
            logger.error(f"Ошибка при векторизации предложения: {e}")
            return None

    def _calculate_sentence_scores(
        self, sentences: List[str]
    ) -> List[Tuple[str, float]]:
        """
        Вычисление оценок важности предложений методом TextRank
        с учетом фильтрации коротких предложений

        Args:
            sentences: Список предложений

        Returns:
            Список кортежей (предложение, оценка)
        """
        # Фильтруем предложения перед оценкой
        filtered_sentences = []
        sentence_indices = []

        for i, sentence in enumerate(sentences):
            if self._filter_sentence(sentence):
                filtered_sentences.append(sentence)
                sentence_indices.append(i)

        if not filtered_sentences:
            logger.warning("Все предложения отфильтрованы как слишком короткие")
            # Возвращаем самые длинные предложения если все отфильтрованы
            sorted_by_length = sorted(
                enumerate(sentences), key=lambda x: len(x[1]), reverse=True
            )
            return [(sent, 1.0) for _, sent in sorted_by_length[:5]]

        if not SKLEARN_AVAILABLE or self.model is None:
            # Fallback: оценка по длине и позиции
            scored_sentences = []
            for i, sent in enumerate(filtered_sentences):
                # Учитываем длину и позицию (начало текста важнее)
                position_score = 1.0 - (sentence_indices[i] / len(sentences))
                length_score = min(
                    len(sent.split()) / 20, 1.0
                )  # Нормализуем по 20 словам
                score = position_score * 0.3 + length_score * 0.7
                scored_sentences.append((sent, score))
            return scored_sentences

        # Получаем векторы для отфильтрованных предложений
        sentence_vectors = []
        valid_sentences = []

        for sentence in filtered_sentences:
            tokens = self.text_processor.preprocess_text(sentence)
            if tokens:  # Проверяем, что есть значимые токены
                vector = self._sentence_to_vector(tokens)
                if vector is not None:
                    sentence_vectors.append(vector)
                    valid_sentences.append(sentence)

        if len(sentence_vectors) < 2:
            # Недостаточно предложений для анализа
            return [(sent, 1.0) for sent in valid_sentences]

        try:
            # Вычисляем матрицу схожести
            similarity_matrix = cosine_similarity(sentence_vectors)

            # Применяем простой алгоритм PageRank
            scores = self._pagerank_algorithm(similarity_matrix)

            # Сопоставляем оценки с предложениями
            scored_sentences = list(zip(valid_sentences, scores))

            return scored_sentences

        except Exception as e:
            logger.error(f"Ошибка при вычислении оценок предложений: {e}")
            # Fallback к простой оценке
            return [(sent, 1.0) for sent in valid_sentences]

    def _pagerank_algorithm(
        self, similarity_matrix: np.ndarray, damping: float = 0.85, max_iter: int = 100
    ) -> List[float]:
        """
        Упрощенный алгоритм PageRank для ранжирования предложений

        Args:
            similarity_matrix: Матрица схожести предложений
            damping: Коэффициент затухания
            max_iter: Максимальное количество итераций

        Returns:
            Список оценок для каждого предложения
        """
        n = similarity_matrix.shape[0]

        # Инициализация: равные веса для всех предложений
        scores = np.ones(n) / n

        # Создаем переходную матрицу
        # Заменяем нули на маленькое значение, чтобы избежать деления на ноль
        similarity_matrix = np.where(similarity_matrix == 0, 1e-8, similarity_matrix)

        # Нормализуем строки матрицы
        row_sums = similarity_matrix.sum(axis=1)
        transition_matrix = similarity_matrix / row_sums[:, np.newaxis]

        # Итеративный алгоритм PageRank
        for _ in range(max_iter):
            new_scores = (1 - damping) / n + damping * np.dot(
                transition_matrix.T, scores
            )

            # Проверяем сходимость
            if np.allclose(scores, new_scores, atol=1e-6):
                break

            scores = new_scores

        return scores.tolist()

    def summarize_text(
        self,
        text: str,
        sentences_count: Optional[int] = None,
        min_sentence_length: Optional[int] = None,
    ) -> List[str]:
        """
        Создание экстрактивной выжимки текста

        Args:
            text: Исходный текст
            sentences_count: Количество предложений в выжимке
            min_sentence_length: Минимальная длина предложения (переопределяет настройки)

        Returns:
            Список предложений выжимки
        """
        if not text.strip():
            logger.warning("Пустой текст для суммаризации")
            return []

        sentences_count = sentences_count or self.config["default_sentences_count"]

        # Временно переопределяем минимальную длину если указана
        if min_sentence_length is not None:
            original_min_length = self.min_summary_sentence_length
            self.min_summary_sentence_length = min_sentence_length

        logger.info(
            f"Начинаем суммаризацию текста длиной {len(text)} символов (цель: {sentences_count} предложений)"
        )

        # Для очень длинных текстов используем упрощенный подход
        if len(text) > 1_000_000:
            logger.warning(
                f"Текст очень длинный ({len(text)} символов), используем упрощенный метод"
            )
            result = self._summarize_long_text(
                text, sentences_count, self.min_summary_sentence_length
            )

            # Восстанавливаем оригинальную настройку
            if min_sentence_length is not None:
                self.min_summary_sentence_length = original_min_length

            return result

        sentences = self.text_processor.split_into_sentences(text)

        if not sentences:
            logger.warning("Не удалось разбить текст на предложения")

            # Восстанавливаем оригинальную настройку
            if min_sentence_length is not None:
                self.min_summary_sentence_length = original_min_length

            return []

        # Фильтруем слишком короткие предложения перед проверкой
        valid_sentences = [s for s in sentences if self._filter_sentence(s)]

        if len(valid_sentences) <= sentences_count:
            logger.info(
                f"Количество подходящих предложений ({len(valid_sentences)}) меньше или равно требуемому"
            )

            # Восстанавливаем оригинальную настройку
            if min_sentence_length is not None:
                self.min_summary_sentence_length = original_min_length

            return valid_sentences

        # Вычисляем оценки важности предложений (уже отфильтрованных)
        scored_sentences = self._calculate_sentence_scores(sentences)

        # Сортируем по оценке (убывание)
        scored_sentences.sort(key=lambda x: x[1], reverse=True)

        # Берем топ-N предложений
        top_sentences = scored_sentences[:sentences_count]

        # Восстанавливаем исходный порядок предложений
        summary_sentences = []

        # Создаем множество для быстрого поиска
        top_sentences_set = {sent for sent, _ in top_sentences}

        for original_sent in sentences:
            if original_sent in top_sentences_set:
                summary_sentences.append(original_sent)
                if len(summary_sentences) >= sentences_count:
                    break

        logger.info(f"Создана выжимка из {len(summary_sentences)} предложений")

        # Восстанавливаем оригинальную настройку
        if min_sentence_length is not None:
            self.min_summary_sentence_length = original_min_length

        return summary_sentences

    def _summarize_long_text(
        self, text: str, sentences_count: int, min_sentence_length: int
    ) -> List[str]:
        """
        Упрощенная суммаризация для очень длинных текстов

        Args:
            text: Исходный текст
            sentences_count: Количество предложений
            min_sentence_length: Минимальная длина предложения

        Returns:
            Список предложений выжимки
        """

        chunks = [
            text[i : i + self.chunk_size] for i in range(0, len(text), self.chunk_size)
        ]

        all_important_sentences = []

        for i, chunk in enumerate(chunks):
            logger.info(f"Обработка части {i + 1}/{len(chunks)}")

            chunk_sentences = self.text_processor.split_into_sentences(chunk)

            if not chunk_sentences:
                continue

            # Фильтруем короткие предложения
            valid_chunk_sentences = [
                s for s in chunk_sentences if self._filter_sentence(s)
            ]

            if not valid_chunk_sentences:
                continue

            # Для каждого чанка выбираем пропорциональное количество предложений
            chunk_sentence_count = max(1, sentences_count // len(chunks))
            if i == 0:  # Первый чанк может содержать больше важной информации
                chunk_sentence_count = max(2, chunk_sentence_count)

            # Простая эвристика: берем первые и последние предложения + самые длинные
            important_sentences = []

            # Первое предложение чанка (если оно достаточно длинное)
            if valid_chunk_sentences:
                important_sentences.append(valid_chunk_sentences[0])

            # Последнее предложение чанка
            if len(valid_chunk_sentences) > 1:
                important_sentences.append(valid_chunk_sentences[-1])

            # Самые информативные предложения (длинные, но не слишком)
            if len(valid_chunk_sentences) > 2:
                # Сортируем по "информативности" - не слишком короткие и не слишком длинные
                middle_sentences = valid_chunk_sentences[1:-1]
                sorted_by_info = sorted(
                    middle_sentences,
                    key=lambda s: min(len(s.split()), 50),  # Оптимальная длина ~50 слов
                    reverse=True,
                )
                remaining_count = chunk_sentence_count - len(important_sentences)
                important_sentences.extend(sorted_by_info[:remaining_count])

            all_important_sentences.extend(important_sentences[:chunk_sentence_count])

        # Удаляем дубликаты, сохраняя порядок
        seen = set()
        unique_sentences = []
        for sent in all_important_sentences:
            if sent not in seen and self._filter_sentence(sent):
                seen.add(sent)
                unique_sentences.append(sent)

        # Возвращаем требуемое количество предложений
        return unique_sentences[:sentences_count]

    def summarize_file(self, file_path: str, **kwargs) -> List[str]:
        """
        Суммаризация файла

        Args:
            file_path: Путь к файлу
            **kwargs: Дополнительные параметры для summarize_text

        Returns:
            Список предложений выжимки
        """
        from semantic_search.utils.file_utils import FileExtractor

        try:
            extractor = FileExtractor()
            text = extractor.extract_text(Path(file_path))

            if not text:
                logger.error(f"Не удалось извлечь текст из файла: {file_path}")
                return []

            return self.summarize_text(text, **kwargs)

        except Exception as e:
            logger.error(f"Ошибка при суммаризации файла {file_path}: {e}")
            return []

    def get_summary_statistics(self, original_text: str, summary: List[str]) -> dict:
        """
        Получение статистики суммаризации

        Args:
            original_text: Исходный текст
            summary: Выжимка

        Returns:
            Словарь со статистикой
        """
        original_sentences = self.text_processor.split_into_sentences(original_text)

        # Считаем только валидные предложения в оригинале
        valid_original_sentences = [
            s for s in original_sentences if self._filter_sentence(s)
        ]

        stats = {
            "original_sentences_count": len(original_sentences),
            "valid_original_sentences_count": len(valid_original_sentences),
            "summary_sentences_count": len(summary),
            "compression_ratio": len(summary) / len(original_sentences)
            if original_sentences
            else 0,
            "valid_compression_ratio": len(summary) / len(valid_original_sentences)
            if valid_original_sentences
            else 0,
            "original_chars_count": len(original_text),
            "summary_chars_count": sum(len(sent) for sent in summary),
            "chars_compression_ratio": sum(len(sent) for sent in summary)
            / len(original_text)
            if original_text
            else 0,
            "avg_sentence_length": sum(len(sent.split()) for sent in summary)
            / len(summary)
            if summary
            else 0,
        }

        return stats


========================================
FILE: src\semantic_search\gui\__init__.py
========================================
"""GUI модуль для Semantic Search"""

from .main_window import MainWindow

__all__ = ["MainWindow"]


========================================
FILE: src\semantic_search\gui\evaluation_widget.py
========================================
"""Виджет для оценки и сравнения методов поиска в GUI"""

import os
from typing import List

from loguru import logger
from PyQt6.QtCore import QThread, pyqtSignal
from PyQt6.QtWidgets import (
    QComboBox,
    QGroupBox,
    QHBoxLayout,
    QLabel,
    QLineEdit,
    QMessageBox,
    QProgressBar,
    QPushButton,
    QTextEdit,
    QVBoxLayout,
    QWidget,
)

from semantic_search.config import EVALUATION_RESULTS_DIR
from semantic_search.evaluation.baselines import (
    Doc2VecSearchAdapter,
    OpenAISearchBaseline,
)
from semantic_search.evaluation.comparison import QueryTestCase, SearchComparison


class EvaluationThread(QThread):
    """Поток для выполнения оценки"""

    progress = pyqtSignal(int, str)
    finished = pyqtSignal(bool, str)
    result_ready = pyqtSignal(dict)

    def __init__(
        self,
        search_engine,
        corpus_info,
        openai_key: str,
        test_cases: List[QueryTestCase],
    ):
        super().__init__()
        self.search_engine = search_engine
        self.corpus_info = corpus_info
        self.openai_key = openai_key
        self.test_cases = test_cases
        self.comparison = SearchComparison(test_cases)

    def run(self):
        """Выполнение сравнения"""
        try:
            # Шаг 1: Подготовка Doc2Vec адаптера
            self.progress.emit(10, "Подготовка Doc2Vec метода...")
            doc2vec_adapter = Doc2VecSearchAdapter(self.search_engine, self.corpus_info)

            # Шаг 2: Подготовка OpenAI baseline
            self.progress.emit(20, "Инициализация OpenAI...")
            try:
                openai_baseline = OpenAISearchBaseline(api_key=self.openai_key)
            except Exception as e:
                self.finished.emit(False, f"Ошибка инициализации OpenAI: {str(e)}")
                return

            # Шаг 3: Индексация документов для OpenAI
            self.progress.emit(30, "Индексация документов через OpenAI API...")

            # Подготовка документов для индексации
            documents = []
            for tokens, doc_id, metadata in self.corpus_info[
                :50
            ]:  # Ограничиваем 50 документами для демо
                # Восстанавливаем текст из токенов (упрощенно)
                text = " ".join(tokens[:500])  # Берем первые 500 токенов
                documents.append((doc_id, text, metadata))

            try:
                openai_baseline.index(documents)
            except Exception as e:
                self.finished.emit(False, f"Ошибка индексации OpenAI: {str(e)}")
                return

            # Шаг 4: Оценка методов
            self.progress.emit(50, "Оценка Doc2Vec...")
            doc2vec_results = self.comparison.evaluate_method(
                doc2vec_adapter, top_k=10, verbose=False
            )

            self.progress.emit(70, "Оценка OpenAI...")
            openai_results = self.comparison.evaluate_method(
                openai_baseline, top_k=10, verbose=False
            )

            # Шаг 5: Генерация отчетов и графиков
            self.progress.emit(85, "Генерация отчетов...")

            # Сравнительная таблица
            df_comparison = self.comparison.compare_methods(
                [doc2vec_adapter, openai_baseline], save_results=True
            )

            # Графики
            self.comparison.plot_comparison(save_plots=True)

            # Текстовый отчет
            report_path = EVALUATION_RESULTS_DIR / "comparison_report.txt"
            report_text = self.comparison.generate_report(report_path)

            # Результаты для GUI
            results = {
                "comparison_df": df_comparison,
                "report_text": report_text,
                "doc2vec_map": doc2vec_results["aggregated"]["MAP"],
                "openai_map": openai_results["aggregated"]["MAP"],
                "doc2vec_time": doc2vec_results["aggregated"]["avg_query_time"],
                "openai_time": openai_results["aggregated"]["avg_query_time"],
            }

            self.progress.emit(100, "Оценка завершена!")
            self.result_ready.emit(results)
            self.finished.emit(True, "Сравнение методов успешно завершено")

        except Exception as e:
            logger.error(f"Ошибка в потоке оценки: {e}")
            self.finished.emit(False, f"Ошибка: {str(e)}")


class EvaluationWidget(QWidget):
    """Виджет для оценки и сравнения методов поиска"""

    def __init__(self, parent=None):
        super().__init__(parent)
        self.search_engine = None
        self.corpus_info = None
        self.evaluation_thread = None
        self.init_ui()

    def init_ui(self):
        """Инициализация интерфейса"""
        layout = QVBoxLayout()

        # Группа настроек OpenAI
        openai_group = QGroupBox("Настройки OpenAI")
        openai_layout = QVBoxLayout()

        key_layout = QHBoxLayout()
        key_layout.addWidget(QLabel("API Key:"))
        self.api_key_edit = QLineEdit()
        self.api_key_edit.setEchoMode(QLineEdit.EchoMode.Password)
        self.api_key_edit.setPlaceholderText("sk-...")

        # Пытаемся загрузить ключ из переменной окружения
        env_key = os.getenv("OPENAI_API_KEY")
        if env_key:
            self.api_key_edit.setText(env_key)

        key_layout.addWidget(self.api_key_edit)
        openai_layout.addLayout(key_layout)

        openai_group.setLayout(openai_layout)
        layout.addWidget(openai_group)

        # Группа тестовых запросов
        test_group = QGroupBox("Тестовые запросы")
        test_layout = QVBoxLayout()

        self.test_cases_combo = QComboBox()
        self.test_cases_combo.addItem("Стандартный набор тестов (5 запросов)")
        self.test_cases_combo.addItem("Расширенный набор (10 запросов)")
        self.test_cases_combo.addItem("Быстрый тест (3 запроса)")

        test_layout.addWidget(self.test_cases_combo)
        test_group.setLayout(test_layout)
        layout.addWidget(test_group)

        # Кнопка запуска
        self.run_button = QPushButton("Запустить сравнение")
        self.run_button.clicked.connect(self.run_evaluation)
        layout.addWidget(self.run_button)

        # Прогресс
        self.progress_bar = QProgressBar()
        self.progress_bar.setVisible(False)
        layout.addWidget(self.progress_bar)

        # Результаты
        self.results_text = QTextEdit()
        self.results_text.setReadOnly(True)
        layout.addWidget(self.results_text)

        self.setLayout(layout)

    def set_search_engine(self, search_engine, corpus_info):
        """Установка поискового движка для оценки"""
        self.search_engine = search_engine
        self.corpus_info = corpus_info

    def get_test_cases(self) -> List[QueryTestCase]:
        """Получение тестовых случаев в зависимости от выбора"""
        comparison = SearchComparison()
        default_cases = comparison.create_default_test_cases()

        selected_index = self.test_cases_combo.currentIndex()

        if selected_index == 0:  # Стандартный набор
            return default_cases[:5]
        elif selected_index == 1:  # Расширенный набор
            # Добавляем дополнительные тесты
            extra_cases = [
                QueryTestCase(
                    query="классификация изображений CNN",
                    relevant_docs={"cnn_tutorial.pdf", "image_classification.pdf"},
                    relevance_scores={
                        "cnn_tutorial.pdf": 3,
                        "image_classification.pdf": 3,
                    },
                ),
                QueryTestCase(
                    query="регуляризация в машинном обучении",
                    relevant_docs={"regularization.pdf", "overfitting.pdf"},
                    relevance_scores={"regularization.pdf": 3, "overfitting.pdf": 2},
                ),
                QueryTestCase(
                    query="word2vec и doc2vec модели",
                    relevant_docs={"word2vec_paper.pdf", "doc2vec_tutorial.pdf"},
                    relevance_scores={
                        "word2vec_paper.pdf": 3,
                        "doc2vec_tutorial.pdf": 3,
                    },
                ),
                QueryTestCase(
                    query="метрики оценки качества классификации",
                    relevant_docs={"ml_metrics.pdf", "evaluation_methods.pdf"},
                    relevance_scores={"ml_metrics.pdf": 3, "evaluation_methods.pdf": 3},
                ),
                QueryTestCase(
                    query="обратное распространение ошибки",
                    relevant_docs={"backpropagation.pdf", "neural_networks.pdf"},
                    relevance_scores={
                        "backpropagation.pdf": 3,
                        "neural_networks.pdf": 2,
                    },
                ),
            ]
            return default_cases + extra_cases
        else:  # Быстрый тест
            return default_cases[:3]

    def run_evaluation(self):
        """Запуск оценки"""
        if not self.search_engine:
            QMessageBox.warning(self, "Ошибка", "Сначала загрузите модель Doc2Vec")
            return

        api_key = self.api_key_edit.text().strip()
        if not api_key:
            QMessageBox.warning(
                self,
                "Ошибка",
                "Введите API ключ OpenAI или установите переменную окружения OPENAI_API_KEY",
            )
            return

        # Получаем тестовые случаи
        test_cases = self.get_test_cases()

        # Отключаем кнопку и показываем прогресс
        self.run_button.setEnabled(False)
        self.progress_bar.setVisible(True)
        self.progress_bar.setValue(0)
        self.results_text.clear()

        # Создаем и запускаем поток
        self.evaluation_thread = EvaluationThread(
            self.search_engine, self.corpus_info, api_key, test_cases
        )

        self.evaluation_thread.progress.connect(self.on_progress)
        self.evaluation_thread.finished.connect(self.on_finished)
        self.evaluation_thread.result_ready.connect(self.on_results_ready)

        self.evaluation_thread.start()

    def on_progress(self, value: int, message: str):
        """Обновление прогресса"""
        self.progress_bar.setValue(value)
        self.results_text.append(f"[{value}%] {message}")

    def on_finished(self, success: bool, message: str):
        """Завершение оценки"""
        self.run_button.setEnabled(True)
        self.progress_bar.setVisible(False)

        if success:
            self.results_text.append(f"\n✅ {message}")
            QMessageBox.information(self, "Успех", message)
        else:
            self.results_text.append(f"\n❌ {message}")
            QMessageBox.critical(self, "Ошибка", message)

    def on_results_ready(self, results: dict):
        """Обработка результатов"""
        # Добавляем основные результаты в текстовое поле
        self.results_text.append("\n" + "=" * 80)
        self.results_text.append("РЕЗУЛЬТАТЫ СРАВНЕНИЯ")
        self.results_text.append("=" * 80)

        # Основные метрики
        doc2vec_map = results["doc2vec_map"]
        openai_map = results["openai_map"]
        improvement = (
            ((doc2vec_map - openai_map) / openai_map) * 100 if openai_map > 0 else 0
        )

        self.results_text.append("\n📊 MAP (Mean Average Precision):")
        self.results_text.append(f"   Doc2Vec: {doc2vec_map:.3f}")
        self.results_text.append(f"   OpenAI:  {openai_map:.3f}")

        if improvement > 0:
            self.results_text.append(f"   ✅ Doc2Vec лучше на {improvement:.1f}%")
        else:
            self.results_text.append(f"   ❌ OpenAI лучше на {-improvement:.1f}%")

        # Скорость
        doc2vec_time = results["doc2vec_time"]
        openai_time = results["openai_time"]
        speed_ratio = openai_time / doc2vec_time if doc2vec_time > 0 else 0

        self.results_text.append("\n⚡ Скорость поиска:")
        self.results_text.append(f"   Doc2Vec: {doc2vec_time:.3f}с")
        self.results_text.append(f"   OpenAI:  {openai_time:.3f}с")
        self.results_text.append(f"   ✅ Doc2Vec быстрее в {speed_ratio:.1f} раз")

        # Полный отчет
        self.results_text.append("\n" + results["report_text"])

        # Информация о сохраненных файлах
        self.results_text.append("\n📁 Файлы сохранены в:")
        self.results_text.append(f"   {EVALUATION_RESULTS_DIR}")
        self.results_text.append("   - comparison_results.csv")
        self.results_text.append("   - comparison_report.txt")
        self.results_text.append("   - plots/comparison_plots.png")


========================================
FILE: src\semantic_search\gui\main_window.py
========================================
"""Главное окно приложения"""

import json
import os
import sys
import time
from pathlib import Path
from typing import List, Optional

from loguru import logger
from PyQt6.QtCore import Qt, QThread, pyqtSignal
from PyQt6.QtGui import QAction, QFont
from PyQt6.QtWidgets import (
    QApplication,
    QCheckBox,
    QComboBox,
    QFileDialog,
    QGroupBox,
    QHBoxLayout,
    QLabel,
    QLineEdit,
    QListWidget,
    QListWidgetItem,
    QMainWindow,
    QMessageBox,
    QProgressBar,
    QPushButton,
    QSpinBox,
    QSplitter,
    QStatusBar,
    QTabWidget,
    QTextEdit,
    QToolBar,
    QVBoxLayout,
    QWidget,
)

from semantic_search.config import GUI_CONFIG, MODELS_DIR
from semantic_search.core.doc2vec_trainer import Doc2VecTrainer
from semantic_search.core.document_processor import DocumentProcessor
from semantic_search.core.search_engine import SemanticSearchEngine
from semantic_search.core.text_summarizer import TextSummarizer
from semantic_search.gui.evaluation_widget import EvaluationWidget
from semantic_search.utils.file_utils import FileExtractor
from semantic_search.utils.statistics import (
    calculate_statistics_from_processed_docs,
    format_statistics_for_display,
)

"""Исправленный класс TrainingThread для правильного подсчета времени"""


class TrainingThread(QThread):
    """Поток для обучения модели"""

    progress = pyqtSignal(int, str)
    finished = pyqtSignal(bool, str)
    statistics = pyqtSignal(dict)

    def __init__(
        self,
        documents_path: Path,
        model_name: str,
        vector_size: int,
        epochs: int,
        window: int = 15,
        min_count: int = 3,
        dm: int = 1,
        negative: int = 10,
        preset: Optional[str] = None,
    ):
        super().__init__()
        self.documents_path = documents_path
        self.model_name = model_name
        self.vector_size = vector_size
        self.epochs = epochs
        self.window = window
        self.min_count = min_count
        self.dm = dm
        self.negative = negative
        self.preset = preset
        self.is_cancelled = False

    def run(self):
        """Выполнение обучения"""
        try:
            # Начинаем отсчет общего времени
            start_time = time.time()

            # Обработка документов
            processor = DocumentProcessor()
            processed_docs = []

            self.progress.emit(10, "Поиск документов...")

            file_extractor = FileExtractor()
            file_paths = file_extractor.find_documents(self.documents_path)

            if not file_paths:
                self.finished.emit(False, "Документы не найдены")
                return

            # Обработка каждого документа
            step_size = 40 / len(file_paths)
            current_progress = 10

            for i, doc in enumerate(processor.process_documents(self.documents_path)):
                if self.is_cancelled:
                    self.finished.emit(False, "Обучение отменено")
                    return

                processed_docs.append(doc)
                current_progress += step_size
                self.progress.emit(
                    int(current_progress),
                    f"Обработан документ {i + 1}/{len(file_paths)}: {doc.relative_path}",
                )

            if not processed_docs:
                self.finished.emit(False, "Не удалось обработать документы")
                return

            # Подготовка корпуса
            corpus = [
                (doc.tokens, doc.relative_path, doc.metadata) for doc in processed_docs
            ]

            # Статистика корпуса
            stats = calculate_statistics_from_processed_docs(processed_docs)
            self.statistics.emit(stats)

            # Анализ языкового состава (новое)
            self.progress.emit(45, "Анализ языкового состава документов...")
            language_info = self._analyze_language_distribution(corpus)

            logger.info(f"Языковой состав: {language_info}")

            # Адаптация параметров на основе языкового состава
            adapted_params = self._adapt_params_for_language(language_info)

            # Применяем адаптированные параметры
            final_vector_size = adapted_params.get("vector_size", self.vector_size)
            final_window = adapted_params.get("window", self.window)
            final_min_count = adapted_params.get("min_count", self.min_count)

            if adapted_params:
                self.progress.emit(
                    48, "Параметры адаптированы для многоязычного корпуса"
                )
                logger.info(f"Адаптированные параметры: {adapted_params}")

            trainer = Doc2VecTrainer()

            # Обучение модели с адаптированными параметрами
            self.progress.emit(
                50,
                f"Обучение модели (векторы: {final_vector_size}, окно: {final_window})...",
            )

            model = trainer.train_model(
                corpus,
                vector_size=final_vector_size,  # Используем адаптированное значение
                epochs=self.epochs,
                window=final_window,  # Используем адаптированное значение
                min_count=final_min_count,  # Используем адаптированное значение
                dm=self.dm,
                negative=self.negative,
                sample=1e-5,
                preset=self.preset,
            )

            if model:
                # Вычисляем общее время включая обработку документов
                training_time = time.time() - start_time

                # Сохраняем метаданные обучения
                trainer.training_metadata = {
                    "training_time_formatted": f"{training_time:.1f}с ({training_time / 60:.1f}м)",
                    "training_date": time.strftime(
                        "%Y-%m-%d %H:%M:%S", time.localtime(start_time)
                    ),
                    "corpus_size": len(processed_docs),
                    "documents_base_path": str(self.documents_path.absolute()),
                    "vector_size": self.vector_size,
                    "epochs": self.epochs,
                    "window": self.window,
                    "min_count": self.min_count,
                    "dm": self.dm,
                    "negative": self.negative,
                    "preset_used": self.preset,
                    "language_distribution": language_info,
                    "python_version": sys.version,
                    "platform": sys.platform,
                }

                self.progress.emit(90, "Сохранение модели...")
                success = trainer.save_model(model, self.model_name)

                if success:
                    self.progress.emit(100, "Обучение завершено!")
                    self.finished.emit(
                        True,
                        f"Модель '{self.model_name}' успешно обучена за {training_time / 60:.1f} минут",
                    )
                else:
                    self.finished.emit(False, "Ошибка при сохранении модели")
            else:
                self.finished.emit(False, "Ошибка при обучении модели")

        except Exception as e:
            logger.error(f"Ошибка в потоке обучения: {e}", exc_info=True)
            self.finished.emit(False, f"Ошибка: {str(e)}")

    def _analyze_language_distribution(self, corpus):
        """Анализ языкового состава корпуса"""
        language_stats = {"russian": 0, "english": 0, "mixed": 0}

        for tokens, doc_id, metadata in corpus[
            :100
        ]:  # Анализируем первые 100 документов
            # Подсчет кириллических и латинских токенов
            cyrillic_tokens = sum(
                1 for t in tokens[:200] if any("\u0400" <= c <= "\u04ff" for c in t)
            )
            latin_tokens = sum(1 for t in tokens[:200] if t.isalpha() and t.isascii())

            total = cyrillic_tokens + latin_tokens
            if total > 0:
                cyrillic_ratio = cyrillic_tokens / total

                if cyrillic_ratio > 0.8:
                    language_stats["russian"] += 1
                elif cyrillic_ratio < 0.2:
                    language_stats["english"] += 1
                else:
                    language_stats["mixed"] += 1

        # Экстраполируем на весь корпус
        sample_size = min(100, len(corpus))
        scale_factor = len(corpus) / sample_size

        return {
            "russian": int(language_stats["russian"] * scale_factor),
            "english": int(language_stats["english"] * scale_factor),
            "mixed": int(language_stats["mixed"] * scale_factor),
            "total": len(corpus),
        }

    def _adapt_params_for_language(self, language_info: dict) -> dict:
        """
        Адаптация параметров обучения на основе языкового состава

        Args:
            language_info: Статистика языков в корпусе

        Returns:
            Адаптированные параметры
        """
        total = language_info["total"]
        if total == 0:
            return {}

        # Вычисляем процентное соотношение
        russian_pct = language_info["russian"] / total
        english_pct = language_info["english"] / total
        mixed_pct = language_info["mixed"] / total

        adapted_params = {}

        # Адаптация размерности векторов
        if mixed_pct > 0.3 or (russian_pct > 0.2 and english_pct > 0.2):
            # Много смешанных документов или оба языка представлены значительно
            adapted_params["vector_size"] = min(400, self.vector_size + 50)
            logger.info(
                f"Увеличена размерность векторов до {adapted_params['vector_size']} для многоязычного корпуса"
            )

        # Адаптация размера окна
        if english_pct > 0.5:
            # Английские тексты часто имеют более короткие предложения
            adapted_params["window"] = max(10, self.window - 2)
        elif mixed_pct > 0.3:
            # Смешанные тексты требуют большего контекста
            adapted_params["window"] = min(20, self.window + 3)

        # Адаптация минимальной частоты
        if total < 100:
            # Маленький корпус - снижаем порог
            adapted_params["min_count"] = max(1, self.min_count - 1)
        elif mixed_pct > 0.3:
            # Смешанный корпус - повышаем порог для фильтрации шума
            adapted_params["min_count"] = self.min_count + 1

        return adapted_params

    def cancel(self):
        """Отмена обучения"""
        self.is_cancelled = True


class SearchThread(QThread):
    """Поток для поиска"""

    results = pyqtSignal(list)
    error = pyqtSignal(str)

    def __init__(self, search_engine: SemanticSearchEngine, query: str, top_k: int):
        super().__init__()
        self.search_engine = search_engine
        self.query = query
        self.top_k = top_k

    def run(self):
        """Выполнение поиска"""
        try:
            results = self.search_engine.search(self.query, top_k=self.top_k)
            self.results.emit(results)
        except Exception as e:
            logger.error(f"Ошибка поиска: {e}")
            self.error.emit(str(e))


class MainWindow(QMainWindow):
    """Главное окно приложения"""

    def __init__(self):
        super().__init__()
        self.current_model = None
        self.search_engine = None
        self.training_thread = None
        self.search_thread = None
        self.summarizer = None

        self.init_ui()
        self.load_models()

    def init_ui(self):
        """Инициализация интерфейса"""
        self.setWindowTitle(GUI_CONFIG["window_title"])
        self.setGeometry(100, 100, *GUI_CONFIG["window_size"])

        # Центральный виджет
        central_widget = QWidget()
        self.setCentralWidget(central_widget)

        # Основной layout
        main_layout = QVBoxLayout(central_widget)

        # Создаем меню
        self.create_menu_bar()

        # Создаем панель инструментов
        self.create_toolbar()

        # Создаем вкладки
        self.tab_widget = QTabWidget()
        main_layout.addWidget(self.tab_widget)

        # Вкладка обучения
        self.create_training_tab()

        # Вкладка поиска
        self.create_search_tab()

        # Вкладка суммаризации
        self.create_summarization_tab()

        # Вкладка статистики
        self.create_statistics_tab()

        # Вкладка оценки и сравнения
        self.create_evaluation_tab()

        # Статус бар
        self.status_bar = QStatusBar()
        self.setStatusBar(self.status_bar)
        self.status_bar.showMessage("Готов к работе")

        # Применяем стили
        self.setStyleSheet("""
            QMainWindow {
                background-color: #f5f5f5;
            }
            QTabWidget::pane {
                border: 1px solid #ddd;
                background-color: white;
            }
            QTabBar::tab {
                padding: 8px 16px;
                margin-right: 2px;
            }
            QTabBar::tab:selected {
                background-color: white;
                border-bottom: 2px solid #0066cc;
            }
            QPushButton {
                padding: 6px 12px;
                border: 1px solid #ddd;
                border-radius: 4px;
                background-color: #0066cc;
                color: white;
                font-weight: bold;
            }
            QPushButton:hover {
                background-color: #0052a3;
            }
            QPushButton:pressed {
                background-color: #004080;
            }
            QLineEdit, QTextEdit, QListWidget {
                border: 1px solid #ddd;
                border-radius: 4px;
                padding: 4px;
            }
            QProgressBar {
                border: 1px solid #ddd;
                border-radius: 4px;
                text-align: center;
            }
            QProgressBar::chunk {
                background-color: #0066cc;
                border-radius: 3px;
            }
        """)

    def create_menu_bar(self):
        """Создание меню"""
        menubar = self.menuBar()

        # Меню Файл
        file_menu = menubar.addMenu("Файл")

        exit_action = QAction("Выход", self)
        exit_action.setShortcut("Ctrl+Q")
        exit_action.triggered.connect(self.close)
        file_menu.addAction(exit_action)

        # Меню Модель
        model_menu = menubar.addMenu("Модель")

        load_model_action = QAction("Загрузить модель", self)
        load_model_action.triggered.connect(self.load_model_dialog)
        model_menu.addAction(load_model_action)

        # Меню Помощь
        help_menu = menubar.addMenu("Помощь")

        about_action = QAction("О программе", self)
        about_action.triggered.connect(self.show_about)
        help_menu.addAction(about_action)

    def create_toolbar(self):
        """Создание панели инструментов"""
        toolbar = QToolBar()
        toolbar.setMovable(False)
        self.addToolBar(toolbar)

        # Комбобокс для выбора модели
        self.model_combo = QComboBox()
        self.model_combo.setMinimumWidth(200)
        self.model_combo.currentTextChanged.connect(self.on_model_changed)

        toolbar.addWidget(QLabel("Модель: "))
        toolbar.addWidget(self.model_combo)
        toolbar.addSeparator()

        # Индикатор статуса модели
        self.model_status_label = QLabel("Модель не загружена")
        self.model_status_label.setStyleSheet("color: red;")
        toolbar.addWidget(self.model_status_label)

    def create_search_tab(self):
        """Создание вкладки поиска"""
        search_widget = QWidget()
        layout = QVBoxLayout(search_widget)

        # Панель поиска
        search_panel = QWidget()
        search_layout = QHBoxLayout(search_panel)

        self.search_input = QLineEdit()
        self.search_input.setPlaceholderText("Введите поисковый запрос...")
        self.search_input.returnPressed.connect(self.perform_search)

        self.search_button = QPushButton("Поиск")
        self.search_button.clicked.connect(self.perform_search)

        self.results_count_spin = QSpinBox()
        self.results_count_spin.setMinimum(1)
        self.results_count_spin.setMaximum(100)
        self.results_count_spin.setValue(10)

        search_layout.addWidget(self.search_input)
        search_layout.addWidget(QLabel("Результатов:"))
        search_layout.addWidget(self.results_count_spin)
        search_layout.addWidget(self.search_button)

        layout.addWidget(search_panel)

        # Разделитель для результатов
        splitter = QSplitter(Qt.Orientation.Horizontal)

        # Список результатов
        self.results_list = QListWidget()
        self.results_list.itemClicked.connect(self.on_result_selected)
        splitter.addWidget(self.results_list)

        # Просмотр документа
        self.document_viewer = QTextEdit()
        self.document_viewer.setReadOnly(True)
        splitter.addWidget(self.document_viewer)

        splitter.setSizes([400, 600])
        layout.addWidget(splitter)

        self.tab_widget.addTab(search_widget, "🔍 Поиск")

    def create_training_tab(self):
        """Создание вкладки обучения"""
        training_widget = QWidget()
        layout = QVBoxLayout(training_widget)

        # Группа выбора документов
        docs_group = QGroupBox("Документы для обучения")
        docs_layout = QHBoxLayout()

        self.docs_path_edit = QLineEdit()
        self.docs_path_edit.setPlaceholderText("Путь к папке с документами...")

        browse_button = QPushButton("Обзор...")
        browse_button.clicked.connect(self.browse_documents)

        docs_layout.addWidget(self.docs_path_edit)
        docs_layout.addWidget(browse_button)
        docs_group.setLayout(docs_layout)

        layout.addWidget(docs_group)

        # Параметры модели
        params_group = QGroupBox("Параметры модели")
        params_layout = QVBoxLayout()

        # Имя модели
        name_layout = QHBoxLayout()
        name_layout.addWidget(QLabel("Имя модели:"))
        self.model_name_edit = QLineEdit("doc2vec_model")
        name_layout.addWidget(self.model_name_edit)
        params_layout.addLayout(name_layout)

        # Выбор пресета
        preset_layout = QHBoxLayout()
        preset_layout.addWidget(QLabel("Пресет настроек:"))
        self.preset_combo = QComboBox()
        self.preset_combo.addItems(
            [
                "Сбалансированный (рекомендуется)",
                "Быстрый (для тестирования)",
                "Качественный (медленный)",
                "Пользовательский",
            ]
        )
        self.preset_combo.currentIndexChanged.connect(self.on_preset_changed)
        preset_layout.addWidget(self.preset_combo)
        params_layout.addLayout(preset_layout)

        # Размерность векторов
        vector_layout = QHBoxLayout()
        vector_layout.addWidget(QLabel("Размерность векторов:"))
        self.vector_size_spin = QSpinBox()
        self.vector_size_spin.setMinimum(50)
        self.vector_size_spin.setMaximum(500)
        self.vector_size_spin.setValue(300)
        vector_layout.addWidget(self.vector_size_spin)
        params_layout.addLayout(vector_layout)

        # Количество эпох
        epochs_layout = QHBoxLayout()
        epochs_layout.addWidget(QLabel("Количество эпох:"))
        self.epochs_spin = QSpinBox()
        self.epochs_spin.setMinimum(1)
        self.epochs_spin.setMaximum(100)
        self.epochs_spin.setValue(30)
        epochs_layout.addWidget(self.epochs_spin)
        params_layout.addLayout(epochs_layout)

        # Дополнительные параметры (расширенные)
        advanced_group = QGroupBox("Расширенные параметры (необязательно)")
        advanced_layout = QVBoxLayout()

        # Размер окна
        window_layout = QHBoxLayout()
        window_layout.addWidget(QLabel("Размер окна контекста:"))
        self.window_spin = QSpinBox()
        self.window_spin.setMinimum(5)
        self.window_spin.setMaximum(50)
        self.window_spin.setValue(15)
        window_layout.addWidget(self.window_spin)
        advanced_layout.addLayout(window_layout)

        # Минимальная частота
        min_count_layout = QHBoxLayout()
        min_count_layout.addWidget(QLabel("Минимальная частота слова:"))
        self.min_count_spin = QSpinBox()
        self.min_count_spin.setMinimum(1)
        self.min_count_spin.setMaximum(10)
        self.min_count_spin.setValue(3)
        min_count_layout.addWidget(self.min_count_spin)
        advanced_layout.addLayout(min_count_layout)

        # DM режим
        dm_layout = QHBoxLayout()
        dm_layout.addWidget(QLabel("Режим обучения:"))
        self.dm_combo = QComboBox()
        self.dm_combo.addItems(
            ["Distributed Memory (DM)", "Distributed Bag of Words (DBOW)"]
        )
        self.dm_combo.setCurrentIndex(0)
        dm_layout.addWidget(self.dm_combo)
        advanced_layout.addLayout(dm_layout)

        # Negative sampling
        negative_layout = QHBoxLayout()
        negative_layout.addWidget(QLabel("Negative sampling:"))
        self.negative_spin = QSpinBox()
        self.negative_spin.setMinimum(0)
        self.negative_spin.setMaximum(20)
        self.negative_spin.setValue(10)
        negative_layout.addWidget(self.negative_spin)
        advanced_layout.addLayout(negative_layout)

        advanced_group.setLayout(advanced_layout)
        layout.addWidget(advanced_group)

        params_group.setLayout(params_layout)
        layout.addWidget(params_group)

        # Кнопка обучения
        self.train_button = QPushButton("Начать обучение")
        self.train_button.clicked.connect(self.start_training)
        layout.addWidget(self.train_button)

        # Прогресс бар
        self.training_progress = QProgressBar()
        self.training_progress.setVisible(False)
        layout.addWidget(self.training_progress)

        # Лог обучения
        self.training_log = QTextEdit()
        self.training_log.setReadOnly(True)
        layout.addWidget(self.training_log)

        self.tab_widget.addTab(training_widget, "🧠 Обучение")

    def on_preset_changed(self, index):
        """Обработка изменения пресета настроек"""
        if index == 0:  # Сбалансированный
            self.vector_size_spin.setValue(300)
            self.epochs_spin.setValue(30)
            self.window_spin.setValue(15)
            self.min_count_spin.setValue(3)
            self.negative_spin.setValue(10)
            self.training_log.append(
                "📋 Выбран сбалансированный пресет (рекомендуется для большинства случаев)"
            )

        elif index == 1:  # Быстрый
            self.vector_size_spin.setValue(200)
            self.epochs_spin.setValue(15)
            self.window_spin.setValue(10)
            self.min_count_spin.setValue(5)
            self.negative_spin.setValue(5)
            self.training_log.append("⚡ Выбран быстрый пресет (для тестирования)")

        elif index == 2:  # Качественный
            self.vector_size_spin.setValue(400)
            self.epochs_spin.setValue(50)
            self.window_spin.setValue(20)
            self.min_count_spin.setValue(2)
            self.negative_spin.setValue(15)
            self.training_log.append(
                "🏆 Выбран качественный пресет (максимальное качество, медленное обучение)"
            )

        elif index == 3:  # Пользовательский
            self.training_log.append(
                "🔧 Пользовательский режим - настройте параметры вручную"
            )

    def create_summarization_tab(self):
        """Создание вкладки суммаризации"""
        summary_widget = QWidget()
        layout = QVBoxLayout(summary_widget)

        # Выбор файла
        file_group = QGroupBox("Выбор документа")
        file_layout = QHBoxLayout()

        self.summary_file_edit = QLineEdit()
        self.summary_file_edit.setPlaceholderText("Путь к файлу...")

        browse_file_button = QPushButton("Обзор...")
        browse_file_button.clicked.connect(self.browse_summary_file)

        file_layout.addWidget(self.summary_file_edit)
        file_layout.addWidget(browse_file_button)
        file_group.setLayout(file_layout)

        layout.addWidget(file_group)

        # Параметры суммаризации
        params_group = QGroupBox("Параметры выжимки")
        params_layout = QVBoxLayout()

        # Количество предложений
        sentences_layout = QHBoxLayout()
        sentences_layout.addWidget(QLabel("Количество предложений:"))

        self.sentences_spin = QSpinBox()
        self.sentences_spin.setMinimum(1)
        self.sentences_spin.setMaximum(20)
        self.sentences_spin.setValue(5)
        self.sentences_spin.setToolTip("Количество предложений в выжимке")
        sentences_layout.addWidget(self.sentences_spin)

        sentences_layout.addStretch()
        params_layout.addLayout(sentences_layout)

        # Минимальная длина предложения
        min_length_layout = QHBoxLayout()
        min_length_layout.addWidget(QLabel("Минимальная длина предложения:"))

        self.min_sentence_length_spin = QSpinBox()
        self.min_sentence_length_spin.setMinimum(10)
        self.min_sentence_length_spin.setMaximum(100)
        self.min_sentence_length_spin.setValue(15)
        self.min_sentence_length_spin.setSuffix(" символов")
        self.min_sentence_length_spin.setToolTip(
            "Предложения короче этого значения не будут включены в выжимку"
        )
        min_length_layout.addWidget(self.min_sentence_length_spin)

        min_length_layout.addStretch()
        params_layout.addLayout(min_length_layout)

        # Минимальное количество слов
        min_words_layout = QHBoxLayout()
        min_words_layout.addWidget(QLabel("Минимум слов в предложении:"))

        self.min_words_spin = QSpinBox()
        self.min_words_spin.setMinimum(3)
        self.min_words_spin.setMaximum(20)
        self.min_words_spin.setValue(5)
        self.min_words_spin.setToolTip(
            "Предложения с меньшим количеством слов будут отфильтрованы"
        )
        min_words_layout.addWidget(self.min_words_spin)

        min_words_layout.addStretch()
        params_layout.addLayout(min_words_layout)

        # Флажок для фильтрации
        self.filter_short_checkbox = QCheckBox(
            "Фильтровать короткие и малоинформативные предложения"
        )
        self.filter_short_checkbox.setChecked(True)
        self.filter_short_checkbox.toggled.connect(self.on_filter_toggled)
        params_layout.addWidget(self.filter_short_checkbox)

        params_group.setLayout(params_layout)
        layout.addWidget(params_group)

        # Кнопка создания выжимки
        button_layout = QHBoxLayout()
        self.summarize_button = QPushButton("Создать выжимку")
        self.summarize_button.clicked.connect(self.create_summary)
        button_layout.addWidget(self.summarize_button)

        # Кнопка сохранения выжимки
        self.save_summary_button = QPushButton("Сохранить выжимку")
        self.save_summary_button.clicked.connect(self.save_summary)
        self.save_summary_button.setEnabled(False)
        button_layout.addWidget(self.save_summary_button)

        button_layout.addStretch()
        layout.addLayout(button_layout)

        # Результат
        splitter = QSplitter(Qt.Orientation.Vertical)

        # Оригинальный текст
        original_group = QGroupBox("Оригинальный текст")
        original_layout = QVBoxLayout()
        self.original_text = QTextEdit()
        self.original_text.setReadOnly(True)
        original_layout.addWidget(self.original_text)
        original_group.setLayout(original_layout)
        splitter.addWidget(original_group)

        # Выжимка
        summary_group = QGroupBox("Выжимка")
        summary_layout = QVBoxLayout()
        self.summary_text = QTextEdit()
        self.summary_text.setReadOnly(True)
        summary_layout.addWidget(self.summary_text)
        summary_group.setLayout(summary_layout)
        splitter.addWidget(summary_group)

        layout.addWidget(splitter)

        self.tab_widget.addTab(summary_widget, "📝 Суммаризация")

        # Сохраняем текущую выжимку для возможности сохранения
        self.current_summary = []

    def on_filter_toggled(self, checked):
        """Обработчик переключения фильтрации"""
        self.min_sentence_length_spin.setEnabled(checked)
        self.min_words_spin.setEnabled(checked)

        if checked:
            self.status_bar.showMessage("Фильтрация коротких предложений включена")
        else:
            self.status_bar.showMessage(
                "Фильтрация отключена - все предложения будут учитываться"
            )

    def save_summary(self):
        """Сохранение выжимки в файл"""
        if not self.current_summary:
            QMessageBox.warning(self, "Ошибка", "Нет выжимки для сохранения")
            return

        file_path, _ = QFileDialog.getSaveFileName(
            self, "Сохранить выжимку", "", "Текстовые файлы (*.txt);;Все файлы (*.*)"
        )

        if file_path:
            try:
                with open(file_path, "w", encoding="utf-8") as f:
                    # Заголовок
                    f.write(f"Выжимка документа: {self.summary_file_edit.text()}\n")
                    f.write(f"Создано: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write("=" * 60 + "\n\n")

                    # Предложения выжимки
                    for i, sentence in enumerate(self.current_summary, 1):
                        f.write(f"{i}. {sentence.strip()}\n\n")

                    # Статистика если есть
                    if hasattr(self, "last_summary_stats"):
                        f.write("\n" + "=" * 60 + "\n")
                        f.write("СТАТИСТИКА СУММАРИЗАЦИИ\n")
                        f.write("=" * 60 + "\n")
                        stats = self.last_summary_stats
                        f.write(
                            f"Исходных предложений: {stats['original_sentences_count']}\n"
                        )
                        f.write(
                            f"Валидных предложений: {stats.get('valid_original_sentences_count', 'н/д')}\n"
                        )
                        f.write(
                            f"Предложений в выжимке: {stats['summary_sentences_count']}\n"
                        )
                        f.write(
                            f"Коэффициент сжатия: {stats['compression_ratio']:.1%}\n"
                        )
                        f.write(
                            f"Средняя длина предложения: {stats.get('avg_sentence_length', 0):.1f} слов\n"
                        )

                QMessageBox.information(
                    self, "Успех", f"Выжимка сохранена в:\n{file_path}"
                )

            except Exception as e:
                QMessageBox.critical(
                    self, "Ошибка", f"Ошибка при сохранении:\n{str(e)}"
                )

    def create_statistics_tab(self):
        """Создание вкладки статистики"""
        stats_widget = QWidget()
        layout = QVBoxLayout(stats_widget)

        # Кнопка обновления
        refresh_button = QPushButton("Обновить статистику")
        refresh_button.clicked.connect(self.update_statistics)
        layout.addWidget(refresh_button)

        # Текстовое поле для статистики
        self.statistics_text = QTextEdit()
        self.statistics_text.setReadOnly(True)
        self.statistics_text.setFont(QFont("Consolas", 10))
        layout.addWidget(self.statistics_text)

        self.tab_widget.addTab(stats_widget, "📊 Статистика")

    def create_evaluation_tab(self):
        """Создание вкладки оценки и сравнения"""
        self.evaluation_widget = EvaluationWidget()
        self.tab_widget.addTab(self.evaluation_widget, "📚 Оценка методов")

    def load_models(self):
        """Загрузка списка доступных моделей"""
        self.model_combo.clear()
        self.model_combo.addItem("Не выбрано")

        try:
            # Создаем директорию если её нет
            MODELS_DIR.mkdir(exist_ok=True, parents=True)

            # Ищем модели в директории
            model_files = list(MODELS_DIR.glob("*.model"))

            for model_file in model_files:
                model_name = model_file.stem
                if model_name:  # Проверяем что имя не пустое
                    self.model_combo.addItem(model_name)

            if len(model_files) > 0:
                self.model_combo.setCurrentIndex(1)
        except Exception as e:
            logger.error(f"Ошибка при загрузке списка моделей: {e}")
            QMessageBox.warning(
                self, "Ошибка", f"Не удалось загрузить список моделей: {e}"
            )

    def on_model_changed(self, model_name: str):
        """Обработчик изменения модели"""
        if not model_name or model_name == "Не выбрано":
            self.current_model = None
            self.search_engine = None
            self.summarizer = None
            self.model_status_label.setText("Модель не загружена")
            self.model_status_label.setStyleSheet("color: red;")

            # Отключаем evaluation widget
            if hasattr(self, "evaluation_widget"):
                self.evaluation_widget.set_search_engine(None, None)
            return

        # Загружаем модель
        try:
            logger.info(f"Загрузка модели: {model_name}")
            trainer = Doc2VecTrainer()
            model = trainer.load_model(model_name)

            if model:
                self.current_model = model

                # Создаем SearchEngine с базовым путем
                self.search_engine = SemanticSearchEngine(
                    model,
                    trainer.corpus_info,
                    trainer.documents_base_path,  # Передаем базовый путь
                )

                self.summarizer = TextSummarizer(model)

                # Передаем данные в evaluation widget
                if hasattr(self, "evaluation_widget"):
                    self.evaluation_widget.set_search_engine(
                        self.search_engine, trainer.corpus_info
                    )

                # Обновляем статус
                status_text = f"Модель '{model_name}' загружена"
                if trainer.documents_base_path:
                    status_text += f" (база: {trainer.documents_base_path.name})"

                self.model_status_label.setText(status_text)
                self.model_status_label.setStyleSheet("color: green;")

                self.status_bar.showMessage(f"Модель '{model_name}' успешно загружена")

                # Логирование для отладки
                if trainer.documents_base_path:
                    logger.info(
                        f"Базовый путь документов: {trainer.documents_base_path}"
                    )
                    logger.info(
                        f"Путь существует: {trainer.documents_base_path.exists()}"
                    )
                else:
                    logger.warning("Базовый путь документов не загружен из модели")

            else:
                logger.error(f"Модель {model_name} не может быть загружена")
                self.current_model = None
                self.search_engine = None
                self.summarizer = None
                self.model_status_label.setText("Ошибка загрузки модели")
                self.model_status_label.setStyleSheet("color: red;")

                if hasattr(self, "evaluation_widget"):
                    self.evaluation_widget.set_search_engine(None, None)

                QMessageBox.warning(
                    self, "Ошибка", f"Не удалось загрузить модель '{model_name}'"
                )
        except Exception as e:
            logger.error(f"Исключение при загрузке модели: {e}", exc_info=True)
            QMessageBox.critical(
                self, "Ошибка", f"Ошибка при загрузке модели: {str(e)}"
            )

    def browse_documents(self):
        """Выбор папки с документами"""
        folder = QFileDialog.getExistingDirectory(self, "Выберите папку с документами")
        if folder:
            self.docs_path_edit.setText(folder)

    def browse_summary_file(self):
        """Выбор файла для суммаризации"""
        file_path, _ = QFileDialog.getOpenFileName(
            self, "Выберите файл", "", "Документы (*.pdf *.docx *.doc);;Все файлы (*.*)"
        )
        if file_path:
            self.summary_file_edit.setText(file_path)

    def start_training(self):
        """Начало обучения модели с сохранением конфигурации"""
        documents_path = self.docs_path_edit.text()
        if not documents_path:
            QMessageBox.warning(self, "Ошибка", "Выберите папку с документами")
            return

        documents_path = Path(documents_path)
        if not documents_path.exists():
            QMessageBox.warning(self, "Ошибка", "Указанная папка не существует")
            return

        model_name = self.model_name_edit.text().strip()
        if not model_name:
            QMessageBox.warning(self, "Ошибка", "Введите имя модели")
            return

        # Проверяем валидность имени модели
        if "/" in model_name or "\\" in model_name or ":" in model_name:
            QMessageBox.warning(
                self, "Ошибка", "Имя модели содержит недопустимые символы"
            )
            return

        # Проверяем, не существует ли уже такая модель
        existing_model = MODELS_DIR / f"{model_name}.model"
        if existing_model.exists():
            reply = QMessageBox.question(
                self,
                "Подтверждение",
                f"Модель '{model_name}' уже существует. Перезаписать?",
                QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No,
            )
            if reply == QMessageBox.StandardButton.No:
                return

        # Собираем текущие параметры из GUI
        current_params = {
            "vector_size": self.vector_size_spin.value(),
            "window": self.window_spin.value(),
            "min_count": self.min_count_spin.value(),
            "epochs": self.epochs_spin.value(),
            "dm": 1 if self.dm_combo.currentIndex() == 0 else 0,
            "negative": self.negative_spin.value(),
        }

        # Обновляем конфигурацию если параметры изменились
        from semantic_search.config import config_manager

        config_changed = False
        current_config = config_manager.config.doc2vec

        for param, value in current_params.items():
            if current_config.get(param) != value:
                config_changed = True
                break

        if config_changed:
            # Обновляем конфигурацию
            config_manager.update_config(doc2vec=current_params)
            logger.info("Конфигурация обновлена с новыми параметрами обучения")

            # Информируем пользователя
            self.status_bar.showMessage("Параметры обучения сохранены в конфигурацию")

        # Отключаем кнопку
        self.train_button.setEnabled(False)
        self.training_progress.setVisible(True)
        self.training_progress.setValue(0)

        # Очищаем лог
        self.training_log.clear()
        self.training_log.append("Начинаем обучение модели...\n")

        # Показываем используемые параметры
        self.training_log.append("📋 Параметры обучения:")
        self.training_log.append(
            f"   Размерность векторов: {current_params['vector_size']}"
        )
        self.training_log.append(f"   Размер окна: {current_params['window']}")
        self.training_log.append(
            f"   Минимальная частота: {current_params['min_count']}"
        )
        self.training_log.append(f"   Количество эпох: {current_params['epochs']}")
        self.training_log.append(
            f"   Режим: {'DM' if current_params['dm'] == 1 else 'DBOW'}"
        )
        self.training_log.append(
            f"   Negative sampling: {current_params['negative']}\n"
        )

        # Определяем пресет
        preset_index = self.preset_combo.currentIndex()
        preset_map = {0: "balanced", 1: "fast", 2: "quality", 3: None}
        preset = preset_map.get(preset_index)

        # Создаем и запускаем поток
        self.training_thread = TrainingThread(
            documents_path,
            model_name,
            current_params["vector_size"],
            current_params["epochs"],
            window=current_params["window"],
            min_count=current_params["min_count"],
            dm=current_params["dm"],
            negative=current_params["negative"],
            preset=preset,
        )

        self.training_thread.progress.connect(self.on_training_progress)
        self.training_thread.finished.connect(self.on_training_finished)
        self.training_thread.statistics.connect(self.on_training_statistics)

        self.training_thread.start()

    def on_training_progress(self, value: int, message: str):
        """Обновление прогресса обучения"""
        self.training_progress.setValue(value)
        self.training_log.append(message)
        self.status_bar.showMessage(message)

    def on_training_statistics(self, stats: dict):
        """Отображение статистики корпуса"""
        stats_text = format_statistics_for_display(stats)
        self.training_log.append("\n" + stats_text + "\n")

    def on_training_finished(self, success: bool, message: str):
        """Завершение обучения"""
        self.train_button.setEnabled(True)
        self.training_progress.setVisible(False)

        if success:
            self.training_log.append(f"\n✅ {message}")
            QMessageBox.information(self, "Успех", message)

            # Обновляем список моделей
            self.load_models()

            # Пытаемся выбрать только что обученную модель
            model_name = self.model_name_edit.text().strip()
            if model_name:
                index = self.model_combo.findText(model_name)
                if index >= 0:
                    self.model_combo.setCurrentIndex(index)
                else:
                    logger.warning(
                        f"Не удалось найти модель '{model_name}' в списке после обучения"
                    )
        else:
            self.training_log.append(f"\n❌ {message}")
            QMessageBox.critical(self, "Ошибка", message)

        self.status_bar.showMessage("Готов к работе")

    def perform_search(self):
        """Выполнение поиска"""
        if not self.search_engine:
            QMessageBox.warning(self, "Ошибка", "Сначала загрузите модель")
            return

        query = self.search_input.text().strip()
        if not query:
            QMessageBox.warning(self, "Ошибка", "Введите поисковый запрос")
            return

        # Очищаем предыдущие результаты
        self.results_list.clear()
        self.document_viewer.clear()

        # Отключаем кнопку
        self.search_button.setEnabled(False)
        self.status_bar.showMessage("Выполняется поиск...")

        # Создаем поток поиска
        self.search_thread = SearchThread(
            self.search_engine, query, self.results_count_spin.value()
        )

        self.search_thread.results.connect(self.on_search_results)
        self.search_thread.error.connect(self.on_search_error)

        self.search_thread.start()

    def on_search_results(self, results: List):
        """Обработка результатов поиска"""
        self.search_button.setEnabled(True)

        if not results:
            self.status_bar.showMessage("Результатов не найдено")
            QMessageBox.information(
                self, "Поиск", "По вашему запросу ничего не найдено"
            )
            return

        self.status_bar.showMessage(f"Найдено результатов: {len(results)}")

        # Отображаем результаты
        for i, result in enumerate(results, 1):
            item = QListWidgetItem(
                f"{i}. {result.doc_id} (схожесть: {result.similarity:.3f})"
            )
            item.setData(Qt.ItemDataRole.UserRole, result)
            self.results_list.addItem(item)

    def on_search_error(self, error: str):
        """Обработка ошибки поиска"""
        self.search_button.setEnabled(True)
        self.status_bar.showMessage("Ошибка поиска")
        QMessageBox.critical(self, "Ошибка", f"Ошибка при поиске: {error}")

    def on_result_selected(self, item: QListWidgetItem):
        """Обработка выбора результата"""
        result = item.data(Qt.ItemDataRole.UserRole)

        try:
            # Получаем относительный путь из результата
            relative_path = result.doc_id

            logger.info(f"Выбран документ: {relative_path}")

            file_path = None

            # Основной способ: используем базовый путь из SearchEngine
            if (
                self.search_engine
                and hasattr(self.search_engine, "documents_base_path")
                and self.search_engine.documents_base_path
            ):
                # Строим полный путь: базовый путь + относительный путь
                file_path = self.search_engine.documents_base_path / relative_path

                logger.info(f"Базовый путь: {self.search_engine.documents_base_path}")
                logger.info(f"Полный путь: {file_path}")
                logger.info(f"Файл существует: {file_path.exists()}")

                # Если файл не найден, пробуем с нормализацией слешей
                if not file_path.exists():
                    # Нормализуем слеши для текущей ОС
                    normalized_relative = relative_path.replace("/", os.sep).replace(
                        "\\", os.sep
                    )
                    file_path = (
                        self.search_engine.documents_base_path / normalized_relative
                    )

                    if file_path.exists():
                        logger.info("Файл найден после нормализации путей")
            else:
                logger.warning("Базовый путь не доступен в SearchEngine")

            # Запасной вариант: проверяем полный путь из метаданных
            if (
                (not file_path or not file_path.exists())
                and result.metadata
                and "full_path" in result.metadata
            ):
                test_path = Path(result.metadata["full_path"])
                if test_path.exists():
                    file_path = test_path
                    logger.info(f"Использован full_path из метаданных: {file_path}")

            # Отображение результата
            if file_path and file_path.exists():
                extractor = FileExtractor()
                text = extractor.extract_text(file_path)

                if text:
                    # Показываем первые 5000 символов
                    preview = text[:5000]
                    if len(text) > 5000:
                        preview += "\n\n... (текст обрезан) ..."

                    self.document_viewer.setPlainText(preview)

                    # Добавляем метаданные
                    metadata_text = "\n\n" + "=" * 60 + "\n"
                    metadata_text += "ИНФОРМАЦИЯ О ДОКУМЕНТЕ\n"
                    metadata_text += "=" * 60 + "\n"
                    metadata_text += f"📄 Документ: {relative_path}\n"
                    metadata_text += f"📁 Полный путь: {file_path}\n"
                    metadata_text += f"📊 Схожесть: {result.similarity:.3f}\n"

                    if self.search_engine and self.search_engine.documents_base_path:
                        metadata_text += f"📂 Базовая папка модели: {self.search_engine.documents_base_path}\n"

                    if result.metadata:
                        metadata_text += (
                            f"💾 Размер: {result.metadata.get('file_size', 0):,} байт\n"
                        )
                        metadata_text += (
                            f"📝 Токенов: {result.metadata.get('tokens_count', 0):,}\n"
                        )
                        metadata_text += f"📑 Расширение: {result.metadata.get('extension', 'н/д')}\n"

                    self.document_viewer.append(metadata_text)
                else:
                    self.document_viewer.setPlainText(
                        f"❌ Не удалось извлечь текст из документа:\n{file_path}\n\n"
                        f"Возможно, файл поврежден или имеет неподдерживаемый формат."
                    )
            else:
                # Подробное сообщение об ошибке
                error_msg = "❌ ФАЙЛ НЕ НАЙДЕН\n\n"
                error_msg += f"Искомый документ: {relative_path}\n\n"

                if self.search_engine and hasattr(
                    self.search_engine, "documents_base_path"
                ):
                    if self.search_engine.documents_base_path:
                        error_msg += f"Базовая папка модели: {self.search_engine.documents_base_path}\n"
                        error_msg += f"Ожидаемый путь: {self.search_engine.documents_base_path / relative_path}\n"
                        error_msg += f"Базовая папка существует: {'✅ Да' if self.search_engine.documents_base_path.exists() else '❌ Нет'}\n"
                    else:
                        error_msg += "⚠️ Базовый путь не сохранен в модели\n"

                error_msg += "\n📋 Возможные причины:\n"
                error_msg += "1. Файлы были перемещены после обучения модели\n"
                error_msg += "2. Модель была обучена на другом компьютере\n"
                error_msg += "3. Изменилась структура папок\n"

                if not (self.search_engine and self.search_engine.documents_base_path):
                    error_msg += "4. Модель была обучена старой версией программы без сохранения базового пути\n"

                error_msg += "\n💡 Рекомендации:\n"
                error_msg += "• Переобучите модель с текущим расположением документов\n"
                error_msg += "• Или переместите документы в исходное расположение\n"

                self.document_viewer.setPlainText(error_msg)

        except Exception as e:
            logger.error(f"Ошибка при отображении документа: {e}", exc_info=True)
            self.document_viewer.setPlainText(
                f"❌ ОШИБКА ПРИ ЗАГРУЗКЕ ДОКУМЕНТА\n\n"
                f"Документ: {result.doc_id}\n"
                f"Ошибка: {str(e)}\n\n"
                f"Проверьте логи для подробной информации."
            )

    def create_summary(self):
        """Создание выжимки документа с учетом параметров фильтрации"""
        if not self.summarizer:
            QMessageBox.warning(self, "Ошибка", "Сначала загрузите модель")
            return

        file_path = self.summary_file_edit.text()
        if not file_path:
            QMessageBox.warning(self, "Ошибка", "Выберите файл для суммаризации")
            return

        file_path = Path(file_path)
        if not file_path.exists():
            QMessageBox.warning(self, "Ошибка", "Файл не существует")
            return

        try:
            # Загружаем текст
            extractor = FileExtractor()
            text = extractor.extract_text(file_path)

            if not text:
                QMessageBox.warning(self, "Ошибка", "Не удалось извлечь текст из файла")
                return

            # Проверяем размер текста
            text_length = len(text)
            logger.info(f"Загружен текст длиной {text_length} символов")

            # Предупреждение для очень больших файлов
            if text_length > 2_000_000:
                reply = QMessageBox.question(
                    self,
                    "Большой файл",
                    f"Файл содержит {text_length:,} символов.\n"
                    "Обработка может занять несколько минут.\n"
                    "Продолжить?",
                    QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No,
                )
                if reply == QMessageBox.StandardButton.No:
                    return

            # Показываем оригинальный текст (ограничиваем превью)
            preview_length = min(5000, text_length)
            preview = text[:preview_length]
            if text_length > preview_length:
                preview += f"\n\n... (показаны первые {preview_length:,} из {text_length:,} символов) ..."
            self.original_text.setPlainText(preview)

            # Обновляем параметры суммаризатора если включена фильтрация
            if self.filter_short_checkbox.isChecked():
                self.summarizer.min_summary_sentence_length = (
                    self.min_sentence_length_spin.value()
                )
                self.summarizer.min_words_in_sentence = self.min_words_spin.value()
            else:
                # Если фильтрация отключена, устанавливаем минимальные значения
                self.summarizer.min_summary_sentence_length = 1
                self.summarizer.min_words_in_sentence = 1

            # Создаем выжимку
            self.status_bar.showMessage(
                "Создание выжимки... Это может занять время для больших файлов"
            )
            QApplication.processEvents()  # Обновляем UI

            sentences_count = self.sentences_spin.value()

            # Отключаем кнопки на время обработки
            self.summarize_button.setEnabled(False)
            self.save_summary_button.setEnabled(False)

            try:
                # Создаем выжимку с учетом фильтрации
                summary = self.summarizer.summarize_text(
                    text,
                    sentences_count=sentences_count,
                    min_sentence_length=self.min_sentence_length_spin.value()
                    if self.filter_short_checkbox.isChecked()
                    else None,
                )

                if summary:
                    # Сохраняем текущую выжимку
                    self.current_summary = summary

                    # Показываем выжимку
                    summary_text = "\n\n".join(
                        f"{i}. {sent.strip()}" for i, sent in enumerate(summary, 1)
                    )
                    self.summary_text.setPlainText(summary_text)

                    # Показываем статистику
                    stats = self.summarizer.get_summary_statistics(text, summary)
                    self.last_summary_stats = (
                        stats  # Сохраняем для последующего сохранения
                    )

                    stats_text = "\n\n--- Статистика ---\n"
                    stats_text += (
                        f"Исходных предложений: {stats['original_sentences_count']}\n"
                    )

                    # Показываем информацию о фильтрации
                    if (
                        self.filter_short_checkbox.isChecked()
                        and "valid_original_sentences_count" in stats
                    ):
                        filtered_count = (
                            stats["original_sentences_count"]
                            - stats["valid_original_sentences_count"]
                        )
                        stats_text += f"Отфильтровано коротких: {filtered_count}\n"
                        stats_text += f"Валидных предложений: {stats['valid_original_sentences_count']}\n"

                    stats_text += (
                        f"Предложений в выжимке: {stats['summary_sentences_count']}\n"
                    )
                    stats_text += f"Сжатие: {stats['compression_ratio']:.1%}\n"
                    stats_text += (
                        f"Исходных символов: {stats['original_chars_count']:,}\n"
                    )
                    stats_text += (
                        f"Символов в выжимке: {stats['summary_chars_count']:,}\n"
                    )

                    if "avg_sentence_length" in stats:
                        stats_text += f"Средняя длина предложения: {stats['avg_sentence_length']:.1f} слов\n"

                    self.summary_text.append(stats_text)

                    # Включаем кнопку сохранения
                    self.save_summary_button.setEnabled(True)
                    self.status_bar.showMessage("Выжимка создана успешно")
                else:
                    QMessageBox.warning(
                        self,
                        "Ошибка",
                        "Не удалось создать выжимку.\n"
                        "Возможно, все предложения слишком короткие.\n"
                        "Попробуйте уменьшить минимальную длину предложения.",
                    )
                    self.status_bar.showMessage("Ошибка создания выжимки")

            finally:
                self.summarize_button.setEnabled(True)

        except Exception as e:
            logger.error(f"Ошибка при создании выжимки: {e}")
            self.summarize_button.setEnabled(True)

            # Специальная обработка ошибки SpaCy
            if "exceeds maximum" in str(e):
                QMessageBox.critical(
                    self,
                    "Ошибка",
                    "Файл слишком большой для обработки.\n"
                    "Попробуйте файл меньшего размера или разделите его на части.",
                )
            else:
                QMessageBox.critical(
                    self, "Ошибка", f"Ошибка при создании выжимки: {str(e)}"
                )

            self.status_bar.showMessage("Ошибка")

    def update_statistics(self):
        """Обновление статистики"""
        stats_text = "📊 СТАТИСТИКА СИСТЕМЫ\n" + "=" * 50 + "\n\n"

        # Статистика моделей
        stats_text += "🧠 МОДЕЛИ:\n"
        model_files = list(MODELS_DIR.glob("*.model"))
        stats_text += f"Всего моделей: {len(model_files)}\n\n"

        # Получаем имя текущей модели
        current_model_name = (
            self.model_combo.currentText() if hasattr(self, "model_combo") else None
        )
        is_current_model_shown = False

        for model_file in model_files:
            model_name = model_file.stem
            file_size_mb = model_file.stat().st_size / 1024 / 1024

            # Проверяем, является ли это текущей моделью
            is_current = (
                current_model_name == model_name and self.current_model is not None
            )

            if is_current:
                is_current_model_shown = True
                stats_text += f"📍 ТЕКУЩАЯ МОДЕЛЬ: {model_name}\n"
            else:
                stats_text += f"📁 {model_name}:\n"

            stats_text += f"   Размер файла: {file_size_mb:.1f} МБ\n"

            # Пытаемся загрузить метаданные модели
            metadata_file = MODELS_DIR / f"{model_name}_metadata.json"
            if metadata_file.exists():
                try:
                    with open(metadata_file, "r", encoding="utf-8") as f:
                        metadata = json.load(f)

                    # Показываем только базовую информацию для не-текущих моделей
                    if not is_current:
                        training_time = metadata.get(
                            "training_time_formatted", "Неизвестно"
                        )
                        training_date = metadata.get("training_date", "Неизвестно")
                        corpus_size = metadata.get("corpus_size", 0)
                        stats_text += f"   Дата обучения: {training_date}\n"
                        stats_text += f"   Время обучения: {training_time}\n"
                        stats_text += f"   Размер корпуса при обучении: {corpus_size}\n"

                    else:
                        # Для текущей модели показываем подробную информацию
                        trainer = Doc2VecTrainer()
                        trainer.model = self.current_model
                        trainer.training_metadata = metadata

                        model_info = trainer.get_model_info()

                        stats_text += (
                            f"   Размерность векторов: {model_info['vector_size']}\n"
                        )
                        stats_text += f"   Размер словаря: {model_info['vocabulary_size']:,} слов\n"
                        stats_text += (
                            f"   Документов в модели: {model_info['documents_count']}\n"
                        )
                        stats_text += f"   Размер окна: {model_info['window']}\n"
                        stats_text += (
                            f"   Минимальная частота: {model_info['min_count']}\n"
                        )
                        stats_text += f"   Эпох обучения: {model_info['epochs']}\n"

                        stats_text += f"   Время обучения: {model_info['training_time_formatted']}\n"
                        stats_text += (
                            f"   Дата обучения: {model_info['training_date']}\n"
                        )

                        # Режим обучения
                        if model_info["dm"] == 1:
                            stats_text += "   Режим: Distributed Memory (DM)\n"
                        else:
                            stats_text += "   Режим: Distributed Bag of Words (DBOW)\n"

                except Exception as e:
                    logger.debug(
                        f"Не удалось загрузить метаданные для {model_name}: {e}"
                    )

            stats_text += "\n"

        # Если текущая модель не была показана в списке (не загружена правильно)
        if self.current_model and not is_current_model_shown:
            stats_text += "⚠️ Текущая модель загружена, но метаданные недоступны\n\n"

        # Системная информация
        try:
            import psutil

            stats_text += "💻 СИСТЕМА:\n"
            stats_text += f"CPU: {psutil.cpu_percent()}% загрузка\n"
            stats_text += f"Память: {psutil.virtual_memory().percent}% использовано\n"
            stats_text += f"Свободно памяти: {psutil.virtual_memory().available / 1024 / 1024 / 1024:.1f} ГБ\n"
        except ImportError:
            stats_text += "\n💻 СИСТЕМА:\n"
            stats_text += "Установите psutil для отображения системной информации\n"

        self.statistics_text.setPlainText(stats_text)

    def load_model_dialog(self):
        """Диалог загрузки модели"""
        file_path, _ = QFileDialog.getOpenFileName(
            self,
            "Выберите файл модели",
            str(MODELS_DIR),
            "Модели (*.model);;Все файлы (*.*)",
        )

        if file_path:
            model_name = Path(file_path).stem

            # Проверяем, есть ли модель в списке
            index = self.model_combo.findText(model_name)
            if index >= 0:
                self.model_combo.setCurrentIndex(index)
            else:
                # Копируем модель в нашу директорию
                import shutil

                try:
                    shutil.copy2(file_path, MODELS_DIR / Path(file_path).name)
                    self.load_models()
                    self.model_combo.setCurrentText(model_name)
                except Exception as e:
                    QMessageBox.critical(
                        self, "Ошибка", f"Не удалось загрузить модель: {str(e)}"
                    )

    def show_about(self):
        """Показать информацию о программе"""
        about_text = """
        <h2>Semantic Document Search</h2>
        <p>Версия 1.0.0</p>
        <p>Приложение для семантического поиска по документам с использованием технологии Doc2Vec.</p>
        <p><b>Возможности:</b></p>
        <ul>
        <li>Семантический поиск по содержимому документов</li>
        <li>Поддержка форматов PDF, DOCX, DOC</li>
        <li>Создание выжимок из документов</li>
        <li>Обучение собственных моделей</li>
        </ul>
        <p><b>Автор:</b> Evgeny Odintsov</p>
        <p><b>Email:</b> ev1genial@gmail.com</p>
        """

        QMessageBox.about(self, "О программе", about_text)

    def closeEvent(self, event):
        """Обработка закрытия окна"""
        # Останавливаем потоки если они запущены
        if self.training_thread and self.training_thread.isRunning():
            self.training_thread.cancel()
            self.training_thread.wait()

        if self.search_thread and self.search_thread.isRunning():
            self.search_thread.wait()

        event.accept()


========================================
FILE: src\semantic_search\utils\__init__.py
========================================
"""Вспомогательные утилиты"""

from .cache_manager import CacheManager
from .file_utils import FileExtractor
from .logging_config import logging_manager, setup_logging
from .notification_system import (
    NotificationManager,
    ProgressTracker,
    notification_manager,
)
from .performance_monitor import PerformanceMonitor
from .task_manager import TaskManager, task_manager
from .text_utils import TextProcessor
from .validators import DataValidator, FileValidator, ValidationError

__all__ = [
    "CacheManager",
    "FileExtractor",
    "setup_logging",
    "logging_manager",
    "NotificationManager",
    "notification_manager",
    "ProgressTracker",
    "PerformanceMonitor",
    "TaskManager",
    "task_manager",
    "TextProcessor",
    "DataValidator",
    "FileValidator",
    "ValidationError",
]


========================================
FILE: src\semantic_search\utils\cache_manager.py
========================================
"""Менеджер кэширования"""

import hashlib
import pickle
from pathlib import Path
from typing import Any, Optional

from loguru import logger


class CacheManager:
    """Менеджер кэширования для результатов поиска и обработки"""

    def __init__(self, cache_dir: Path):
        self.cache_dir = cache_dir
        self.cache_dir.mkdir(exist_ok=True, parents=True)

    def _get_cache_key(self, data: str) -> str:
        """Генерация ключа кэша"""
        return hashlib.md5(data.encode()).hexdigest()

    def get(self, key: str) -> Optional[Any]:
        """Получение данных из кэша"""
        cache_file = self.cache_dir / f"{self._get_cache_key(key)}.pkl"

        if cache_file.exists():
            try:
                with open(cache_file, "rb") as f:
                    return pickle.load(f)
            except Exception as e:
                logger.warning(f"Ошибка чтения кэша: {e}")

        return None

    def set(self, key: str, value: Any) -> bool:
        """Сохранение данных в кэш"""
        cache_file = self.cache_dir / f"{self._get_cache_key(key)}.pkl"

        try:
            with open(cache_file, "wb") as f:
                pickle.dump(value, f)
            return True
        except Exception as e:
            logger.error(f"Ошибка записи в кэш: {e}")
            return False

    def clear(self) -> bool:
        """Очистка кэша"""
        try:
            for cache_file in self.cache_dir.glob("*.pkl"):
                cache_file.unlink()
            logger.info("Кэш очищен")
            return True
        except Exception as e:
            logger.error(f"Ошибка очистки кэша: {e}")
            return False


========================================
FILE: src\semantic_search\utils\file_utils.py
========================================
"""Утилиты для работы с файлами"""

from collections import Counter
from pathlib import Path
from typing import List, Optional

import pymupdf
from docx import Document as DocxDocument
from loguru import logger

from semantic_search.config import SUPPORTED_EXTENSIONS

try:
    import win32com.client

    DOC_SUPPORT = True
except ImportError:
    DOC_SUPPORT = False
    logger.warning("pywin32 не установлен. Поддержка .doc файлов недоступна")


class FileExtractor:
    """Класс для извлечения текста из различных форматов файлов"""

    def __init__(self):
        self.word_app: Optional[object] = None
        if DOC_SUPPORT:
            try:
                self.word_app = win32com.client.Dispatch("Word.Application")
                self.word_app.Visible = False
            except Exception as e:
                logger.warning(f"Не удалось инициализировать Word Application: {e}")

    def __del__(self):
        """Освобождение ресурсов"""
        if self.word_app is not None:
            try:
                # Проверяем, что объект поддерживает метод Quit
                if hasattr(self.word_app, "Quit"):
                    self.word_app.Quit()
            except Exception:
                pass

    def find_documents(self, root_path: Path) -> List[Path]:
        """
        Рекурсивный поиск документов в директории

        Args:
            root_path: Путь к корневой директории

        Returns:
            Список путей к найденным файлам
        """
        if not root_path.exists():
            raise FileNotFoundError(f"Директория не найдена: {root_path}")

        found_files = []
        logger.info(f"Поиск документов в: {root_path}")

        for file_path in root_path.rglob("*"):
            if file_path.is_file() and file_path.suffix.lower() in SUPPORTED_EXTENSIONS:
                found_files.append(file_path)

        # Логирование статистики
        logger.info(f"Найдено файлов: {len(found_files)}")
        ext_counter = Counter(f.suffix.lower() for f in found_files)
        for ext in SUPPORTED_EXTENSIONS:
            count = ext_counter.get(ext, 0)
            if count > 0:
                logger.info(f"  {ext}: {count}")

        return found_files

    def extract_from_pdf(self, file_path: Path) -> str:
        """Улучшенная извлечение текста из PDF с обработкой больших файлов"""
        try:
            doc = pymupdf.open(file_path)
            text_parts = []

            # Проверяем количество страниц
            total_pages = len(doc)
            if total_pages > 1000:
                logger.warning(
                    f"PDF содержит {total_pages} страниц. Обработка может занять время"
                )

            # Обрабатываем с прогресс-баром для больших файлов
            page_iterator = range(len(doc))
            if total_pages > 50:
                try:
                    from tqdm import tqdm

                    page_iterator = tqdm(
                        page_iterator, desc=f"Извлечение из {file_path.name}"
                    )
                except ImportError:
                    pass

            for page_num in page_iterator:
                try:
                    page = doc[page_num]
                    page_text = page.get_text()

                    # Фильтруем явно мусорные страницы
                    if len(page_text.strip()) > 50:  # Минимум 50 символов
                        text_parts.append(page_text)

                except Exception as e:
                    logger.warning(f"Ошибка при обработке страницы {page_num}: {e}")
                    continue

            doc.close()

            full_text = "\n".join(text_parts)
            logger.info(f"Извлечено {len(full_text)} символов из {total_pages} страниц")

            return full_text.strip()

        except Exception as e:
            logger.error(f"Ошибка при извлечении текста из PDF {file_path}: {e}")
            return ""

    def extract_from_docx(self, file_path: Path) -> str:
        """Извлечение текста из DOCX файла"""
        try:
            doc = DocxDocument(file_path)
            text_parts = []

            # Извлекаем текст из параграфов
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    text_parts.append(paragraph.text.strip())

            # Извлекаем текст из таблиц
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        if cell.text.strip():
                            text_parts.append(cell.text.strip())

            return "\n".join(text_parts)

        except Exception as e:
            logger.error(f"Ошибка при извлечении текста из DOCX {file_path}: {e}")
            return ""

    def extract_from_doc(self, file_path: Path) -> str:
        """Извлечение текста из DOC файла (только Windows)"""
        if not DOC_SUPPORT or self.word_app is None:
            logger.warning(f"Поддержка .doc файлов недоступна: {file_path}")
            return ""

        try:
            doc = self.word_app.Documents.Open(str(file_path.absolute()))
            text = doc.Content.Text
            doc.Close()
            return text.strip()

        except Exception as e:
            logger.error(f"Ошибка при извлечении текста из DOC {file_path}: {e}")
            return ""

    def extract_text(self, file_path: Path) -> str:
        """
        Универсальная функция извлечения текста

        Args:
            file_path: Путь к файлу

        Returns:
            Извлеченный текст
        """
        extension = file_path.suffix.lower()

        if extension == ".pdf":
            return self.extract_from_pdf(file_path)
        elif extension == ".docx":
            return self.extract_from_docx(file_path)
        elif extension == ".doc":
            return self.extract_from_doc(file_path)
        else:
            logger.warning(f"Неподдерживаемый формат файла: {file_path}")
            return ""


========================================
FILE: src\semantic_search\utils\logging_config.py
========================================
"""Улучшенная конфигурация логирования"""

import sys
from typing import Optional

from loguru import logger

from semantic_search.config import LOGS_DIR


class LoggingManager:
    """Менеджер системы логирования"""

    def __init__(self):
        self.handlers = {}
        self.is_configured = False

    def setup_logging(
        self,
        level: str = "INFO",
        enable_file_logging: bool = True,
        enable_rotation: bool = True,
        custom_format: Optional[str] = None,
    ) -> None:
        """
        Расширенная настройка системы логирования

        Args:
            level: Уровень логирования
            enable_file_logging: Включить запись в файл
            enable_rotation: Включить ротацию логов
            custom_format: Пользовательский формат логов
        """
        if self.is_configured:
            return

        # Удаляем стандартный handler
        logger.remove()

        # Формат для консоли
        console_format = custom_format or (
            "<green>{time:YYYY-MM-DD HH:mm:ss}</green> | "
            "<level>{level: <8}</level> | "
            "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> | "
            "<level>{message}</level>"
        )

        # Консольный вывод
        console_handler = logger.add(
            sys.stderr,
            level=level,
            colorize=True,
            format=console_format,
            diagnose=True,
            backtrace=True,
        )
        self.handlers["console"] = console_handler

        if enable_file_logging:
            # Основной лог файл
            main_log = LOGS_DIR / "semantic_search.log"
            file_format = "{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} | {message}"

            if enable_rotation:
                main_handler = logger.add(
                    main_log,
                    level=level,
                    format=file_format,
                    rotation="10 MB",
                    retention="2 weeks",
                    compression="zip",
                    serialize=False,
                )
            else:
                main_handler = logger.add(main_log, level=level, format=file_format)

            self.handlers["main"] = main_handler

            # Отдельный файл для ошибок
            error_log = LOGS_DIR / "errors.log"
            error_handler = logger.add(
                error_log,
                level="ERROR",
                format=file_format,
                rotation="5 MB",
                retention="1 month",
                compression="zip",
            )
            self.handlers["error"] = error_handler

            # Лог производительности
            perf_log = LOGS_DIR / "performance.log"
            perf_handler = logger.add(
                perf_log,
                level="INFO",
                format="{time:YYYY-MM-DD HH:mm:ss} | {message}",
                filter=lambda record: "PERF" in record["message"],
                rotation="50 MB",
                retention="1 week",
            )
            self.handlers["performance"] = perf_handler

        self.is_configured = True
        logger.info("Система логирования настроена")

        # Логируем системную информацию
        self._log_system_info()

    def _log_system_info(self):
        """Логирование информации о системе"""
        try:
            import platform

            import psutil

            logger.info(f"Система: {platform.system()} {platform.release()}")
            logger.info(f"Python: {platform.python_version()}")
            logger.info(f"Процессор: {platform.processor()}")
            logger.info(f"ОЗУ: {psutil.virtual_memory().total / 1024**3:.1f} ГБ")
            logger.info(
                f"Свободное место: {psutil.disk_usage('/').free / 1024**3:.1f} ГБ"
            )

        except Exception as e:
            logger.debug(f"Не удалось получить системную информацию: {e}")

    def add_performance_log(self, operation: str, duration: float, **kwargs):
        """Добавление записи о производительности"""
        perf_data = {"operation": operation, "duration": duration, **kwargs}

        logger.info(f"PERF: {perf_data}")

    def cleanup(self):
        """Очистка ресурсов логирования"""
        for handler_id in self.handlers.values():
            try:
                logger.remove(handler_id)
            except ValueError:
                pass

        self.handlers.clear()
        self.is_configured = False


# Глобальный экземпляр
logging_manager = LoggingManager()


def setup_logging(level: str = "INFO") -> None:
    """Обратная совместимость"""
    logging_manager.setup_logging(level)


========================================
FILE: src\semantic_search\utils\notification_system.py
========================================
"""Система уведомлений и прогресса"""

import time
from dataclasses import dataclass
from enum import Enum
from queue import Empty, Queue
from threading import Event, Thread
from typing import Any, Callable, Dict, Optional

from loguru import logger


class NotificationType(Enum):
    """Типы уведомлений"""

    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    SUCCESS = "success"
    PROGRESS = "progress"


@dataclass
class Notification:
    """Структура уведомления"""

    type: NotificationType
    title: str
    message: str
    details: Optional[str] = None
    progress: Optional[float] = None  # 0.0 - 1.0
    timestamp: float = None

    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = time.time()


class ProgressTracker:
    """Трекер прогресса операций"""

    def __init__(self, total_steps: int, description: str = ""):
        self.total_steps = total_steps
        self.current_step = 0
        self.description = description
        self.start_time = time.time()
        self.callbacks = []

    def add_callback(self, callback: Callable[[Dict[str, Any]], None]):
        """Добавление callback для обновлений прогресса"""
        self.callbacks.append(callback)

    def update(self, step: Optional[int] = None, message: str = ""):
        """Обновление прогресса"""
        if step is not None:
            self.current_step = step
        else:
            self.current_step += 1

        progress = min(self.current_step / self.total_steps, 1.0)
        elapsed = time.time() - self.start_time

        if progress > 0:
            eta = elapsed / progress * (1 - progress)
        else:
            eta = 0

        progress_info = {
            "progress": progress,
            "current_step": self.current_step,
            "total_steps": self.total_steps,
            "elapsed": elapsed,
            "eta": eta,
            "message": message,
            "description": self.description,
        }

        # Вызываем callbacks
        for callback in self.callbacks:
            try:
                callback(progress_info)
            except Exception as e:
                logger.error(f"Ошибка в progress callback: {e}")

    def finish(self, message: str = "Завершено"):
        """Завершение операции"""
        self.current_step = self.total_steps
        self.update(message=message)


class NotificationManager:
    """Менеджер системы уведомлений"""

    def __init__(self):
        self.subscribers = []
        self.notification_queue = Queue()
        self.is_running = False
        self.worker_thread = None
        self.stop_event = Event()

    def start(self):
        """Запуск системы уведомлений"""
        if self.is_running:
            return

        self.is_running = True
        self.stop_event.clear()
        self.worker_thread = Thread(target=self._worker, daemon=True)
        self.worker_thread.start()

        logger.info("Система уведомлений запущена")

    def stop(self):
        """Остановка системы уведомлений"""
        if not self.is_running:
            return

        self.is_running = False
        self.stop_event.set()

        if self.worker_thread:
            self.worker_thread.join(timeout=1.0)

        logger.info("Система уведомлений остановлена")

    def subscribe(self, callback: Callable[[Notification], None]):
        """Подписка на уведомления"""
        self.subscribers.append(callback)

    def unsubscribe(self, callback: Callable[[Notification], None]):
        """Отписка от уведомлений"""
        if callback in self.subscribers:
            self.subscribers.remove(callback)

    def notify(self, notification: Notification):
        """Отправка уведомления"""
        if self.is_running:
            self.notification_queue.put(notification)
        else:
            # Если система не запущена, отправляем сразу
            self._send_notification(notification)

    def _worker(self):
        """Рабочий поток для обработки уведомлений"""
        while not self.stop_event.is_set():
            try:
                notification = self.notification_queue.get(timeout=0.1)
                self._send_notification(notification)
                self.notification_queue.task_done()
            except Empty:
                continue
            except Exception as e:
                logger.error(f"Ошибка в worker уведомлений: {e}")

    def _send_notification(self, notification: Notification):
        """Отправка уведомления подписчикам"""
        for callback in self.subscribers:
            try:
                callback(notification)
            except Exception as e:
                logger.error(f"Ошибка в callback уведомления: {e}")

    # Удобные методы для создания уведомлений
    def info(self, title: str, message: str, details: Optional[str] = None):
        """Информационное уведомление"""
        self.notify(Notification(NotificationType.INFO, title, message, details))

    def warning(self, title: str, message: str, details: Optional[str] = None):
        """Предупреждение"""
        self.notify(Notification(NotificationType.WARNING, title, message, details))

    def error(self, title: str, message: str, details: Optional[str] = None):
        """Уведомление об ошибке"""
        self.notify(Notification(NotificationType.ERROR, title, message, details))

    def success(self, title: str, message: str, details: Optional[str] = None):
        """Уведомление об успехе"""
        self.notify(Notification(NotificationType.SUCCESS, title, message, details))


# Глобальный экземпляр
notification_manager = NotificationManager()


========================================
FILE: src\semantic_search\utils\performance_monitor.py
========================================
"""Мониторинг производительности"""

import time
from contextlib import contextmanager
from typing import Any, Dict

import psutil
from loguru import logger


class PerformanceMonitor:
    """Мониторинг производительности операций"""

    def __init__(self):
        self.metrics = {}

    @contextmanager
    def measure_operation(self, operation_name: str):
        """Контекстный менеджер для измерения времени операции"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB

        try:
            yield
        finally:
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB

            duration = end_time - start_time
            memory_delta = end_memory - start_memory

            self.metrics[operation_name] = {
                "duration": duration,
                "memory_start": start_memory,
                "memory_end": end_memory,
                "memory_delta": memory_delta,
                "timestamp": time.time(),
            }

            logger.info(
                f"{operation_name}: {duration:.2f}s, Память: {memory_delta:+.1f}MB"
            )

    def get_system_info(self) -> Dict[str, Any]:
        """Получение информации о системе"""
        return {
            "cpu_count": psutil.cpu_count(),
            "cpu_percent": psutil.cpu_percent(interval=1),
            "memory_total": psutil.virtual_memory().total / 1024**3,  # GB
            "memory_available": psutil.virtual_memory().available / 1024**3,  # GB
            "memory_percent": psutil.virtual_memory().percent,
            "disk_usage": psutil.disk_usage("/").percent
            if psutil.disk_usage("/")
            else 0,
        }


========================================
FILE: src\semantic_search\utils\statistics.py
========================================
"""Утилиты для вычисления статистики"""

from collections import Counter
from typing import Any, Dict, List

from semantic_search.core.document_processor import ProcessedDocument


def calculate_statistics_from_processed_docs(
    docs_data: List[ProcessedDocument],
) -> Dict[str, Any]:
    """
    Вычисление статистики из уже обработанных документов

    Args:
        docs_data: Список обработанных документов

    Returns:
        Словарь со статистикой:
        - processed_files: количество обработанных файлов
        - total_tokens: общее количество токенов
        - avg_tokens_per_doc: среднее количество токенов на документ
        - extensions_count: количество файлов по расширениям
        - largest_doc: информация о самом большом документе
        - smallest_doc: информация о самом маленьком документе
        - total_chars: общее количество символов
        - avg_chars_per_doc: среднее количество символов на документ
    """
    if not docs_data:
        return {
            "processed_files": 0,
            "total_tokens": 0,
            "avg_tokens_per_doc": 0.0,
            "extensions_count": {},
            "largest_doc": None,
            "smallest_doc": None,
            "total_chars": 0,
            "avg_chars_per_doc": 0.0,
        }

    # Основная статистика
    stats = {
        "processed_files": len(docs_data),
        "total_tokens": sum(doc.metadata["tokens_count"] for doc in docs_data),
        "total_chars": sum(doc.metadata["text_length"] for doc in docs_data),
        "extensions_count": dict(
            Counter(doc.metadata["extension"] for doc in docs_data)
        ),
    }

    # Средние значения
    stats["avg_tokens_per_doc"] = stats["total_tokens"] / stats["processed_files"]
    stats["avg_chars_per_doc"] = stats["total_chars"] / stats["processed_files"]

    # Самый большой и маленький документы
    docs_by_tokens = sorted(docs_data, key=lambda x: x.metadata["tokens_count"])
    stats["smallest_doc"] = {
        "path": docs_by_tokens[0].relative_path,
        "tokens": docs_by_tokens[0].metadata["tokens_count"],
        "chars": docs_by_tokens[0].metadata["text_length"],
    }
    stats["largest_doc"] = {
        "path": docs_by_tokens[-1].relative_path,
        "tokens": docs_by_tokens[-1].metadata["tokens_count"],
        "chars": docs_by_tokens[-1].metadata["text_length"],
    }

    return stats


def format_statistics_for_display(stats: Dict[str, Any]) -> str:
    """
    Форматирование статистики для красивого вывода

    Args:
        stats: Словарь со статистикой из calculate_statistics_from_processed_docs

    Returns:
        Отформатированная строка со статистикой
    """
    if stats["processed_files"] == 0:
        return "❌ Нет обработанных документов"

    lines = [
        "📊 Статистика корпуса:",
        f"📁 Обработано документов: {stats['processed_files']}",
        f"🔤 Общее количество токенов: {stats['total_tokens']:,}",
        f"📄 Среднее токенов на документ: {stats['avg_tokens_per_doc']:.1f}",
        f"📝 Общее количество символов: {stats['total_chars']:,}",
        f"📖 Среднее символов на документ: {stats['avg_chars_per_doc']:.1f}",
        f"📋 Форматы файлов: {stats['extensions_count']}",
    ]

    if stats["largest_doc"]:
        lines.extend(
            [
                "📈 Самый большой документ:",
                f"   📄 {stats['largest_doc']['path']}",
                f"   🔤 {stats['largest_doc']['tokens']} токенов, {stats['largest_doc']['chars']} символов",
            ]
        )

    if stats["smallest_doc"]:
        lines.extend(
            [
                "📉 Самый маленький документ:",
                f"   📄 {stats['smallest_doc']['path']}",
                f"   🔤 {stats['smallest_doc']['tokens']} токенов, {stats['smallest_doc']['chars']} символов",
            ]
        )

    return "\n".join(lines)


def calculate_model_statistics(model_info: Dict[str, Any]) -> str:
    """
    Форматирование статистики модели для вывода

    Args:
        model_info: Информация о модели из Doc2VecTrainer.get_model_info()

    Returns:
        Отформатированная строка со статистикой модели
    """
    if model_info.get("status") != "loaded":
        return f"❌ Модель недоступна: {model_info.get('status', 'неизвестно')}"

    lines = [
        "🧠 Статистика модели:",
        f"✅ Статус: {model_info['status']}",
        f"📏 Размерность векторов: {model_info['vector_size']}",
        f"📚 Размер словаря: {model_info['vocabulary_size']:,} слов",
        f"📄 Документов в модели: {model_info['documents_count']}",
        f"🔍 Размер окна контекста: {model_info['window']}",
        f"📊 Минимальная частота слова: {model_info['min_count']}",
        f"🔄 Количество эпох обучения: {model_info['epochs']}",
    ]

    # Добавляем информацию о времени обучения
    if "training_time_formatted" in model_info:
        lines.append(f"⏱️ Время обучения: {model_info['training_time_formatted']}")

    if "training_date" in model_info:
        lines.append(f"📅 Дата обучения: {model_info['training_date']}")

    if "corpus_size" in model_info and model_info["corpus_size"] > 0:
        lines.append(
            f"📑 Размер корпуса при обучении: {model_info['corpus_size']} документов"
        )

    # Режим обучения
    if model_info.get("dm") == 1:
        lines.append("🔧 Режим: Distributed Memory (DM)")
    else:
        lines.append("🔧 Режим: Distributed Bag of Words (DBOW)")

    # Дополнительные параметры
    if model_info.get("negative", 0) > 0:
        lines.append(f"➖ Negative sampling: {model_info['negative']}")
    if model_info.get("hs") == 1:
        lines.append("🌳 Hierarchical Softmax: включен")

    return "\n".join(lines)


========================================
FILE: src\semantic_search\utils\task_manager.py
========================================
"""Менеджер долгосрочных задач"""

import time
import uuid
from concurrent.futures import Future, ThreadPoolExecutor
from dataclasses import dataclass, field
from enum import Enum
from threading import Lock
from typing import Any, Callable, Dict, List, Optional

from loguru import logger

from .notification_system import ProgressTracker, notification_manager


class TaskStatus(Enum):
    """Статусы задач"""

    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


@dataclass
class Task:
    """Структура задачи"""

    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    name: str = ""
    description: str = ""
    status: TaskStatus = TaskStatus.PENDING
    progress: float = 0.0
    result: Any = None
    error: Optional[str] = None
    start_time: Optional[float] = None
    end_time: Optional[float] = None
    future: Optional[Future] = None
    progress_tracker: Optional[ProgressTracker] = None

    @property
    def duration(self) -> Optional[float]:
        """Длительность выполнения задачи"""
        if self.start_time and self.end_time:
            return self.end_time - self.start_time
        elif self.start_time:
            return time.time() - self.start_time
        return None


class TaskManager:
    """Менеджер задач для выполнения длительных операций"""

    def __init__(self, max_workers: int = 4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.tasks: Dict[str, Task] = {}
        self.lock = Lock()

        logger.info(f"TaskManager инициализирован с {max_workers} потоками")

    def submit_task(
        self,
        func: Callable,
        args: tuple = (),
        kwargs: dict = None,
        name: str = "",
        description: str = "",
        track_progress: bool = False,
        total_steps: int = 0,
    ) -> str:
        """
        Создание и запуск задачи

        Args:
            func: Функция для выполнения
            args: Аргументы функции
            kwargs: Именованные аргументы
            name: Название задачи
            description: Описание задачи
            track_progress: Включить отслеживание прогресса
            total_steps: Общее количество шагов (для прогресса)

        Returns:
            ID задачи
        """
        kwargs = kwargs or {}

        task = Task(name=name or func.__name__, description=description)

        if track_progress and total_steps > 0:
            task.progress_tracker = ProgressTracker(total_steps, name)
            # Добавляем tracker в kwargs если функция его ожидает
            if "progress_tracker" in func.__code__.co_varnames:
                kwargs["progress_tracker"] = task.progress_tracker

        # Оборачиваем функцию для отслеживания статуса
        def wrapped_func():
            task.status = TaskStatus.RUNNING
            task.start_time = time.time()

            try:
                notification_manager.info(
                    "Задача запущена", f"Начато выполнение: {task.name}"
                )

                result = func(*args, **kwargs)

                task.status = TaskStatus.COMPLETED
                task.result = result
                task.progress = 1.0

                notification_manager.success(
                    "Задача завершена",
                    f"Успешно завершена: {task.name}",
                    f"Время выполнения: {task.duration:.1f}с",
                )

                return result

            except Exception as e:
                task.status = TaskStatus.FAILED
                task.error = str(e)

                notification_manager.error(
                    "Ошибка задачи", f"Ошибка в задаче: {task.name}", str(e)
                )

                logger.error(f"Ошибка в задаче {task.id}: {e}")
                raise

            finally:
                task.end_time = time.time()

        # Запускаем задачу
        future = self.executor.submit(wrapped_func)
        task.future = future

        with self.lock:
            self.tasks[task.id] = task

        logger.info(f"Задача {task.id} ({name}) поставлена в очередь")
        return task.id

    def get_task(self, task_id: str) -> Optional[Task]:
        """Получение информации о задаче"""
        with self.lock:
            return self.tasks.get(task_id)

    def get_all_tasks(self) -> List[Task]:
        """Получение всех задач"""
        with self.lock:
            return list(self.tasks.values())

    def get_running_tasks(self) -> List[Task]:
        """Получение выполняющихся задач"""
        return [
            task for task in self.get_all_tasks() if task.status == TaskStatus.RUNNING
        ]

    def cancel_task(self, task_id: str) -> bool:
        """Отмена задачи"""
        task = self.get_task(task_id)
        if not task or not task.future:
            return False

        if task.future.cancel():
            task.status = TaskStatus.CANCELLED
            logger.info(f"Задача {task_id} отменена")
            return True

        return False

    def wait_for_task(self, task_id: str, timeout: Optional[float] = None) -> Any:
        """Ожидание завершения задачи"""
        task = self.get_task(task_id)
        if not task or not task.future:
            raise ValueError(f"Задача {task_id} не найдена")

        return task.future.result(timeout=timeout)

    def cleanup_finished_tasks(self, max_keep: int = 100):
        """Очистка завершенных задач"""
        with self.lock:
            finished_tasks = [
                (task_id, task)
                for task_id, task in self.tasks.items()
                if task.status
                in [TaskStatus.COMPLETED, TaskStatus.FAILED, TaskStatus.CANCELLED]
            ]

            if len(finished_tasks) > max_keep:
                # Сортируем по времени завершения и удаляем старые
                finished_tasks.sort(key=lambda x: x[1].end_time or 0)
                to_remove = finished_tasks[:-max_keep]

                for task_id, _ in to_remove:
                    del self.tasks[task_id]

                logger.info(f"Удалено {len(to_remove)} завершенных задач")

    def shutdown(self, wait: bool = True):
        """Завершение работы менеджера задач"""
        logger.info("Завершение работы TaskManager...")
        self.executor.shutdown(wait=wait)


# Глобальный экземпляр
task_manager = TaskManager()


========================================
FILE: src\semantic_search\utils\text_utils.py
========================================
# В файле src/semantic_search/utils/text_utils.py
# Обновленный TextProcessor с поддержкой двух языков

import re
from typing import List, Optional, Tuple

from loguru import logger

from semantic_search.config import SPACY_MODELS, TEXT_PROCESSING_CONFIG

# Глобальные переменные для ленивой загрузки
_nlp_ru = None
_nlp_en = None
_spacy_available = None
_initialization_attempted = False


def check_spacy_model_availability() -> dict:
    """
    Проверка доступности SpaCy моделей

    Returns:
        Словарь с информацией о состоянии моделей
    """
    info = {
        "spacy_installed": False,
        "ru_model_found": False,
        "en_model_found": False,
        "ru_model_loadable": False,
        "en_model_loadable": False,
        "models_info": {},
        "error": None,
    }

    try:
        import spacy

        info["spacy_installed"] = True

        # Проверка русской модели
        ru_model = SPACY_MODELS.get("ru", "ru_core_news_sm")
        try:
            nlp_ru = spacy.load(ru_model)
            info["ru_model_found"] = True
            info["ru_model_loadable"] = True
            info["models_info"]["ru"] = ru_model
        except OSError:
            info["error"] = f"Русская модель '{ru_model}' не найдена"

        # Проверка английской модели
        en_model = SPACY_MODELS.get("en", "en_core_web_sm")
        try:
            nlp_en = spacy.load(en_model)
            info["en_model_found"] = True
            info["en_model_loadable"] = True
            info["models_info"]["en"] = en_model
        except OSError:
            if not info["error"]:
                info["error"] = f"Английская модель '{en_model}' не найдена"
            else:
                info["error"] += f", английская модель '{en_model}' не найдена"

    except ImportError as e:
        info["error"] = f"SpaCy не установлен: {e}"

    return info


def _initialize_spacy() -> Tuple[Optional[object], Optional[object], bool]:
    """Ленивая инициализация SpaCy моделей для русского и английского"""
    global _nlp_ru, _nlp_en, _spacy_available, _initialization_attempted

    if _initialization_attempted:
        return _nlp_ru, _nlp_en, _spacy_available

    _initialization_attempted = True

    try:
        import spacy

        # Загружаем русскую модель
        ru_model = SPACY_MODELS.get("ru", "ru_core_news_sm")
        try:
            _nlp_ru = spacy.load(ru_model)
            _nlp_ru.max_length = TEXT_PROCESSING_CONFIG.get(
                "spacy_max_length", 3_000_000
            )
            logger.info(f"SpaCy русская модель '{ru_model}' загружена")
        except OSError:
            logger.warning(f"SpaCy русская модель '{ru_model}' не найдена")
            _nlp_ru = None

        # Загружаем английскую модель
        en_model = SPACY_MODELS.get("en", "en_core_web_sm")
        try:
            _nlp_en = spacy.load(en_model)
            _nlp_en.max_length = TEXT_PROCESSING_CONFIG.get(
                "spacy_max_length", 3_000_000
            )
            logger.info(f"SpaCy английская модель '{en_model}' загружена")
        except OSError:
            logger.warning(f"SpaCy английская модель '{en_model}' не найдена")
            _nlp_en = None

        _spacy_available = (_nlp_ru is not None) or (_nlp_en is not None)

    except ImportError:
        logger.warning("SpaCy не установлен")
        _nlp_ru = None
        _nlp_en = None
        _spacy_available = False

    return _nlp_ru, _nlp_en, _spacy_available


class TextProcessor:
    """Класс для препроцессинга текста с поддержкой русского и английского языков"""

    def __init__(self):
        self.config = TEXT_PROCESSING_CONFIG
        self._nlp_ru = None
        self._nlp_en = None
        self._spacy_available = None
        self.max_chunk_size = self.config.get("chunk_size", 800_000)

    def _get_nlp(self):
        """Получить SpaCy модели (с ленивой загрузкой)"""
        if self._nlp_ru is None and self._nlp_en is None:
            self._nlp_ru, self._nlp_en, self._spacy_available = _initialize_spacy()
        return self._nlp_ru, self._nlp_en, self._spacy_available

    def detect_language(self, text: str) -> str:
        """
        Определение преобладающего языка текста

        Returns:
            'ru' - русский, 'en' - английский, 'mixed' - смешанный
        """
        # Анализируем первые 1000 символов
        sample = text[:1000]

        # Подсчет алфавитных символов
        cyrillic = sum(1 for c in sample if "\u0400" <= c <= "\u04ff")
        latin = sum(1 for c in sample if ("a" <= c <= "z") or ("A" <= c <= "Z"))

        total = cyrillic + latin
        if total == 0:
            return "unknown"

        cyrillic_ratio = cyrillic / total

        if cyrillic_ratio > 0.8:
            return "ru"
        elif cyrillic_ratio < 0.2:
            return "en"
        else:
            return "mixed"

    def clean_text(self, text: str) -> str:
        """
        Базовая очистка текста

        Args:
            text: Исходный текст

        Returns:
            Очищенный текст
        """
        if not text:
            return ""

        # Сохраняем больше символов для многоязычных текстов
        text = re.sub(r'[^\w\s\-.,!?;:()\[\]""«»\']+', " ", text, flags=re.UNICODE)
        text = re.sub(r"\s+", " ", text)

        # Фильтруем слишком короткие слова
        words = text.split()
        words = [word for word in words if len(word) > 1 or word.lower() in ["i", "a"]]

        return " ".join(words).strip()

    def preprocess_with_spacy(self, text: str, language: str = "auto") -> List[str]:
        """
        Обработка текста с использованием SpaCy

        Args:
            text: Исходный текст
            language: 'ru', 'en' или 'auto' для автоопределения

        Returns:
            Список обработанных токенов
        """
        nlp_ru, nlp_en, spacy_available = self._get_nlp()

        if not spacy_available:
            return self.preprocess_basic(text)

        # Определяем язык если не указан
        if language == "auto":
            language = self.detect_language(text)
            logger.debug(f"Определен язык: {language}")

        # Выбираем подходящую модель
        if language == "ru" and nlp_ru:
            nlp = nlp_ru
        elif language == "en" and nlp_en:
            nlp = nlp_en
        elif language == "mixed":
            # Для смешанного текста обрабатываем по частям
            return self._process_mixed_text(text)
        else:
            # Fallback на доступную модель
            nlp = nlp_ru or nlp_en
            if not nlp:
                return self.preprocess_basic(text)

        # Обработка через SpaCy
        tokens = []

        # Для длинных текстов - по частям
        if len(text) > self.max_chunk_size:
            for i in range(0, len(text), self.max_chunk_size):
                chunk = text[i : i + self.max_chunk_size]
                chunk_tokens = self._process_spacy_chunk(chunk, nlp)
                tokens.extend(chunk_tokens)
        else:
            tokens = self._process_spacy_chunk(text, nlp)

        return tokens

    def _process_mixed_text(self, text: str) -> List[str]:
        """Обработка текста со смешанными языками"""
        nlp_ru, nlp_en, _ = self._get_nlp()

        # Разбиваем на предложения
        sentences = self.split_into_sentences(text)
        all_tokens = []

        for sentence in sentences:
            lang = self.detect_language(sentence)

            if lang == "ru" and nlp_ru:
                tokens = self._process_spacy_chunk(sentence, nlp_ru)
            elif lang == "en" and nlp_en:
                tokens = self._process_spacy_chunk(sentence, nlp_en)
            else:
                # Если нет подходящей модели, используем базовую обработку
                tokens = self.preprocess_basic(sentence)

            all_tokens.extend(tokens)

        return all_tokens

    def _process_spacy_chunk(self, text: str, nlp) -> List[str]:
        """Обработка одного чанка текста через SpaCy"""
        doc = nlp(text)
        tokens = []

        for token in doc:
            # Фильтруем токены
            if (
                not token.is_punct
                and not token.is_space
                and not token.is_stop
                and len(token.text) >= self.config["min_token_length"]
                and (token.is_alpha or token.like_num)  # Буквы или числа
            ):
                # Лемматизация если включена
                if self.config["lemmatize"]:
                    tokens.append(token.lemma_.lower())
                else:
                    tokens.append(token.text.lower())

        return tokens

    def preprocess_basic(self, text: str) -> List[str]:
        """
        Базовая обработка текста без SpaCy
        """
        text = text.lower()

        # Простая токенизация
        tokens = re.findall(r"\b\w+\b", text, re.UNICODE)

        # Фильтрация
        tokens = [
            token for token in tokens if len(token) >= self.config["min_token_length"]
        ]

        return tokens

    def preprocess_text(self, text: str) -> List[str]:
        """
        Главная функция препроцессинга текста
        """
        if not text:
            return []

        cleaned_text = self.clean_text(text)
        if not cleaned_text:
            return []

        # Определяем язык и обрабатываем
        tokens = self.preprocess_with_spacy(cleaned_text)

        return tokens

    def split_into_sentences(self, text: str) -> List[str]:
        """
        Разбиение текста на предложения с учетом многоязычности
        """
        if not text:
            return []

        nlp_ru, nlp_en, spacy_available = self._get_nlp()
        min_sentence_length = self.config.get("min_sentence_length", 10)

        if spacy_available:
            # Определяем язык
            lang = self.detect_language(text)

            # Выбираем модель
            if lang == "ru" and nlp_ru:
                nlp = nlp_ru
            elif lang == "en" and nlp_en:
                nlp = nlp_en
            else:
                # Для смешанного или неопределенного - базовый метод
                return self._split_sentences_basic(text, min_sentence_length)

            try:
                doc = nlp(text)
                sentences = [sent.text.strip() for sent in doc.sents]
                sentences = [
                    sent for sent in sentences if len(sent) >= min_sentence_length
                ]
                return sentences
            except Exception as e:
                logger.warning(f"Ошибка при разбиении через SpaCy: {e}")
                return self._split_sentences_basic(text, min_sentence_length)
        else:
            return self._split_sentences_basic(text, min_sentence_length)

    def _split_sentences_basic(self, text: str, min_sentence_length: int) -> List[str]:
        """
        Базовое разбиение текста на предложения без SpaCy

        Args:
            text: Исходный текст
            min_sentence_length: Минимальная длина предложения

        Returns:
            Список предложений
        """
        # Простое разбиение по знакам препинания
        # Поддержка русских и английских сокращений
        abbreviations = {
            "г.",
            "гг.",
            "т.д.",
            "т.п.",
            "др.",
            "пр.",
            "см.",
            "стр.",
            "Mr.",
            "Mrs.",
            "Dr.",
            "Prof.",
            "Inc.",
            "Ltd.",
            "Co.",
            "vs.",
            "etc.",
            "i.e.",
            "e.g.",
        }

        # Замена сокращений временными маркерами
        temp_text = text
        replacements = {}
        for i, abbr in enumerate(abbreviations):
            marker = f"__ABBR{i}__"
            replacements[marker] = abbr
            temp_text = temp_text.replace(abbr, marker)

        # Разбиение по основным знакам препинания
        import re

        sentences = re.split(r"[.!?]+", temp_text)

        # Восстановление сокращений
        result_sentences = []
        for sent in sentences:
            # Восстанавливаем сокращения
            for marker, abbr in replacements.items():
                sent = sent.replace(marker, abbr)

            sent = sent.strip()
            if len(sent) >= min_sentence_length:
                result_sentences.append(sent)

        return result_sentences

    def get_spacy_status(self) -> str:
        """Получить статус SpaCy моделей"""
        nlp_ru, nlp_en, _ = self._get_nlp()

        status_parts = []

        if nlp_ru:
            status_parts.append("✅ Русская модель")
        else:
            status_parts.append("❌ Русская модель не установлена")

        if nlp_en:
            status_parts.append("✅ Английская модель")
        else:
            status_parts.append("❌ Английская модель не установлена")

        return " | ".join(status_parts)


========================================
FILE: src\semantic_search\utils\validators.py
========================================
"""Валидаторы для проверки данных"""

from pathlib import Path
from typing import Any, Dict, Optional, Union


class ValidationError(Exception):
    """Ошибка валидации"""

    pass


class DataValidator:
    """Валидатор для различных типов данных"""

    @staticmethod
    def validate_file_path(path: Union[str, Path], must_exist: bool = True) -> Path:
        """
        Валидация пути к файлу

        Args:
            path: Путь к файлу
            must_exist: Файл должен существовать

        Returns:
            Валидный Path объект

        Raises:
            ValidationError: При невалидном пуге
        """
        if isinstance(path, str):
            path = Path(path)

        if not isinstance(path, Path):
            raise ValidationError(
                f"Путь должен быть строкой или Path объектом: {type(path)}"
            )

        if must_exist and not path.exists():
            raise ValidationError(f"Файл не найден: {path}")

        if must_exist and not path.is_file():
            raise ValidationError(f"Путь не является файлом: {path}")

        return path

    @staticmethod
    def validate_directory_path(
        path: Union[str, Path], must_exist: bool = True
    ) -> Path:
        """Валидация пути к директории"""
        if isinstance(path, str):
            path = Path(path)

        if not isinstance(path, Path):
            raise ValidationError(
                f"Путь должен быть строкой или Path объектом: {type(path)}"
            )

        if must_exist and not path.exists():
            raise ValidationError(f"Директория не найдена: {path}")

        if must_exist and not path.is_dir():
            raise ValidationError(f"Путь не является директорией: {path}")

        return path

    @staticmethod
    def validate_text(
        text: str, min_length: int = 1, max_length: Optional[int] = None
    ) -> str:
        """Валидация текста"""
        if not isinstance(text, str):
            raise ValidationError(f"Текст должен быть строкой: {type(text)}")

        text = text.strip()

        if len(text) < min_length:
            raise ValidationError(
                f"Текст слишком короткий. Минимум: {min_length}, получено: {len(text)}"
            )

        if max_length and len(text) > max_length:
            raise ValidationError(
                f"Текст слишком длинный. Максимум: {max_length}, получено: {len(text)}"
            )

        return text

    @staticmethod
    def validate_search_params(
        query: str,
        top_k: Optional[int] = None,
        similarity_threshold: Optional[float] = None,
    ) -> Dict[str, Any]:
        """Валидация параметров поиска"""

        # Валидация запроса
        query = DataValidator.validate_text(query, min_length=2, max_length=1000)

        # Валидация количества результатов
        if top_k is not None:
            if not isinstance(top_k, int) or top_k < 1 or top_k > 1000:
                raise ValidationError(
                    f"top_k должно быть целым числом от 1 до 1000: {top_k}"
                )

        # Валидация порога схожести
        if similarity_threshold is not None:
            if not isinstance(similarity_threshold, (int, float)) or not (
                0 <= similarity_threshold <= 1
            ):
                raise ValidationError(
                    f"similarity_threshold должен быть числом от 0 до 1: {similarity_threshold}"
                )

        return {
            "query": query,
            "top_k": top_k,
            "similarity_threshold": similarity_threshold,
        }

    @staticmethod
    def validate_model_params(**params) -> Dict[str, Any]:
        """Валидация параметров модели Doc2Vec"""
        validated = {}

        # Размерность векторов
        if "vector_size" in params:
            vs = params["vector_size"]
            if not isinstance(vs, int) or not (50 <= vs <= 1000):
                raise ValidationError(f"vector_size должен быть от 50 до 1000: {vs}")
            validated["vector_size"] = vs

        # Размер окна
        if "window" in params:
            w = params["window"]
            if not isinstance(w, int) or not (1 <= w <= 50):
                raise ValidationError(f"window должен быть от 1 до 50: {w}")
            validated["window"] = w

        # Минимальная частота
        if "min_count" in params:
            mc = params["min_count"]
            if not isinstance(mc, int) or not (1 <= mc <= 100):
                raise ValidationError(f"min_count должен быть от 1 до 100: {mc}")
            validated["min_count"] = mc

        # Количество эпох
        if "epochs" in params:
            e = params["epochs"]
            if not isinstance(e, int) or not (1 <= e <= 1000):
                raise ValidationError(f"epochs должен быть от 1 до 1000: {e}")
            validated["epochs"] = e

        # Количество потоков
        if "workers" in params:
            w = params["workers"]
            if not isinstance(w, int) or not (1 <= w <= 32):
                raise ValidationError(f"workers должен быть от 1 до 32: {w}")
            validated["workers"] = w

        return validated


class FileValidator:
    """Валидатор для файлов"""

    SUPPORTED_EXTENSIONS = {".pdf", ".docx", ".doc", ".txt"}
    MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB

    @classmethod
    def validate_document_file(cls, file_path: Path) -> Dict[str, Any]:
        """
        Комплексная валидация файла документа

        Returns:
            Словарь с результатами валидации
        """
        result = {"valid": True, "errors": [], "warnings": [], "file_info": {}}

        try:
            # Проверка существования
            if not file_path.exists():
                result["errors"].append(f"Файл не найден: {file_path}")
                result["valid"] = False
                return result

            # Проверка, что это файл
            if not file_path.is_file():
                result["errors"].append(f"Путь не является файлом: {file_path}")
                result["valid"] = False
                return result

            # Информация о файле
            stat = file_path.stat()
            result["file_info"] = {
                "size": stat.st_size,
                "size_mb": stat.st_size / 1024 / 1024,
                "extension": file_path.suffix.lower(),
                "name": file_path.name,
            }

            # Проверка расширения
            if file_path.suffix.lower() not in cls.SUPPORTED_EXTENSIONS:
                result["errors"].append(
                    f"Неподдерживаемое расширение: {file_path.suffix}"
                )
                result["valid"] = False

            # Проверка размера
            if stat.st_size > cls.MAX_FILE_SIZE:
                result["errors"].append(
                    f"Файл слишком большой: {stat.st_size / 1024 / 1024:.1f}MB"
                )
                result["valid"] = False
            elif stat.st_size > cls.MAX_FILE_SIZE * 0.8:
                result["warnings"].append(
                    f"Большой файл: {stat.st_size / 1024 / 1024:.1f}MB"
                )

            # Проверка пустого файла
            if stat.st_size == 0:
                result["errors"].append("Файл пустой")
                result["valid"] = False
            elif stat.st_size < 100:  # Меньше 100 байт
                result["warnings"].append("Очень маленький файл")

            # Проверка доступности для чтения
            try:
                with open(file_path, "rb") as f:
                    f.read(1)
            except PermissionError:
                result["errors"].append("Нет прав на чтение файла")
                result["valid"] = False
            except Exception as e:
                result["errors"].append(f"Ошибка при чтении файла: {e}")
                result["valid"] = False

        except Exception as e:
            result["errors"].append(f"Неожиданная ошибка валидации: {e}")
            result["valid"] = False

        return result


========================================
FILE: src\semantic_search\evaluation\__init__.py
========================================
"""Модуль для оценки и сравнения методов поиска"""

from .baselines import BaseSearchMethod, OpenAISearchBaseline
from .comparison import SearchComparison
from .metrics import SearchMetrics

__all__ = [
    "BaseSearchMethod",
    "OpenAISearchBaseline",
    "SearchComparison",
    "SearchMetrics",
]


========================================
FILE: src\semantic_search\evaluation\baselines.py
========================================
"""Базовые классы и альтернативные методы поиска"""

import json
import os
import time
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
from loguru import logger

from semantic_search.core.search_engine import SearchResult
from semantic_search.utils.text_utils import TextProcessor


class BaseSearchMethod(ABC):
    """Абстрактный базовый класс для методов поиска"""

    def __init__(self, name: str):
        self.name = name
        self.index_time = 0.0
        self.indexed_documents = []

    @abstractmethod
    def index(self, documents: List[Tuple[str, str, str]]) -> None:
        """
        Индексация документов

        Args:
            documents: Список кортежей (doc_id, text, metadata)
        """
        pass

    @abstractmethod
    def search(self, query: str, top_k: int = 10) -> List[SearchResult]:
        """
        Поиск документов

        Args:
            query: Поисковый запрос
            top_k: Количество результатов

        Returns:
            Список результатов поиска
        """
        pass

    def get_method_name(self) -> str:
        """Получить название метода"""
        return self.name

    def get_stats(self) -> Dict[str, Any]:
        """Получить статистику метода"""
        return {
            "method_name": self.name,
            "indexed_documents": len(self.indexed_documents),
            "index_time": self.index_time,
        }


class Doc2VecSearchAdapter(BaseSearchMethod):
    """Адаптер для существующего Doc2Vec поиска"""

    def __init__(self, search_engine, corpus_info):
        super().__init__("Doc2Vec")
        self.search_engine = search_engine
        self.corpus_info = corpus_info

    def index(self, documents: List[Tuple[str, str, str]]) -> None:
        """Doc2Vec уже проиндексирован при обучении"""
        self.indexed_documents = documents

    def search(self, query: str, top_k: int = 10) -> List[SearchResult]:
        """Использует существующий поисковый движок"""
        return self.search_engine.search(query, top_k=top_k)


class OpenAISearchBaseline(BaseSearchMethod):
    """Поиск с использованием OpenAI embeddings"""

    def __init__(
        self, api_key: Optional[str] = None, model: str = "text-embedding-ada-002"
    ):
        super().__init__(f"OpenAI ({model})")
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.model = model
        self.embeddings = {}
        self.documents = {}
        self.text_processor = TextProcessor()

        if not self.api_key:
            raise ValueError(
                "OpenAI API key не найден. Установите переменную окружения OPENAI_API_KEY"
            )

        # Lazy import для опциональной зависимости
        try:
            import openai

            self.openai = openai
            self.client = openai.OpenAI(api_key=self.api_key)
        except ImportError:
            raise ImportError("Установите openai: pip install openai")

    def _get_embedding(self, text: str) -> List[float]:
        """Получить embedding для текста"""
        try:
            # Ограничиваем длину текста
            if len(text) > 8000:
                text = text[:8000]

            response = self.client.embeddings.create(model=self.model, input=text)
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Ошибка при получении embedding: {e}")
            raise

    def index(self, documents: List[Tuple[str, str, str]]) -> None:
        """
        Индексация документов через OpenAI API

        Args:
            documents: Список кортежей (doc_id, text, metadata)
        """
        start_time = time.time()
        logger.info(f"Начинаем индексацию {len(documents)} документов через OpenAI")

        for i, (doc_id, text, metadata) in enumerate(documents):
            try:
                # Создаем краткое представление документа для embedding
                # Берем первые 2000 символов + последние 1000
                if len(text) > 3000:
                    text_sample = text[:2000] + " ... " + text[-1000:]
                else:
                    text_sample = text

                # Получаем embedding
                embedding = self._get_embedding(text_sample)

                self.embeddings[doc_id] = np.array(embedding)
                self.documents[doc_id] = {"text": text, "metadata": metadata}

                if (i + 1) % 10 == 0:
                    logger.info(f"Проиндексировано {i + 1}/{len(documents)} документов")

                # Задержка для соблюдения rate limits
                time.sleep(0.1)

            except Exception as e:
                logger.error(f"Ошибка при индексации документа {doc_id}: {e}")
                continue

        self.indexed_documents = documents
        self.index_time = time.time() - start_time
        logger.info(f"Индексация завершена за {self.index_time:.2f} секунд")

    def search(self, query: str, top_k: int = 10) -> List[SearchResult]:
        """
        Поиск документов по запросу

        Args:
            query: Поисковый запрос
            top_k: Количество результатов

        Returns:
            Список результатов поиска
        """
        if not self.embeddings:
            logger.error("Индекс пуст. Сначала проиндексируйте документы")
            return []

        try:
            # Получаем embedding запроса
            query_embedding = np.array(self._get_embedding(query))

            # Вычисляем косинусное сходство
            similarities = []
            for doc_id, doc_embedding in self.embeddings.items():
                # Косинусное сходство
                similarity = np.dot(query_embedding, doc_embedding) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)
                )
                similarities.append((doc_id, similarity))

            # Сортируем по убыванию сходства
            similarities.sort(key=lambda x: x[1], reverse=True)

            # Создаем результаты
            results = []
            for doc_id, similarity in similarities[:top_k]:
                metadata = self.documents[doc_id].get("metadata", {})
                results.append(SearchResult(doc_id, float(similarity), metadata))

            return results

        except Exception as e:
            logger.error(f"Ошибка при поиске: {e}")
            return []

    def save_index(self, path: Path) -> None:
        """Сохранить индекс для повторного использования"""
        data = {
            "embeddings": {k: v.tolist() for k, v in self.embeddings.items()},
            "documents": self.documents,
            "model": self.model,
        }

        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f)

        logger.info(f"Индекс сохранен в {path}")

    def load_index(self, path: Path) -> None:
        """Загрузить индекс"""
        if not path.exists():
            raise FileNotFoundError(f"Файл индекса не найден: {path}")

        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)

        self.embeddings = {k: np.array(v) for k, v in data["embeddings"].items()}
        self.documents = data["documents"]
        self.indexed_documents = list(self.documents.keys())

        logger.info(f"Индекс загружен из {path}")


========================================
FILE: src\semantic_search\evaluation\comparison.py
========================================
"""Модуль для сравнения различных методов поиска"""

import json
import time
from pathlib import Path
from typing import Dict, List, Optional, Set

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from loguru import logger

from semantic_search.config import DATA_DIR

from .baselines import BaseSearchMethod
from .metrics import SearchMetrics


class QueryTestCase:
    """Тестовый случай для оценки поиска"""

    def __init__(
        self,
        query: str,
        relevant_docs: Set[str],
        relevance_scores: Optional[Dict[str, float]] = None,
        description: str = "",
    ):
        self.query = query
        self.relevant_docs = relevant_docs
        self.relevance_scores = relevance_scores or {}
        self.description = description


class SearchComparison:
    """Класс для сравнения методов поиска"""

    def __init__(self, test_cases: Optional[List[QueryTestCase]] = None):
        self.test_cases = test_cases or []
        self.results = {}
        self.metrics = SearchMetrics()

    def add_test_case(self, test_case: QueryTestCase) -> None:
        """Добавить тестовый случай"""
        self.test_cases.append(test_case)

    def create_default_test_cases(self) -> List[QueryTestCase]:
        """Создать стандартный набор тестовых случаев для демонстрации"""
        test_cases = [
            # QueryTestCase(
            #     query="машинное обучение и нейронные сети",
            #     relevant_docs={
            #         "ml_basics.pdf",
            #         "neural_networks.pdf",
            #         "deep_learning.pdf",
            #     },
            #     relevance_scores={
            #         "ml_basics.pdf": 3,
            #         "neural_networks.pdf": 3,
            #         "deep_learning.pdf": 2,
            #         "ai_overview.pdf": 1,
            #     },
            #     description="Базовый запрос по ML",
            # ),
            # QueryTestCase(
            #     query="глубокое обучение для обработки изображений",
            #     relevant_docs={
            #         "cnn_tutorial.pdf",
            #         "image_processing.pdf",
            #         "deep_learning.pdf",
            #     },
            #     relevance_scores={
            #         "cnn_tutorial.pdf": 3,
            #         "image_processing.pdf": 3,
            #         "deep_learning.pdf": 2,
            #         "computer_vision.pdf": 2,
            #     },
            #     description="Специализированный запрос по CV",
            # ),
            QueryTestCase(
                query="методы глокализации",
                relevant_docs={
                    "nlp_transformers.pdf",
                    "bert_paper.pdf",
                    "attention_mechanism.pdf",
                },
                relevance_scores={
                    "nlp_transformers.pdf": 3,
                    "bert_paper.pdf": 3,
                    "attention_mechanism.pdf": 2,
                    "nlp_basics.pdf": 1,
                },
                description="Запрос по NLP",
            ),
            # QueryTestCase(
            #     query="градиентный спуск оптимизация",
            #     relevant_docs={"optimization_methods.pdf", "gradient_descent.pdf"},
            #     relevance_scores={
            #         "optimization_methods.pdf": 3,
            #         "gradient_descent.pdf": 3,
            #         "ml_basics.pdf": 1,
            #     },
            #     description="Запрос по методам оптимизации",
            # ),
            # QueryTestCase(
            #     query="рекуррентные нейронные сети LSTM",
            #     relevant_docs={
            #         "rnn_tutorial.pdf",
            #         "lstm_explained.pdf",
            #         "sequence_models.pdf",
            #     },
            #     relevance_scores={
            #         "rnn_tutorial.pdf": 3,
            #         "lstm_explained.pdf": 3,
            #         "sequence_models.pdf": 2,
            #     },
            #     description="Запрос по RNN",
            # ),
        ]

        return test_cases

    def evaluate_method(
        self, method: BaseSearchMethod, top_k: int = 10, verbose: bool = True
    ) -> Dict[str, any]:
        """
        Оценить метод поиска на всех тестовых случаях

        Args:
            method: Метод поиска для оценки
            top_k: Количество результатов для извлечения
            verbose: Выводить прогресс

        Returns:
            Словарь с результатами оценки
        """
        method_name = method.get_method_name()

        if verbose:
            logger.info(f"Оценка метода: {method_name}")

        all_metrics = []
        query_times = []
        all_results = []

        for i, test_case in enumerate(self.test_cases):
            if verbose and (i + 1) % 5 == 0:
                logger.info(f"Обработано запросов: {i + 1}/{len(self.test_cases)}")

            # Измеряем время выполнения запроса
            start_time = time.time()
            search_results = method.search(test_case.query, top_k=top_k)
            query_time = time.time() - start_time
            query_times.append(query_time)

            # Извлекаем ID документов из результатов
            retrieved_docs = [result.doc_id for result in search_results]

            # Вычисляем метрики
            metrics = self.metrics.calculate_all_metrics(
                retrieved=retrieved_docs,
                relevant=test_case.relevant_docs,
                relevance_scores=test_case.relevance_scores,
                k_values=[1, 5, 10],
            )

            # Добавляем информацию о запросе
            metrics["query"] = test_case.query
            metrics["query_time"] = query_time

            all_metrics.append(metrics)
            all_results.append((retrieved_docs, test_case.relevant_docs))

        # Вычисляем агрегированные метрики
        aggregated = self._aggregate_metrics(all_metrics)

        # MAP и MRR
        aggregated["MAP"] = self.metrics.mean_average_precision(all_results)
        aggregated["MRR"] = self.metrics.mean_reciprocal_rank(all_results)

        # Статистика по времени
        aggregated["avg_query_time"] = np.mean(query_times)
        aggregated["std_query_time"] = np.std(query_times)
        aggregated["median_query_time"] = np.median(query_times)

        # Сохраняем результаты
        self.results[method_name] = {
            "aggregated": aggregated,
            "detailed": all_metrics,
            "method_stats": method.get_stats(),
        }

        if verbose:
            logger.info(f"Оценка {method_name} завершена")
            logger.info(f"Среднее время запроса: {aggregated['avg_query_time']:.3f}с")
            logger.info(f"MAP: {aggregated['MAP']:.3f}")
            logger.info(f"MRR: {aggregated['MRR']:.3f}")

        return self.results[method_name]

    def _aggregate_metrics(self, metrics_list: List[Dict]) -> Dict[str, float]:
        """Агрегировать метрики по всем запросам"""
        aggregated = {}

        # Получаем все ключи метрик (исключая служебные)
        metric_keys = [
            k for k in metrics_list[0].keys() if k not in ["query", "query_time"]
        ]

        # Вычисляем среднее для каждой метрики
        for key in metric_keys:
            values = [m[key] for m in metrics_list]
            aggregated[f"avg_{key}"] = np.mean(values)
            aggregated[f"std_{key}"] = np.std(values)

        return aggregated

    def compare_methods(
        self,
        methods: List[BaseSearchMethod],
        top_k: int = 10,
        save_results: bool = True,
    ) -> pd.DataFrame:
        """
        Сравнить несколько методов

        Args:
            methods: Список методов для сравнения
            top_k: Количество результатов
            save_results: Сохранить результаты в файл

        Returns:
            DataFrame с результатами сравнения
        """
        logger.info(f"Начинаем сравнение {len(methods)} методов")

        # Оцениваем каждый метод
        for method in methods:
            self.evaluate_method(method, top_k=top_k)

        # Создаем сравнительную таблицу
        comparison_data = []

        for method_name, results in self.results.items():
            row = {
                "Method": method_name,
                "MAP": results["aggregated"]["MAP"],
                "MRR": results["aggregated"]["MRR"],
                "Avg Query Time (s)": results["aggregated"]["avg_query_time"],
                "Index Time (s)": results["method_stats"]["index_time"],
            }

            # Добавляем метрики для разных k
            for k in [1, 5, 10]:
                row[f"P@{k}"] = results["aggregated"][f"avg_precision@{k}"]
                row[f"R@{k}"] = results["aggregated"][f"avg_recall@{k}"]
                if f"avg_ndcg@{k}" in results["aggregated"]:
                    row[f"NDCG@{k}"] = results["aggregated"][f"avg_ndcg@{k}"]

            comparison_data.append(row)

        df_comparison = pd.DataFrame(comparison_data)

        # Сохраняем результаты
        if save_results:
            results_dir = DATA_DIR / "evaluation_results"
            results_dir.mkdir(exist_ok=True)

            # Сохраняем таблицу
            df_comparison.to_csv(results_dir / "comparison_results.csv", index=False)

            # Сохраняем детальные результаты
            with open(
                results_dir / "detailed_results.json", "w", encoding="utf-8"
            ) as f:
                json.dump(self.results, f, indent=2)

            logger.info(f"Результаты сохранены в {results_dir}")

        return df_comparison

    def plot_comparison(self, save_plots: bool = True) -> None:
        """Визуализация результатов сравнения"""
        if not self.results:
            logger.error("Нет результатов для визуализации")
            return

        # Подготовка данных для визуализации
        methods = list(self.results.keys())

        # Создаем фигуру с подграфиками
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle("Сравнение методов поиска", fontsize=16)

        # 1. Сравнение основных метрик
        ax1 = axes[0, 0]
        metrics_data = {
            "MAP": [self.results[m]["aggregated"]["MAP"] for m in methods],
            "MRR": [self.results[m]["aggregated"]["MRR"] for m in methods],
            "P@10": [
                self.results[m]["aggregated"]["avg_precision@10"] for m in methods
            ],
            "R@10": [self.results[m]["aggregated"]["avg_recall@10"] for m in methods],
        }

        x = np.arange(len(methods))
        width = 0.2

        for i, (metric, values) in enumerate(metrics_data.items()):
            ax1.bar(x + i * width, values, width, label=metric)

        ax1.set_xlabel("Методы")
        ax1.set_ylabel("Значение метрики")
        ax1.set_title("Основные метрики качества поиска")
        ax1.set_xticks(x + width * 1.5)
        ax1.set_xticklabels(methods)
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # 2. Precision-Recall для разных k
        ax2 = axes[0, 1]
        k_values = [1, 5, 10]

        for method in methods:
            precisions = [
                self.results[method]["aggregated"][f"avg_precision@{k}"]
                for k in k_values
            ]
            recalls = [
                self.results[method]["aggregated"][f"avg_recall@{k}"] for k in k_values
            ]
            ax2.plot(recalls, precisions, marker="o", label=method)

        ax2.set_xlabel("Recall")
        ax2.set_ylabel("Precision")
        ax2.set_title("Precision-Recall кривые")
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # 3. Время выполнения
        ax3 = axes[1, 0]
        query_times = [self.results[m]["aggregated"]["avg_query_time"] for m in methods]
        index_times = [self.results[m]["method_stats"]["index_time"] for m in methods]

        x = np.arange(len(methods))
        width = 0.35

        ax3.bar(x - width / 2, query_times, width, label="Среднее время запроса")
        ax3.bar(x + width / 2, index_times, width, label="Время индексации")

        ax3.set_xlabel("Методы")
        ax3.set_ylabel("Время (секунды)")
        ax3.set_title("Производительность методов")
        ax3.set_xticks(x)
        ax3.set_xticklabels(methods)
        ax3.legend()
        ax3.grid(True, alpha=0.3)

        # 4. NDCG для разных k
        ax4 = axes[1, 1]

        for method in methods:
            if "avg_ndcg@1" in self.results[method]["aggregated"]:
                ndcg_values = [
                    self.results[method]["aggregated"][f"avg_ndcg@{k}"]
                    for k in k_values
                ]
                ax4.plot(k_values, ndcg_values, marker="s", label=method)

        ax4.set_xlabel("k")
        ax4.set_ylabel("NDCG@k")
        ax4.set_title("NDCG для различных k")
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        ax4.set_xticks(k_values)

        plt.tight_layout()

        if save_plots:
            plots_dir = DATA_DIR / "evaluation_results" / "plots"
            plots_dir.mkdir(exist_ok=True, parents=True)
            plt.savefig(
                plots_dir / "comparison_plots.png", dpi=300, bbox_inches="tight"
            )
            logger.info(f"Графики сохранены в {plots_dir}")

        plt.show()

    def plot_detailed_metrics(self, method_name: str, save_plot: bool = True) -> None:
        """Детальная визуализация метрик для конкретного метода"""
        if method_name not in self.results:
            logger.error(f"Результаты для метода {method_name} не найдены")
            return

        detailed = self.results[method_name]["detailed"]

        # Создаем DataFrame для удобства
        df = pd.DataFrame(detailed)

        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f"Детальный анализ метода: {method_name}", fontsize=16)

        # 1. Распределение Average Precision
        ax1 = axes[0, 0]
        ax1.hist(
            df["average_precision"], bins=20, alpha=0.7, color="blue", edgecolor="black"
        )
        ax1.axvline(
            df["average_precision"].mean(),
            color="red",
            linestyle="--",
            label=f"Среднее: {df['average_precision'].mean():.3f}",
        )
        ax1.set_xlabel("Average Precision")
        ax1.set_ylabel("Количество запросов")
        ax1.set_title("Распределение Average Precision")
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # 2. Время выполнения запросов
        ax2 = axes[0, 1]
        ax2.scatter(range(len(df)), df["query_time"], alpha=0.6)
        ax2.axhline(
            df["query_time"].mean(),
            color="red",
            linestyle="--",
            label=f"Среднее: {df['query_time'].mean():.3f}s",
        )
        ax2.set_xlabel("Номер запроса")
        ax2.set_ylabel("Время (секунды)")
        ax2.set_title("Время выполнения запросов")
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # 3. Метрики по k
        ax3 = axes[1, 0]
        k_values = [1, 5, 10]
        metrics = ["precision", "recall", "f1"]

        for metric in metrics:
            values = [df[f"{metric}@{k}"].mean() for k in k_values]
            ax3.plot(k_values, values, marker="o", label=metric.capitalize())

        ax3.set_xlabel("k")
        ax3.set_ylabel("Значение метрики")
        ax3.set_title("Метрики для различных k")
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        ax3.set_xticks(k_values)

        # 4. Топ-10 лучших и худших запросов по AP
        ax4 = axes[1, 1]

        sorted_df = df.sort_values("average_precision")
        worst_5 = sorted_df.head(5)
        best_5 = sorted_df.tail(5)

        combined = pd.concat([worst_5, best_5])
        colors = ["red"] * 5 + ["green"] * 5

        y_pos = np.arange(len(combined))
        ax4.barh(y_pos, combined["average_precision"], color=colors, alpha=0.7)

        # Обрезаем длинные запросы для отображения
        labels = [q[:50] + "..." if len(q) > 50 else q for q in combined["query"]]
        ax4.set_yticks(y_pos)
        ax4.set_yticklabels(labels, fontsize=8)
        ax4.set_xlabel("Average Precision")
        ax4.set_title("Лучшие и худшие запросы по AP")
        ax4.grid(True, alpha=0.3, axis="x")

        plt.tight_layout()

        if save_plot:
            plots_dir = DATA_DIR / "evaluation_results" / "plots"
            plots_dir.mkdir(exist_ok=True, parents=True)
            plt.savefig(
                plots_dir / f"{method_name.replace(' ', '_')}_detailed.png",
                dpi=300,
                bbox_inches="tight",
            )
            logger.info(f"Детальные графики сохранены для {method_name}")

        plt.show()

    def generate_report(self, output_path: Optional[Path] = None) -> str:
        """
        Генерация текстового отчета о сравнении

        Args:
            output_path: Путь для сохранения отчета

        Returns:
            Текст отчета
        """
        if not self.results:
            return "Нет результатов для генерации отчета"

        report = []
        report.append("=" * 80)
        report.append("ОТЧЕТ О СРАВНЕНИИ МЕТОДОВ ПОИСКА")
        report.append("=" * 80)
        report.append("")

        # Общая информация
        report.append(f"Количество тестовых запросов: {len(self.test_cases)}")
        report.append(f"Оцененные методы: {', '.join(self.results.keys())}")
        report.append("")

        # Сводная таблица
        report.append("СВОДНАЯ ТАБЛИЦА РЕЗУЛЬТАТОВ")
        report.append("-" * 80)

        # Создаем DataFrame для красивого вывода
        comparison_data = []
        for method_name, results in self.results.items():
            row = {
                "Метод": method_name,
                "MAP": f"{results['aggregated']['MAP']:.3f}",
                "MRR": f"{results['aggregated']['MRR']:.3f}",
                "P@10": f"{results['aggregated']['avg_precision@10']:.3f}",
                "R@10": f"{results['aggregated']['avg_recall@10']:.3f}",
                "Время запроса (с)": f"{results['aggregated']['avg_query_time']:.3f}",
                "Время индексации (с)": f"{results['method_stats']['index_time']:.1f}",
            }
            comparison_data.append(row)

        df = pd.DataFrame(comparison_data)
        report.append(df.to_string(index=False))
        report.append("")

        # Выводы
        report.append("ОСНОВНЫЕ ВЫВОДЫ")
        report.append("-" * 80)

        # Определяем лучший метод по MAP
        best_method = max(self.results.items(), key=lambda x: x[1]["aggregated"]["MAP"])
        report.append(
            f"✓ Лучший метод по MAP: {best_method[0]} ({best_method[1]['aggregated']['MAP']:.3f})"
        )

        # Определяем самый быстрый метод
        fastest_method = min(
            self.results.items(), key=lambda x: x[1]["aggregated"]["avg_query_time"]
        )
        report.append(
            f"✓ Самый быстрый метод: {fastest_method[0]} ({fastest_method[1]['aggregated']['avg_query_time']:.3f}с)"
        )

        # Сравнение Doc2Vec и OpenAI
        if "Doc2Vec" in self.results and any(
            "OpenAI" in m for m in self.results.keys()
        ):
            doc2vec_map = self.results["Doc2Vec"]["aggregated"]["MAP"]
            openai_method = next(m for m in self.results.keys() if "OpenAI" in m)
            openai_map = self.results[openai_method]["aggregated"]["MAP"]

            improvement = ((doc2vec_map - openai_map) / openai_map) * 100

            report.append("")
            report.append("СРАВНЕНИЕ DOC2VEC И OPENAI")
            report.append("-" * 80)

            if doc2vec_map > openai_map:
                report.append(
                    f"✓ Doc2Vec превосходит {openai_method} по MAP на {improvement:.1f}%"
                )
            else:
                report.append(
                    f"✗ {openai_method} превосходит Doc2Vec по MAP на {-improvement:.1f}%"
                )

            # Сравнение по скорости
            doc2vec_time = self.results["Doc2Vec"]["aggregated"]["avg_query_time"]
            openai_time = self.results[openai_method]["aggregated"]["avg_query_time"]
            time_ratio = openai_time / doc2vec_time

            report.append(f"✓ Doc2Vec быстрее {openai_method} в {time_ratio:.1f} раз")

            # Экономическая выгода
            report.append("")
            report.append("ЭКОНОМИЧЕСКАЯ ЭФФЕКТИВНОСТЬ")
            report.append("-" * 80)
            report.append("При 1000 запросов в день:")

            # Примерная стоимость OpenAI embeddings
            openai_cost_per_1k_tokens = 0.0001  # $0.0001 per 1K tokens
            avg_tokens_per_query = 50  # примерно
            daily_cost = (
                1000 * avg_tokens_per_query / 1000
            ) * openai_cost_per_1k_tokens
            monthly_cost = daily_cost * 30

            report.append(f"- Стоимость OpenAI: ~${monthly_cost:.2f}/месяц")
            report.append("- Стоимость Doc2Vec: $0 (после обучения)")
            report.append(f"- Экономия: ${monthly_cost:.2f}/месяц")

        report.append("")
        report.append("=" * 80)
        report.append(f"Отчет сгенерирован: {time.strftime('%Y-%m-%d %H:%M:%S')}")

        report_text = "\n".join(report)

        # Сохраняем отчет
        if output_path:
            with open(output_path, "w", encoding="utf-8") as f:
                f.write(report_text)
            logger.info(f"Отчет сохранен в {output_path}")

        return report_text


========================================
FILE: src\semantic_search\evaluation\metrics.py
========================================
"""Метрики для оценки качества поиска"""

from typing import Dict, List, Set, Tuple

import numpy as np


class SearchMetrics:
    """Класс для вычисления метрик качества поиска"""

    @staticmethod
    def precision_at_k(retrieved: List[str], relevant: Set[str], k: int) -> float:
        """
        Точность на позиции k

        Args:
            retrieved: Список найденных документов (в порядке ранжирования)
            relevant: Множество релевантных документов
            k: Позиция для вычисления точности

        Returns:
            Precision@k
        """
        if k <= 0 or not retrieved:
            return 0.0

        retrieved_k = retrieved[:k]
        relevant_in_k = sum(1 for doc in retrieved_k if doc in relevant)

        return relevant_in_k / k

    @staticmethod
    def recall_at_k(retrieved: List[str], relevant: Set[str], k: int) -> float:
        """
        Полнота на позиции k

        Args:
            retrieved: Список найденных документов
            relevant: Множество релевантных документов
            k: Позиция для вычисления полноты

        Returns:
            Recall@k
        """
        if not relevant or k <= 0:
            return 0.0

        retrieved_k = retrieved[:k]
        relevant_in_k = sum(1 for doc in retrieved_k if doc in relevant)

        return relevant_in_k / len(relevant)

    @staticmethod
    def average_precision(retrieved: List[str], relevant: Set[str]) -> float:
        """
        Средняя точность (AP)

        Args:
            retrieved: Список найденных документов
            relevant: Множество релевантных документов

        Returns:
            Average Precision
        """
        if not relevant or not retrieved:
            return 0.0

        precisions = []
        relevant_found = 0

        for i, doc in enumerate(retrieved):
            if doc in relevant:
                relevant_found += 1
                precision = relevant_found / (i + 1)
                precisions.append(precision)

        if not precisions:
            return 0.0

        return sum(precisions) / len(relevant)

    @staticmethod
    def mean_average_precision(results: List[Tuple[List[str], Set[str]]]) -> float:
        """
        Средняя точность по всем запросам (MAP)

        Args:
            results: Список кортежей (retrieved, relevant) для каждого запроса

        Returns:
            Mean Average Precision
        """
        if not results:
            return 0.0

        ap_scores = [
            SearchMetrics.average_precision(retrieved, relevant)
            for retrieved, relevant in results
        ]

        return sum(ap_scores) / len(ap_scores)

    @staticmethod
    def dcg_at_k(
        retrieved: List[str], relevance_scores: Dict[str, float], k: int
    ) -> float:
        """
        Discounted Cumulative Gain на позиции k

        Args:
            retrieved: Список найденных документов
            relevance_scores: Словарь с оценками релевантности (0-3)
            k: Позиция для вычисления DCG

        Returns:
            DCG@k
        """
        if k <= 0 or not retrieved:
            return 0.0

        dcg = 0.0
        for i, doc in enumerate(retrieved[:k]):
            rel = relevance_scores.get(doc, 0)
            # Используем log2(i+2) так как индексация начинается с 0
            dcg += (2**rel - 1) / np.log2(i + 2)

        return dcg

    @staticmethod
    def ndcg_at_k(
        retrieved: List[str], relevance_scores: Dict[str, float], k: int
    ) -> float:
        """
        Normalized Discounted Cumulative Gain на позиции k

        Args:
            retrieved: Список найденных документов
            relevance_scores: Словарь с оценками релевантности
            k: Позиция для вычисления NDCG

        Returns:
            NDCG@k
        """
        dcg = SearchMetrics.dcg_at_k(retrieved, relevance_scores, k)

        # Идеальный порядок - сортировка по убыванию релевантности
        ideal_order = sorted(
            relevance_scores.keys(), key=lambda x: relevance_scores[x], reverse=True
        )

        idcg = SearchMetrics.dcg_at_k(ideal_order, relevance_scores, k)

        if idcg == 0:
            return 0.0

        return dcg / idcg

    @staticmethod
    def mean_reciprocal_rank(results: List[Tuple[List[str], Set[str]]]) -> float:
        """
        Mean Reciprocal Rank (MRR)

        Args:
            results: Список кортежей (retrieved, relevant) для каждого запроса

        Returns:
            MRR
        """
        if not results:
            return 0.0

        reciprocal_ranks = []

        for retrieved, relevant in results:
            # Находим позицию первого релевантного документа
            for i, doc in enumerate(retrieved):
                if doc in relevant:
                    reciprocal_ranks.append(1 / (i + 1))
                    break
            else:
                # Если релевантных документов не найдено
                reciprocal_ranks.append(0.0)

        return sum(reciprocal_ranks) / len(reciprocal_ranks)

    @staticmethod
    def f1_at_k(retrieved: List[str], relevant: Set[str], k: int) -> float:
        """
        F1-мера на позиции k

        Args:
            retrieved: Список найденных документов
            relevant: Множество релевантных документов
            k: Позиция для вычисления F1

        Returns:
            F1@k
        """
        precision = SearchMetrics.precision_at_k(retrieved, relevant, k)
        recall = SearchMetrics.recall_at_k(retrieved, relevant, k)

        if precision + recall == 0:
            return 0.0

        return 2 * (precision * recall) / (precision + recall)

    @staticmethod
    def calculate_all_metrics(
        retrieved: List[str],
        relevant: Set[str],
        relevance_scores: Dict[str, float] = None,
        k_values: List[int] = [1, 5, 10],
    ) -> Dict[str, float]:
        """
        Вычислить все метрики для одного запроса

        Args:
            retrieved: Список найденных документов
            relevant: Множество релевантных документов
            relevance_scores: Оценки релевантности (для NDCG)
            k_values: Значения k для метрик

        Returns:
            Словарь с метриками
        """
        metrics = {}

        # Метрики для разных k
        for k in k_values:
            metrics[f"precision@{k}"] = SearchMetrics.precision_at_k(
                retrieved, relevant, k
            )
            metrics[f"recall@{k}"] = SearchMetrics.recall_at_k(retrieved, relevant, k)
            metrics[f"f1@{k}"] = SearchMetrics.f1_at_k(retrieved, relevant, k)

            if relevance_scores:
                metrics[f"ndcg@{k}"] = SearchMetrics.ndcg_at_k(
                    retrieved, relevance_scores, k
                )

        # Общие метрики
        metrics["average_precision"] = SearchMetrics.average_precision(
            retrieved, relevant
        )

        # MRR для одного запроса
        for i, doc in enumerate(retrieved):
            if doc in relevant:
                metrics["reciprocal_rank"] = 1 / (i + 1)
                break
        else:
            metrics["reciprocal_rank"] = 0.0

        return metrics


========================================
FILE: scripts\build.py
========================================
"""Скрипт для сборки исполняемого файла с помощью PyInstaller"""

import argparse
import shutil
import sys
from pathlib import Path


def clean_build_dirs():
    """Очистка старых директорий сборки"""
    dirs_to_clean = ["build", "dist"]
    for dir_name in dirs_to_clean:
        if Path(dir_name).exists():
            shutil.rmtree(dir_name)
            print(f"✓ Удалена директория {dir_name}")


def create_spec_file(onefile=False, windowed=True, name="SemanticSearch"):
    """Создание spec файла для PyInstaller"""

    # Пути
    src_path = Path(__file__).parent.parent / "src"
    main_script = src_path / "semantic_search" / "main.py"

    # Дополнительные данные
    datas = [
        # Конфигурации
        ("config", "config"),
        # Иконка если есть
        # ("assets/icon.ico", "assets"),
    ]

    # Скрытые импорты
    hidden_imports = [
        "semantic_search",
        "semantic_search.core",
        "semantic_search.gui",
        "semantic_search.utils",
        "semantic_search.evaluation",
        "PyQt6.QtCore",
        "PyQt6.QtGui",
        "PyQt6.QtWidgets",
        "gensim.models.doc2vec",
        "sklearn.metrics.pairwise",
        "spacy",
        "click",
        "loguru",
        "docx",
        "pymupdf",
    ]

    # Исключения
    excludes = [
        "matplotlib",
        "notebook",
        "jupyter",
        "pytest",
        "mypy",
        "ruff",
    ]

    spec_content = f"""# -*- mode: python ; coding: utf-8 -*-

a = Analysis(
    [r'{main_script}'],
    pathex=[r'{src_path}'],
    binaries=[],
    datas={datas},
    hiddenimports={hidden_imports},
    hookspath=[],
    hooksconfig={{}},
    runtime_hooks=[],
    excludes={excludes},
    noarchive=False,
)

pyz = PYZ(a.pure)

"""

    if onefile:
        spec_content += f"""exe = EXE(
    pyz,
    a.scripts,
    a.binaries,
    a.datas,
    [],
    name='{name}',
    debug=False,
    bootloader_ignore_signals=False,
    strip=False,
    upx=True,
    upx_exclude=[],
    runtime_tmpdir=None,
    console=not {windowed},
    disable_windowed_traceback=False,
    argv_emulation=False,
    target_arch=None,
    codesign_identity=None,
    entitlements_file=None,
)"""
    else:
        spec_content += f"""exe = EXE(
    pyz,
    a.scripts,
    [],
    exclude_binaries=True,
    name='{name}',
    debug=False,
    bootloader_ignore_signals=False,
    strip=False,
    upx=True,
    console=not {windowed},
    disable_windowed_traceback=False,
    argv_emulation=False,
    target_arch=None,
    codesign_identity=None,
    entitlements_file=None,
)

coll = COLLECT(
    exe,
    a.binaries,
    a.datas,
    strip=False,
    upx=True,
    upx_exclude=[],
    name='{name}',
)"""

    spec_file = Path(f"{name}.spec")
    with open(spec_file, "w", encoding="utf-8") as f:
        f.write(spec_content)

    print(f"✓ Создан spec файл: {spec_file}")
    return spec_file


def build_executable(spec_file, clean=True):
    """Запуск PyInstaller для сборки"""
    import subprocess

    cmd = ["pyinstaller"]

    if clean:
        cmd.append("--clean")

    cmd.extend(["--noconfirm", str(spec_file)])

    print("\n🔨 Запуск PyInstaller...")
    print(f"Команда: {' '.join(cmd)}")

    try:
        result = subprocess.run(cmd, check=True)
        print("\n✅ Сборка завершена успешно!")
        return True
    except subprocess.CalledProcessError as e:
        print(f"\n❌ Ошибка сборки: {e}")
        return False


def post_build_actions(name="SemanticSearch", onefile=False):
    """Действия после сборки"""

    if onefile:
        exe_path = Path("dist") / f"{name}.exe"
        if exe_path.exists():
            print(f"\n📦 Исполняемый файл создан: {exe_path}")
            print(f"   Размер: {exe_path.stat().st_size / 1024 / 1024:.1f} МБ")
    else:
        dist_dir = Path("dist") / name
        if dist_dir.exists():
            exe_path = dist_dir / f"{name}.exe"
            if exe_path.exists():
                print(f"\n📦 Приложение собрано в: {dist_dir}")
                print(f"   Исполняемый файл: {exe_path}")

                # Создаем директории для данных
                for dir_name in ["data", "logs", "config"]:
                    (dist_dir / dir_name).mkdir(exist_ok=True)
                print("   ✓ Созданы директории для данных")

                # Копируем README
                readme_src = Path("README.md")
                if readme_src.exists():
                    shutil.copy2(readme_src, dist_dir)
                    print("   ✓ Скопирован README.md")


def main():
    parser = argparse.ArgumentParser(
        description="Сборка Semantic Search в исполняемый файл"
    )
    parser.add_argument("--onefile", action="store_true", help="Собрать в один файл")
    parser.add_argument(
        "--windowed", action="store_true", help="Без консоли (GUI режим)"
    )
    parser.add_argument(
        "--name", default="SemanticSearch", help="Имя исполняемого файла"
    )
    parser.add_argument(
        "--no-clean", action="store_true", help="Не очищать старые сборки"
    )

    args = parser.parse_args()

    print("🚀 Semantic Search Builder")
    print("=" * 50)

    # Проверка PyInstaller
    try:
        import PyInstaller

        print(f"✓ PyInstaller {PyInstaller.__version__} установлен")
    except ImportError:
        print("❌ PyInstaller не установлен!")
        print("Установите: poetry add --group dev pyinstaller")
        sys.exit(1)

    # Очистка старых сборок
    if not args.no_clean:
        clean_build_dirs()

    # Создание spec файла
    spec_file = create_spec_file(
        onefile=args.onefile, windowed=args.windowed, name=args.name
    )

    # Сборка
    if build_executable(spec_file, clean=not args.no_clean):
        post_build_actions(args.name, args.onefile)

        print("\n💡 Подсказки:")
        print("   - Для уменьшения размера используйте UPX")
        print("   - Добавьте иконку в assets/icon.ico")
        print("   - Протестируйте на чистой системе")

        if args.onefile:
            print("\n⚠️  Предупреждение: ")
            print("   Режим onefile может работать медленнее")
            print("   и вызывать ложные срабатывания антивирусов")
    else:
        print("\n❌ Сборка не удалась. Проверьте логи выше.")
        sys.exit(1)


if __name__ == "__main__":
    main()


========================================
FILE: scripts\cache_clear.py
========================================
#!/usr/bin/env python
"""Скрипт для очистки кэша приложения"""

import sys
from pathlib import Path

from semantic_search.config import CACHE_DIR
from semantic_search.utils.cache_manager import CacheManager

# Добавляем путь к src в sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))


def clear_cache():
    """Очистка кэша приложения"""
    print("🧹 Очистка кэша Semantic Search")
    print(f"📁 Директория кэша: {CACHE_DIR}")

    # Проверяем существование директории
    if not CACHE_DIR.exists():
        print("ℹ️  Директория кэша не существует. Нечего очищать.")
        return

    # Подсчитываем размер кэша
    cache_files = list(CACHE_DIR.glob("*.pkl"))
    total_size = sum(f.stat().st_size for f in cache_files)

    if not cache_files:
        print("ℹ️  Кэш пуст. Нечего очищать.")
        return

    print(f"📊 Найдено файлов: {len(cache_files)}")
    print(f"💾 Общий размер: {total_size / 1024 / 1024:.2f} МБ")

    # Запрашиваем подтверждение
    response = input("\n❓ Вы уверены, что хотите очистить кэш? (y/n): ")

    if response.lower() != "y":
        print("❌ Очистка отменена.")
        return

    # Очищаем кэш
    cache_manager = CacheManager(CACHE_DIR)
    if cache_manager.clear():
        print("✅ Кэш успешно очищен.")
        print(f"🗑️  Удалено {len(cache_files)} файлов")
        print(f"💨 Освобождено {total_size / 1024 / 1024:.2f} МБ")
    else:
        print("❌ Произошла ошибка при очистке кэша.")


def main():
    """Главная функция"""
    try:
        clear_cache()
    except KeyboardInterrupt:
        print("\n\n⚠️  Операция прервана пользователем.")
        sys.exit(1)
    except Exception as e:
        print(f"\n❌ Ошибка: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()


========================================
FILE: scripts\check_dependencies.py
========================================
#!/usr/bin/env python
"""Скрипт для проверки установленных зависимостей"""

import importlib
import subprocess
import sys
from pathlib import Path


def check_module(module_name: str, import_name: str = None) -> tuple[bool, str]:
    """Проверка доступности модуля"""
    import_name = import_name or module_name
    try:
        module = importlib.import_module(import_name)
        version = getattr(module, "__version__", "unknown")
        return True, version
    except ImportError:
        return False, None


def check_spacy_models():
    """Проверка установленных моделей SpaCy"""
    models = {
        "ru_core_news_sm": "Русская модель (маленькая)",
        "en_core_web_sm": "Английская модель (маленькая)",
    }

    results = {}
    try:
        import spacy

        for model_name, description in models.items():
            try:
                nlp = spacy.load(model_name)
                results[model_name] = (True, description)
            except OSError:
                results[model_name] = (False, description)
    except ImportError:
        for model_name, description in models.items():
            results[model_name] = (False, description)

    return results


def check_system_dependencies():
    """Проверка системных зависимостей"""
    results = {}

    # Проверка Python версии
    python_version = (
        f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
    )
    results["Python"] = (
        sys.version_info.major == 3 and 10 <= sys.version_info.minor <= 12,
        python_version,
    )

    # Проверка Poetry
    try:
        result = subprocess.run(["poetry", "--version"], capture_output=True, text=True)
        if result.returncode == 0:
            version = result.stdout.strip().split()[-1]
            results["Poetry"] = (True, version)
        else:
            results["Poetry"] = (False, None)
    except FileNotFoundError:
        results["Poetry"] = (False, None)

    # Проверка Microsoft Word (только на Windows)
    if sys.platform == "win32":
        try:
            import win32com.client

            word = win32com.client.Dispatch("Word.Application")
            results["Microsoft Word"] = (True, "installed")
            word.Quit()
        except:
            results["Microsoft Word"] = (False, None)

    return results


def main():
    """Главная функция"""
    print("🔍 Проверка зависимостей Semantic Search")
    print("=" * 60)

    # Основные зависимости
    core_dependencies = [
        ("pymupdf", "fitz", "PDF обработка"),
        ("python-docx", "docx", "DOCX обработка"),
        ("spacy", None, "NLP обработка"),
        ("gensim", None, "Doc2Vec модель"),
        ("PyQt6", "PyQt6.QtCore", "GUI интерфейс"),
        ("loguru", None, "Логирование"),
        ("click", None, "CLI интерфейс"),
        ("scikit-learn", "sklearn", "ML утилиты"),
        ("psutil", None, "Системная информация"),
        ("numpy", None, "Численные вычисления"),
        ("pandas", None, "Обработка данных"),
        ("matplotlib", None, "Графики"),
        ("tqdm", None, "Прогресс-бары"),
    ]

    # Опциональные зависимости
    optional_dependencies = [
        ("openai", None, "OpenAI интеграция"),
        ("seaborn", None, "Улучшенные графики"),
    ]

    # Windows-специфичные
    if sys.platform == "win32":
        core_dependencies.append(("pywin32", "win32com", "DOC обработка"))

    print("\n📦 Основные зависимости:")
    print("-" * 60)

    all_ok = True
    for package_name, import_name, description in core_dependencies:
        installed, version = check_module(package_name, import_name)
        if installed:
            print(f"✅ {package_name:<20} {version:<15} {description}")
        else:
            print(f"❌ {package_name:<20} {'НЕ УСТАНОВЛЕН':<15} {description}")
            all_ok = False

    print("\n📦 Опциональные зависимости:")
    print("-" * 60)

    for package_name, import_name, description in optional_dependencies:
        installed, version = check_module(package_name, import_name)
        if installed:
            print(f"✅ {package_name:<20} {version:<15} {description}")
        else:
            print(f"⚠️  {package_name:<20} {'не установлен':<15} {description}")

    # Проверка SpaCy моделей
    print("\n🧠 SpaCy модели:")
    print("-" * 60)

    spacy_models = check_spacy_models()
    spacy_ok = True
    for model_name, (installed, description) in spacy_models.items():
        if installed:
            print(f"✅ {model_name:<20} {description}")
        else:
            print(f"❌ {model_name:<20} {description} - НЕ УСТАНОВЛЕНА")
            spacy_ok = False

    # Системные зависимости
    print("\n💻 Системные зависимости:")
    print("-" * 60)

    system_deps = check_system_dependencies()
    for dep_name, (installed, version) in system_deps.items():
        if installed:
            print(f"✅ {dep_name:<20} {version}")
        else:
            if dep_name == "Microsoft Word":
                print(f"⚠️  {dep_name:<20} не установлен (требуется для .doc файлов)")
            else:
                print(f"❌ {dep_name:<20} НЕ УСТАНОВЛЕН")
                all_ok = False

    # Итоговый статус
    print("\n" + "=" * 60)

    if all_ok and spacy_ok:
        print("✅ Все зависимости установлены корректно!")
        print("\n🚀 Приложение готово к запуску:")
        print("   GUI: poetry run semantic-search")
        print("   CLI: poetry run semantic-search-cli --help")
    else:
        print("⚠️  Обнаружены проблемы с зависимостями!")

        print("\n📋 Рекомендации по устранению:")

        if not all_ok:
            print("\n1. Установите недостающие зависимости:")
            print("   poetry install")

        if not spacy_ok:
            print("\n2. Установите языковые модели SpaCy:")
            print("   poetry run python scripts/setup_spacy.py")
            print("   или вручную:")
            print("   poetry run python -m spacy download ru_core_news_sm")
            print("   poetry run python -m spacy download en_core_web_sm")

        if not system_deps.get("Poetry", (False, None))[0]:
            print("\n3. Установите Poetry:")
            print("   Инструкции: https://python-poetry.org/docs/#installation")

    # Проверка путей
    print("\n📁 Проверка структуры проекта:")
    print("-" * 60)

    project_root = Path(__file__).parent.parent
    important_paths = [
        (project_root / "src" / "semantic_search", "Исходный код"),
        (project_root / "data", "Директория данных"),
        (project_root / "pyproject.toml", "Конфигурация Poetry"),
    ]

    for path, description in important_paths:
        if path.exists():
            print(f"✅ {str(path.relative_to(project_root)):<30} {description}")
        else:
            print(
                f"❌ {str(path.relative_to(project_root)):<30} {description} - НЕ НАЙДЕН"
            )


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\n❌ Ошибка при проверке: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


========================================
FILE: scripts\cleanup.py
========================================
#!/usr/bin/env python
"""Скрипт для очистки временных файлов и логов"""

import shutil
import sys
from datetime import datetime, timedelta
from pathlib import Path

from semantic_search.config import CACHE_DIR, DATA_DIR, LOGS_DIR, MODELS_DIR, TEMP_DIR

# Добавляем путь к src в sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))


def format_file_size(size_bytes):
    """Форматирование размера файла"""
    for unit in ["B", "KB", "MB", "GB"]:
        if size_bytes < 1024.0:
            return f"{size_bytes:.1f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.1f} TB"


def get_directory_size(directory: Path) -> int:
    """Получить размер директории"""
    total_size = 0
    if directory.exists():
        for file_path in directory.rglob("*"):
            if file_path.is_file():
                total_size += file_path.stat().st_size
    return total_size


def clean_temp_files():
    """Очистка временных файлов"""
    print("\n🗑️  Очистка временных файлов...")

    if not TEMP_DIR.exists():
        print("   ℹ️  Директория temp не найдена")
        return 0

    temp_files = list(TEMP_DIR.rglob("*"))
    temp_size = get_directory_size(TEMP_DIR)

    if not temp_files:
        print("   ✅ Временные файлы отсутствуют")
        return 0

    print(f"   📁 Найдено файлов: {len([f for f in temp_files if f.is_file()])}")
    print(f"   💾 Размер: {format_file_size(temp_size)}")

    try:
        shutil.rmtree(TEMP_DIR)
        TEMP_DIR.mkdir(exist_ok=True)
        print(f"   ✅ Очищено {format_file_size(temp_size)}")
        return temp_size
    except Exception as e:
        print(f"   ❌ Ошибка: {e}")
        return 0


def clean_old_logs(days: int = 7):
    """Очистка старых логов"""
    print(f"\n📜 Очистка логов старше {days} дней...")

    if not LOGS_DIR.exists():
        print("   ℹ️  Директория логов не найдена")
        return 0

    cutoff_date = datetime.now() - timedelta(days=days)
    old_logs = []
    total_size = 0

    for log_file in LOGS_DIR.glob("*.log*"):
        if log_file.is_file():
            modified_time = datetime.fromtimestamp(log_file.stat().st_mtime)
            if modified_time < cutoff_date:
                old_logs.append(log_file)
                total_size += log_file.stat().st_size

    if not old_logs:
        print("   ✅ Старые логи отсутствуют")
        return 0

    print(f"   📁 Найдено старых логов: {len(old_logs)}")
    print(f"   💾 Размер: {format_file_size(total_size)}")

    removed_size = 0
    for log_file in old_logs:
        try:
            size = log_file.stat().st_size
            log_file.unlink()
            removed_size += size
        except Exception as e:
            print(f"   ⚠️  Не удалось удалить {log_file.name}: {e}")

    print(f"   ✅ Очищено {format_file_size(removed_size)}")
    return removed_size


def clean_cache():
    """Очистка кэша"""
    print("\n💾 Очистка кэша...")

    if not CACHE_DIR.exists():
        print("   ℹ️  Директория кэша не найдена")
        return 0

    from semantic_search.utils.cache_manager import CacheManager

    cache_size = get_directory_size(CACHE_DIR)
    cache_files = list(CACHE_DIR.glob("*.pkl"))

    if not cache_files:
        print("   ✅ Кэш пуст")
        return 0

    print(f"   📁 Найдено файлов: {len(cache_files)}")
    print(f"   💾 Размер: {format_file_size(cache_size)}")

    cache_manager = CacheManager(CACHE_DIR)
    if cache_manager.clear():
        print(f"   ✅ Очищено {format_file_size(cache_size)}")
        return cache_size
    else:
        print("   ❌ Ошибка при очистке кэша")
        return 0


def clean_evaluation_results():
    """Очистка результатов оценки"""
    print("\n📊 Очистка результатов оценки...")

    eval_dir = DATA_DIR / "evaluation_results"
    if not eval_dir.exists():
        print("   ℹ️  Директория результатов не найдена")
        return 0

    eval_size = get_directory_size(eval_dir)
    eval_files = list(eval_dir.rglob("*"))

    if not eval_files:
        print("   ✅ Результаты отсутствуют")
        return 0

    print(f"   📁 Найдено файлов: {len([f for f in eval_files if f.is_file()])}")
    print(f"   💾 Размер: {format_file_size(eval_size)}")

    try:
        shutil.rmtree(eval_dir)
        eval_dir.mkdir(exist_ok=True)
        print(f"   ✅ Очищено {format_file_size(eval_size)}")
        return eval_size
    except Exception as e:
        print(f"   ❌ Ошибка: {e}")
        return 0


def show_disk_usage():
    """Показать использование диска"""
    print("\n📊 Использование дискового пространства:")
    print("=" * 50)

    directories = [
        (DATA_DIR, "Данные"),
        (MODELS_DIR, "Модели"),
        (CACHE_DIR, "Кэш"),
        (TEMP_DIR, "Временные файлы"),
        (LOGS_DIR, "Логи"),
        (DATA_DIR / "evaluation_results", "Результаты оценки"),
    ]

    total_size = 0
    for directory, name in directories:
        if directory.exists():
            size = get_directory_size(directory)
            total_size += size
            print(f"{name:.<30} {format_file_size(size):>15}")

    print("-" * 50)
    print(f"{'ИТОГО':.<30} {format_file_size(total_size):>15}")


def main():
    """Главная функция"""
    import argparse

    parser = argparse.ArgumentParser(description="Очистка временных файлов и кэша")
    parser.add_argument(
        "--all",
        "-a",
        action="store_true",
        help="Очистить все (временные файлы, кэш, старые логи)",
    )
    parser.add_argument(
        "--temp", "-t", action="store_true", help="Очистить только временные файлы"
    )
    parser.add_argument(
        "--cache", "-c", action="store_true", help="Очистить только кэш"
    )
    parser.add_argument(
        "--logs", "-l", action="store_true", help="Очистить старые логи"
    )
    parser.add_argument(
        "--eval", "-e", action="store_true", help="Очистить результаты оценки"
    )
    parser.add_argument(
        "--days",
        type=int,
        default=7,
        help="Удалять логи старше указанного количества дней (по умолчанию: 7)",
    )
    parser.add_argument(
        "--usage", "-u", action="store_true", help="Показать использование диска"
    )
    parser.add_argument(
        "--yes", "-y", action="store_true", help="Не запрашивать подтверждение"
    )

    args = parser.parse_args()

    # Если не указаны флаги, показываем использование диска
    if not any([args.all, args.temp, args.cache, args.logs, args.eval, args.usage]):
        args.usage = True

    print("🧹 Утилита очистки Semantic Search")
    print("=" * 50)

    if args.usage:
        show_disk_usage()
        return

    # Определяем что будем очищать
    actions = []
    if args.all or args.temp:
        actions.append(("временные файлы", clean_temp_files))
    if args.all or args.cache:
        actions.append(("кэш", clean_cache))
    if args.all or args.logs:
        actions.append(
            (f"логи старше {args.days} дней", lambda: clean_old_logs(args.days))
        )
    if args.eval:
        actions.append(("результаты оценки", clean_evaluation_results))

    # Показываем что будет очищено
    print("\n🎯 Будут очищены:")
    for name, _ in actions:
        print(f"   • {name}")

    # Запрашиваем подтверждение
    if not args.yes:
        response = input("\n❓ Продолжить? (y/n): ")
        if response.lower() != "y":
            print("❌ Очистка отменена.")
            return

    # Выполняем очистку
    total_cleaned = 0
    for name, action in actions:
        cleaned = action()
        total_cleaned += cleaned

    print(f"\n✅ Всего очищено: {format_file_size(total_cleaned)}")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n⚠️  Операция прервана.")
        sys.exit(1)
    except Exception as e:
        print(f"\n❌ Ошибка: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


========================================
FILE: scripts\demo_evaluation.py
========================================
"""Демонстрационный скрипт для оценки и сравнения методов поиска"""

import os
from pathlib import Path

from semantic_search.core.doc2vec_trainer import Doc2VecTrainer
from semantic_search.core.search_engine import SemanticSearchEngine
from semantic_search.evaluation.baselines import (
    Doc2VecSearchAdapter,
    OpenAISearchBaseline,
)
from semantic_search.evaluation.comparison import QueryTestCase, SearchComparison


def create_demo_test_cases():
    """Создание демонстрационных тестовых случаев"""

    test_cases = [
        QueryTestCase(
            query="Понятие глокализации в современной лингвистике",
            relevant_docs={
                "Транслигвизм/-1.pdf",
                "SALMAN RUSHDIE/Language is assumed by many to be a stable medium of communication.docx",
                "Глобализация и глокализация/glokalizatsiya-i-vozvrat-etnichnosti-v-vek-globalizatsii.pdf",
            },
            relevance_scores={
                "Транслигвизм/-1.pdf": 3,
                "SALMAN RUSHDIE/Language is assumed by many to be a stable medium of communication.docx": 3,
                "Глобализация и глокализация/glokalizatsiya-i-vozvrat-etnichnosti-v-vek-globalizatsii.pdf": 2,
                "Лингвокреативность/Linguistic_Creativity_Cognitive_And_Communicative_.pdf": 1,
            },
            description="Понятие глокализации в современной лингвистике",
        ),
        QueryTestCase(
            query="Транслингвизм и транслингвальная литература",
            relevant_docs={
                "Транслигвизм/-1.pdf",
                "SALMAN RUSHDIE/rushdie-1997-notes-on-writing-and-the-nation.pdf",
                "glocal_strategy.pdf",
            },
            relevance_scores={
                "Транслигвизм/-1.pdf": 3,
                "SALMAN RUSHDIE/rushdie-1997-notes-on-writing-and-the-nation.pdf": 3,
                "SALMAN RUSHDIE/Hybridization_Heteroglossia_and_the_engl.doc": 2,
            },
            description="Транслингвизм и транслингвальная литература",
        ),
        QueryTestCase(
            query="Гетерология и диалогизм",
            relevant_docs={"cultural_marketing.pdf", "cross_cultural_comm.pdf"},
            relevance_scores={
                "Транслигвизм/-1.pdf": 3,
                "SALMAN RUSHDIE/12.docx": 3,
                " Бахтин/Zebroski-MikhailBakhtinQuestion-1992.pdf": 1,
            },
            description="Гетерология и диалогизм",
        ),
    ]

    return test_cases


def main():
    """Основная функция демонстрации"""
    print("=" * 80)
    print("ДЕМОНСТРАЦИЯ СРАВНЕНИЯ DOC2VEC И OPENAI EMBEDDINGS")
    print("=" * 80)

    # Проверка API ключа
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("\n❌ ОШИБКА: OpenAI API key не найден!")
        print("Установите переменную окружения OPENAI_API_KEY")
        print("Например: set OPENAI_API_KEY=sk-...")
        return

    # Загрузка модели Doc2Vec
    print("\n📂 Загрузка модели Doc2Vec...")
    model_name = "doc2vec_model"  # Замените на имя вашей модели

    trainer = Doc2VecTrainer()
    model = trainer.load_model(model_name)

    if not model:
        print(f"❌ Не удалось загрузить модель '{model_name}'")
        print("Сначала обучите модель командой:")
        print("poetry run semantic-search-cli train -d /path/to/documents")
        return

    print(f"✅ Модель загружена: {len(model.dv)} документов")

    # Создание поискового движка
    search_engine = SemanticSearchEngine(model, trainer.corpus_info)

    # Создание тестовых случаев
    print("\n🧪 Подготовка тестовых случаев...")
    test_cases = create_demo_test_cases()
    print(f"   Создано {len(test_cases)} тестовых запросов")

    # Создание объекта сравнения
    comparison = SearchComparison(test_cases)

    # Создание адаптеров методов
    print("\n🔧 Инициализация методов поиска...")

    # Doc2Vec адаптер
    doc2vec_adapter = Doc2VecSearchAdapter(search_engine, trainer.corpus_info)
    print("✅ Doc2Vec адаптер готов")

    # OpenAI baseline
    try:
        openai_baseline = OpenAISearchBaseline(api_key=api_key)
        print("✅ OpenAI baseline инициализирован")
    except Exception as e:
        print(f"❌ Ошибка инициализации OpenAI: {e}")
        return

    # Подготовка документов для OpenAI
    print("\n📚 Индексация документов для OpenAI...")
    print("   (Для демонстрации используем только первые 20 документов)")

    # Берем подвыборку документов
    demo_documents = []
    for i, (tokens, doc_id, metadata) in enumerate(trainer.corpus_info[:20]):
        # Восстанавливаем текст из токенов
        text = " ".join(tokens[:300])  # Первые 300 токенов
        demo_documents.append((doc_id, text, metadata))

    try:
        openai_baseline.index(demo_documents)
        print(f"✅ Проиндексировано {len(demo_documents)} документов")
    except Exception as e:
        print(f"❌ Ошибка индексации: {e}")
        return

    # Оценка методов
    print("\n📊 ОЦЕНКА МЕТОДОВ")
    print("-" * 80)

    # Doc2Vec
    print("\n1️⃣ Оценка Doc2Vec...")
    doc2vec_results = comparison.evaluate_method(
        doc2vec_adapter, top_k=10, verbose=True
    )

    # OpenAI
    print("\n2️⃣ Оценка OpenAI embeddings...")
    openai_results = comparison.evaluate_method(openai_baseline, top_k=10, verbose=True)

    # Результаты
    print("\n📈 РЕЗУЛЬТАТЫ СРАВНЕНИЯ")
    print("=" * 80)

    # Создаем сравнительную таблицу
    df_comparison = comparison.compare_methods(
        [doc2vec_adapter, openai_baseline], save_results=True
    )

    print("\nСравнительная таблица:")
    print(df_comparison.to_string(index=False))

    # Основные выводы
    doc2vec_map = doc2vec_results["aggregated"]["MAP"]
    openai_map = openai_results["aggregated"]["MAP"]

    print("\n🎯 ОСНОВНЫЕ ВЫВОДЫ:")
    print("-" * 80)

    if doc2vec_map > openai_map:
        improvement = ((doc2vec_map - openai_map) / openai_map) * 100
        print(f"✅ Doc2Vec превосходит OpenAI по качеству поиска на {improvement:.1f}%")
    else:
        improvement = ((openai_map - doc2vec_map) / doc2vec_map) * 100
        print(f"❌ OpenAI превосходит Doc2Vec по качеству поиска на {improvement:.1f}%")

    # Скорость
    doc2vec_time = doc2vec_results["aggregated"]["avg_query_time"]
    openai_time = openai_results["aggregated"]["avg_query_time"]
    speed_ratio = openai_time / doc2vec_time

    print(f"\n✅ Doc2Vec работает в {speed_ratio:.1f} раз быстрее OpenAI")
    print(f"   Doc2Vec: {doc2vec_time:.3f}с на запрос")
    print(f"   OpenAI:  {openai_time:.3f}с на запрос")

    # Экономическая эффективность
    print("\n💰 Экономическая эффективность:")
    yearly_cost = 1000 * 50 / 1000 * 0.0001 * 365  # Примерный расчет
    print(f"   При 1000 запросов в день экономия составит ~${yearly_cost:.0f} в год")

    # Генерация графиков
    print("\n📊 Генерация графиков...")
    try:
        comparison.plot_comparison(save_plots=True)
        print("✅ Графики сохранены в data/evaluation_results/plots/")
    except Exception as e:
        print(f"⚠️ Не удалось создать графики: {e}")

    # Детальный отчет
    print("\n📄 Генерация детального отчета...")
    report_path = Path("data/evaluation_results/comparison_report.txt")
    report = comparison.generate_report(report_path)

    print("\n✅ Все результаты сохранены в: data/evaluation_results/")
    print("\n" + "=" * 80)


if __name__ == "__main__":
    main()


========================================
FILE: scripts\export_project.py
========================================
#!/usr/bin/env python
"""Скрипт для экспорта всего проекта в один текстовый файл для проверки"""

import os
from datetime import datetime
from pathlib import Path


def should_include_file(file_path: Path, output_file: str) -> bool:
    """Определяет, нужно ли включать файл"""

    # ВАЖНО: Исключаем сам файл экспорта!
    if file_path.name == output_file or file_path.name == "project_export.txt":
        return False

    # Исключаем временные файлы экспорта
    if file_path.name.startswith("project_export") and file_path.suffix == ".txt":
        return False

    # Исключаем директории
    exclude_dirs = {
        "__pycache__",
        ".git",
        ".venv",
        "venv",
        ".env",
        "dist",
        "build",
        ".pytest_cache",
        ".mypy_cache",
        "node_modules",
        ".idea",
        ".vscode",
        "logs",
        "data/models",
        "data/cache",
        "data/temp",
        ".ruff_cache",
        "htmlcov",
        ".coverage",
    }

    # Исключаем расширения
    exclude_extensions = {
        ".pyc",
        ".pyo",
        ".pyd",
        ".so",
        ".dll",
        ".dylib",
        ".exe",
        ".bin",
        ".pkl",
        ".npy",
        ".model",
        ".log",
        ".tmp",
        ".cache",
        ".lock",
        ".db",
        ".zip",
        ".tar",
        ".gz",
        ".rar",
        ".7z",
    }

    # Проверяем, не находится ли файл в исключенной директории
    for parent in file_path.parents:
        if parent.name in exclude_dirs:
            return False

    # Проверяем расширение
    if file_path.suffix in exclude_extensions:
        return False

    # Исключаем большие файлы (больше 1MB)
    try:
        if file_path.is_file() and file_path.stat().st_size > 1_000_000:
            return False
    except:
        return False

    return True


def collect_project_files(root_path: Path, output_file: str) -> list[Path]:
    """Собирает все файлы проекта, которые нужно включить"""
    included_files = []
    seen_paths = set()  # Для отслеживания уже добавленных файлов

    # Определяем приоритет файлов
    priority_order = [
        "pyproject.toml",
        "README.md",
        "LICENSE",
        ".gitignore",
        "src/semantic_search/__init__.py",
        "src/semantic_search/config.py",
        "src/semantic_search/main.py",
        "src/semantic_search/core",
        "src/semantic_search/gui",
        "src/semantic_search/utils",
        "src/semantic_search/evaluation",
        "scripts",
        "tests",
    ]

    # Рекурсивно обходим директории
    for path in root_path.rglob("*"):
        # Преобразуем в абсолютный путь для сравнения
        abs_path = path.absolute()

        # Проверяем, не добавляли ли уже этот файл
        if abs_path in seen_paths:
            continue

        if path.is_file() and should_include_file(path, output_file):
            included_files.append(path)
            seen_paths.add(abs_path)

    # Сортируем файлы по приоритету
    def get_sort_key(file_path: Path):
        rel_path = str(file_path.relative_to(root_path)).replace("\\", "/")

        # Ищем приоритет
        for i, priority in enumerate(priority_order):
            if rel_path == priority or rel_path.startswith(priority + "/"):
                return (i, rel_path)

        # Остальные файлы в конец
        return (len(priority_order), rel_path)

    included_files.sort(key=get_sort_key)

    # Удаляем дубликаты (на всякий случай)
    unique_files = []
    seen_contents = set()

    for file_path in included_files:
        file_key = (file_path.name, file_path.stat().st_size)
        if file_key not in seen_contents:
            unique_files.append(file_path)
            seen_contents.add(file_key)

    return unique_files


def export_project(output_file: str = "project_export.txt", include_data: bool = False):
    """Экспортирует проект в текстовый файл"""
    project_root = Path(__file__).parent.parent

    # Удаляем старый файл экспорта если существует
    output_path = Path(output_file)
    if output_path.exists():
        print(f"🗑️  Удаляем старый файл экспорта: {output_file}")
        output_path.unlink()

    # Собираем файлы
    print("📂 Сканирование проекта...")
    all_files = collect_project_files(project_root, output_file)

    # Фильтруем data файлы если не нужны
    if not include_data:
        all_files = [
            f for f in all_files if "data" not in f.parts or f.name == ".gitkeep"
        ]

    print(f"📄 Найдено файлов для экспорта: {len(all_files)}")

    # Проверяем на дубликаты
    file_names = [f.name for f in all_files]
    duplicates = [name for name in file_names if file_names.count(name) > 1]
    if duplicates:
        print(f"⚠️  Обнаружены файлы с одинаковыми именами: {set(duplicates)}")

    # Записываем в файл
    with open(output_file, "w", encoding="utf-8") as f:
        # Заголовок
        f.write("=" * 80 + "\n")
        f.write("SEMANTIC SEARCH PROJECT EXPORT\n")
        f.write(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Root: {project_root}\n")
        f.write("=" * 80 + "\n\n")

        # Структура проекта (простой список)
        f.write("PROJECT STRUCTURE:\n")
        f.write("-" * 40 + "\n")

        current_dir = None
        for file_path in all_files:
            rel_path = file_path.relative_to(project_root)
            parent_dir = rel_path.parent

            # Показываем директорию при смене
            if parent_dir != current_dir:
                current_dir = parent_dir
                if str(parent_dir) != ".":
                    f.write(f"\n{parent_dir}/\n")
                else:
                    f.write("\n[root]\n")

            f.write(f"  - {file_path.name}\n")

        # Содержимое файлов
        f.write("\n" + "=" * 80 + "\n")
        f.write("FILE CONTENTS:\n")
        f.write("=" * 80 + "\n")

        # Записываем содержимое каждого файла
        written_files = set()  # Отслеживаем записанные файлы

        for file_path in all_files:
            # Создаем уникальный ключ для файла
            file_key = str(file_path.absolute())

            # Проверяем, не записали ли уже
            if file_key in written_files:
                print(f"⚠️  Пропускаем дубликат: {file_path.relative_to(project_root)}")
                continue

            written_files.add(file_key)
            rel_path = file_path.relative_to(project_root)

            f.write(f"\n\n{'=' * 40}\n")
            f.write(f"FILE: {rel_path}\n")
            f.write(f"{'=' * 40}\n")

            try:
                # Текстовые файлы
                if file_path.suffix in [
                    ".py",
                    ".toml",
                    ".md",
                    ".txt",
                    ".json",
                    ".yaml",
                    ".yml",
                    ".bat",
                    ".sh",
                    ".cfg",
                    ".ini",
                ]:
                    content = file_path.read_text(encoding="utf-8", errors="ignore")
                    f.write(content)
                    if not content.endswith("\n"):
                        f.write("\n")
                else:
                    # Бинарные файлы
                    f.write(f"[Binary file - {file_path.stat().st_size} bytes]\n")

            except Exception as e:
                f.write(f"[Error reading file: {e}]\n")

        # Итоговая статистика
        f.write("\n" + "=" * 80 + "\n")
        f.write("EXPORT SUMMARY:\n")
        f.write("-" * 40 + "\n")
        f.write(f"Total files included: {len(written_files)}\n")
        f.write(f"Export file: {output_file}\n")
        f.write(f"Export size: {os.path.getsize(output_file):,} bytes\n")
        f.write("=" * 80 + "\n")

    # Результат
    file_size_mb = os.path.getsize(output_file) / 1024 / 1024
    print(f"✅ Проект экспортирован в: {output_file}")
    print(f"📊 Размер файла: {file_size_mb:.2f} MB")
    print(f"📁 Включено файлов: {len(written_files)}")

    if file_size_mb > 10:
        print("\n⚠️  Файл больше 10 MB, может быть слишком большим для некоторых чатов")
        print("💡 Попробуйте исключить некоторые файлы или использовать архив")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Экспорт проекта в текстовый файл для проверки"
    )
    parser.add_argument(
        "-o",
        "--output",
        default="project_export.txt",
        help="Имя выходного файла (по умолчанию: project_export.txt)",
    )
    parser.add_argument(
        "--include-data", action="store_true", help="Включить файлы из директории data/"
    )

    args = parser.parse_args()

    export_project(args.output, args.include_data)


========================================
FILE: scripts\inspect_corpus.py
========================================
"""scripts/inspect_corpus.py - Скрипт для изучения документов в модели"""

import sys
from pathlib import Path

from loguru import logger

from semantic_search.core.doc2vec_trainer import Doc2VecTrainer


def inspect_corpus(model_name: str = "doc2vec_model"):
    """Показать информацию о документах в модели"""

    trainer = Doc2VecTrainer()
    model = trainer.load_model(model_name)

    if not model:
        logger.error(f"Не удалось загрузить модель {model_name}")
        return

    if not trainer.corpus_info:
        logger.error("Информация о корпусе не найдена")
        return

    print(f"\n📚 ДОКУМЕНТЫ В МОДЕЛИ '{model_name}':")
    print("=" * 80)
    print(f"Всего документов: {len(trainer.corpus_info)}\n")

    # Группируем по расширениям
    by_extension = {}
    for tokens, doc_id, metadata in trainer.corpus_info:
        ext = Path(doc_id).suffix
        if ext not in by_extension:
            by_extension[ext] = []
        by_extension[ext].append(doc_id)

    # Показываем статистику по типам
    print("📊 По типам файлов:")
    for ext, docs in sorted(by_extension.items()):
        print(f"  {ext}: {len(docs)} файлов")

    # Показываем все документы
    print("\n📄 Список документов:")
    for i, (tokens, doc_id, metadata) in enumerate(trainer.corpus_info, 1):
        tokens_count = metadata.get("tokens_count", len(tokens))
        print(f"{i:3d}. {doc_id:<50} ({tokens_count} токенов)")

    # Примеры для создания тестов
    print("\n💡 Примеры для создания тестовых случаев:")
    print("relevant_docs = {")
    for i, (_, doc_id, _) in enumerate(trainer.corpus_info[:5]):
        print(f'    "{doc_id}",')
    print("}")

    print("\n✅ Используйте эти имена файлов в QueryTestCase!")


if __name__ == "__main__":
    import sys

    model_name = sys.argv[1] if len(sys.argv) > 1 else "doc2vec_model"
    inspect_corpus(model_name)


========================================
FILE: scripts\list_models.py
========================================
#!/usr/bin/env python
"""Скрипт для просмотра доступных моделей"""

import json
import sys
from datetime import datetime
from pathlib import Path

from semantic_search.config import MODELS_DIR
from semantic_search.core.doc2vec_trainer import Doc2VecTrainer

# Добавляем путь к src в sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))


def format_file_size(size_bytes):
    """Форматирование размера файла"""
    for unit in ["B", "KB", "MB", "GB"]:
        if size_bytes < 1024.0:
            return f"{size_bytes:.1f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.1f} TB"


def list_models():
    """Список всех доступных моделей"""
    print("📚 Доступные модели Doc2Vec")
    print("=" * 80)

    if not MODELS_DIR.exists():
        print("❌ Директория моделей не найдена.")
        return

    model_files = list(MODELS_DIR.glob("*.model"))

    if not model_files:
        print("ℹ️  Модели не найдены.")
        print(f"   Директория моделей: {MODELS_DIR}")
        print("\n💡 Создайте модель командой:")
        print("   poetry run semantic-search-cli train -d /path/to/documents")
        return

    print(f"📁 Директория моделей: {MODELS_DIR}")
    print(f"🔢 Найдено моделей: {len(model_files)}\n")

    # Сортируем по дате изменения (новые первые)
    model_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)

    for i, model_file in enumerate(model_files, 1):
        model_name = model_file.stem
        file_size = model_file.stat().st_size
        modified_time = datetime.fromtimestamp(model_file.stat().st_mtime)

        print(f"{i}. 🧠 {model_name}")
        print(f"   📏 Размер: {format_file_size(file_size)}")
        print(f"   📅 Изменен: {modified_time.strftime('%Y-%m-%d %H:%M:%S')}")

        # Пытаемся загрузить метаданные
        metadata_file = MODELS_DIR / f"{model_name}_metadata.json"
        if metadata_file.exists():
            try:
                with open(metadata_file, "r", encoding="utf-8") as f:
                    metadata = json.load(f)

                if "corpus_size" in metadata:
                    print(f"   📄 Документов: {metadata['corpus_size']}")
                if "vector_size" in metadata:
                    print(f"   🎯 Размерность: {metadata['vector_size']}")
                if "epochs" in metadata:
                    print(f"   🔄 Эпох: {metadata['epochs']}")
                if "training_time_formatted" in metadata:
                    print(
                        f"   ⏱️  Время обучения: {metadata['training_time_formatted']}"
                    )
                if "documents_base_path" in metadata:
                    base_path = Path(metadata["documents_base_path"])
                    print(f"   📂 База документов: {base_path.name}")

            except Exception as e:
                print(f"   ⚠️  Не удалось загрузить метаданные: {e}")

        # Проверяем наличие corpus_info
        corpus_info_file = MODELS_DIR / f"{model_name}_corpus_info.pkl"
        if corpus_info_file.exists():
            corpus_size = format_file_size(corpus_info_file.stat().st_size)
            print(f"   💾 Corpus info: {corpus_size}")

        print()  # Пустая строка между моделями


def show_model_details(model_name: str):
    """Показать детальную информацию о модели"""
    print(f"\n📋 Детальная информация о модели: {model_name}")
    print("=" * 80)

    trainer = Doc2VecTrainer()
    model = trainer.load_model(model_name)

    if model is None:
        print(f"❌ Не удалось загрузить модель '{model_name}'")
        return

    info = trainer.get_model_info()

    print(f"✅ Статус: {info['status']}")
    print(f"📏 Размерность векторов: {info['vector_size']}")
    print(f"📚 Размер словаря: {info['vocabulary_size']:,} слов")
    print(f"📄 Документов в модели: {info['documents_count']}")
    print(f"🔍 Размер окна: {info['window']}")
    print(f"📊 Минимальная частота: {info['min_count']}")
    print(f"🔄 Эпох обучения: {info['epochs']}")

    if info["dm"] == 1:
        print("🔧 Режим: Distributed Memory (DM)")
    else:
        print("🔧 Режим: Distributed Bag of Words (DBOW)")

    print(f"➖ Negative sampling: {info['negative']}")
    print(f"🌳 Hierarchical Softmax: {'Да' if info['hs'] == 1 else 'Нет'}")
    print(f"📉 Sample threshold: {info['sample']}")

    if "training_time_formatted" in info:
        print(f"\n⏱️  Время обучения: {info['training_time_formatted']}")
    if "training_date" in info:
        print(f"📅 Дата обучения: {info['training_date']}")

    # Показываем примеры документов
    if trainer.corpus_info:
        print("\n📑 Примеры документов в модели:")
        for i, (tokens, doc_id, metadata) in enumerate(trainer.corpus_info[:5]):
            print(f"   {i + 1}. {doc_id}")
            if "tokens_count" in metadata:
                print(f"      Токенов: {metadata['tokens_count']}")


def main():
    """Главная функция"""
    import argparse

    parser = argparse.ArgumentParser(description="Управление моделями Doc2Vec")
    parser.add_argument(
        "--details", "-d", help="Показать детальную информацию о модели"
    )

    args = parser.parse_args()

    if args.details:
        show_model_details(args.details)
    else:
        list_models()


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n⚠️  Операция прервана.")
        sys.exit(1)
    except Exception as e:
        print(f"\n❌ Ошибка: {e}")
        sys.exit(1)


========================================
FILE: scripts\print_project_tree.py
========================================
import fnmatch
import os
from pathlib import Path


def print_tree(start_path, prefix="", ignore=None, leaf_dirs=None):
    if ignore is None:
        ignore = set()
    if leaf_dirs is None:
        leaf_dirs = {"cache", "models", "logs"}

    # Собираем содержимое директории с учетом игнорирования
    contents = []
    for item in os.listdir(start_path):
        skip = False
        # Проверяем шаблоны игнорирования
        for pattern in ignore:
            if fnmatch.fnmatch(item, pattern):
                skip = True
                break
        if skip:
            continue
        contents.append(item)

    # Сортируем для последовательного отображения
    contents.sort()

    for i, item in enumerate(contents):
        path = Path(start_path) / item
        is_last = i == len(contents) - 1

        # Добавляем '/' к директориям
        display_name = f"{item}/" if os.path.isdir(path) else item
        print(f"{prefix}{'└── ' if is_last else '├── '}{display_name}")

        # Рекурсивный обход только для НЕлистовых директорий
        if os.path.isdir(path) and item not in leaf_dirs:
            ext = "    " if is_last else "│   "
            print_tree(path, prefix + ext, ignore, leaf_dirs)


if __name__ == "__main__":
    print(".")
    ignore_set = {
        "__pycache__",
        ".git",
        ".venv",
        "*.pyc",
        ".pytest_cache",
        "tree.py",
        ".benchmarks",
        ".coverage",
        "*__pycache__*",
        "poetry.lock",
        "__init__.py",
        ".gitignore",
    }
    print_tree(".", ignore=ignore_set)


========================================
FILE: scripts\run_semantic_search.bat
========================================
@echo off
REM Батник для запуска Semantic Search на Windows

echo ====================================
echo    Semantic Search Launcher
echo ====================================
echo.

REM Проверяем наличие Poetry
where poetry >nul 2>nul
if %errorlevel% neq 0 (
    echo [ERROR] Poetry не найден в PATH!
    echo.
    echo Установите Poetry: https://python-poetry.org/docs/#installation
    echo.
    pause
    exit /b 1
)

REM Проверяем наличие виртуального окружения
if not exist ".venv" (
    echo [INFO] Создание виртуального окружения...
    poetry install
    if %errorlevel% neq 0 (
        echo [ERROR] Ошибка установки зависимостей!
        pause
        exit /b 1
    )
)

REM Меню выбора
:menu
echo Выберите режим запуска:
echo.
echo 1. GUI приложение
echo 2. CLI интерфейс
echo 3. Проверка зависимостей
echo 4. Список моделей
echo 5. Очистка кэша
echo 6. Выход
echo.

set /p choice="Введите номер (1-6): "

if "%choice%"=="1" goto gui
if "%choice%"=="2" goto cli
if "%choice%"=="3" goto check
if "%choice%"=="4" goto models
if "%choice%"=="5" goto cache
if "%choice%"=="6" goto end

echo Неверный выбор!
echo.
goto menu

:gui
echo.
echo Запуск GUI приложения...
poetry run semantic-search
goto end

:cli
echo.
echo Запуск CLI интерфейса...
echo.
poetry run semantic-search-cli --help
echo.
pause
goto menu

:check
echo.
echo Проверка зависимостей...
echo.
poetry run python scripts/check_dependencies.py
echo.
pause
goto menu

:models
echo.
echo Список доступных моделей...
echo.
poetry run python scripts/list_models.py
echo.
pause
goto menu

:cache
echo.
echo Очистка кэша...
echo.
poetry run python scripts/cache_clear.py
echo.
pause
goto menu

:end
echo.
echo Спасибо за использование Semantic Search!
timeout /t 3 >nul


========================================
FILE: scripts\run_semantic_search.sh
========================================
#!/bin/bash
# Скрипт для запуска Semantic Search на Linux/Mac

echo "===================================="
echo "    Semantic Search Launcher"
echo "===================================="
echo

# Цвета для вывода
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Проверяем наличие Poetry
if ! command -v poetry &> /dev/null; then
    echo -e "${RED}[ERROR]${NC} Poetry не найден!"
    echo
    echo "Установите Poetry: https://python-poetry.org/docs/#installation"
    echo
    exit 1
fi

# Проверяем/создаем виртуальное окружение
if [ ! -d ".venv" ]; then
    echo -e "${YELLOW}[INFO]${NC} Создание виртуального окружения..."
    poetry install
    if [ $? -ne 0 ]; then
        echo -e "${RED}[ERROR]${NC} Ошибка установки зависимостей!"
        exit 1
    fi
fi

# Функция для показа меню
show_menu() {
    echo
    echo "Выберите режим запуска:"
    echo
    echo "1. GUI приложение"
    echo "2. CLI интерфейс"
    echo "3. Проверка зависимостей"
    echo "4. Список моделей"
    echo "5. Очистка кэша"
    echo "6. Выход"
    echo
}

# Основной цикл
while true; do
    show_menu
    read -p "Введите номер (1-6): " choice
    
    case $choice in
        1)
            echo
            echo -e "${GREEN}Запуск GUI приложения...${NC}"
            poetry run semantic-search
            break
            ;;
        2)
            echo
            echo -e "${GREEN}Запуск CLI интерфейса...${NC}"
            echo
            poetry run semantic-search-cli --help
            echo
            read -p "Нажмите Enter для продолжения..."
            ;;
        3)
            echo
            echo -e "${GREEN}Проверка зависимостей...${NC}"
            echo
            poetry run python scripts/check_dependencies.py
            echo
            read -p "Нажмите Enter для продолжения..."
            ;;
        4)
            echo
            echo -e "${GREEN}Список доступных моделей...${NC}"
            echo
            poetry run python scripts/list_models.py
            echo
            read -p "Нажмите Enter для продолжения..."
            ;;
        5)
            echo
            echo -e "${GREEN}Очистка кэша...${NC}"
            echo
            poetry run python scripts/cache_clear.py
            echo
            read -p "Нажмите Enter для продолжения..."
            ;;
        6)
            echo
            echo -e "${GREEN}Спасибо за использование Semantic Search!${NC}"
            exit 0
            ;;
        *)
            echo -e "${RED}Неверный выбор!${NC}"
            ;;
    esac
done


========================================
FILE: scripts\setup_spacy.py
========================================
# Обновите файл scripts/setup_spacy.py:

"""Скрипт для установки и проверки SpaCy моделей"""

import subprocess
import sys

import click
import spacy
from loguru import logger


def check_spacy_model(model_name: str) -> bool:
    """Проверка наличия модели SpaCy"""
    try:
        spacy.load(model_name)
        return True
    except OSError:
        return False


def download_spacy_model(model_name: str) -> bool:
    """Загрузка модели SpaCy"""
    try:
        logger.info(f"Загрузка модели {model_name}...")
        subprocess.check_call(
            [sys.executable, "-m", "spacy", "download", model_name],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
        )
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"Ошибка при загрузке модели: {e}")
        return False


@click.command()
@click.option(
    "--russian/--no-russian",
    default=True,
    help="Установить русскую модель",
)
@click.option(
    "--english/--no-english",
    default=True,
    help="Установить английскую модель",
)
def main(russian: bool, english: bool):
    """Установка и проверка SpaCy моделей для русского и английского языков"""

    logger.info("Проверка установки SpaCy...")

    try:
        import spacy

        logger.info(f"SpaCy версии {spacy.__version__} установлен")
    except ImportError:
        logger.error("SpaCy не установлен!")
        logger.info("Установите SpaCy командой: pip install spacy")
        sys.exit(1)

    models_to_install = []

    if russian:
        models_to_install.append(("ru_core_news_sm", "Русская"))
    if english:
        models_to_install.append(("en_core_web_sm", "Английская"))

    if not models_to_install:
        logger.warning("Не выбрана ни одна модель для установки")
        return

    installed_count = 0

    for model_name, model_desc in models_to_install:
        logger.info(f"\n{'=' * 50}")
        logger.info(f"Проверка {model_desc} модели ({model_name})...")

        if check_spacy_model(model_name):
            logger.success(f"{model_desc} модель уже установлена!")

            # Проверяем работу модели
            try:
                nlp = spacy.load(model_name)
                test_text = (
                    "Это тестовое предложение."
                    if "ru" in model_name
                    else "This is a test sentence."
                )
                doc = nlp(test_text)
                logger.info(f"Модель работает корректно. Токенов: {len(doc)}")
                logger.info(f"Язык: {nlp.lang}")
                logger.info(f"Компоненты: {nlp.pipe_names}")
                installed_count += 1

            except Exception as e:
                logger.error(f"Ошибка при тестировании модели: {e}")

        else:
            logger.warning(f"{model_desc} модель не найдена")

            response = click.confirm(
                f"Хотите загрузить {model_desc} модель?", default=True
            )
            if response:
                if download_spacy_model(model_name):
                    logger.success(f"{model_desc} модель успешно загружена!")

                    # Проверяем после загрузки
                    if check_spacy_model(model_name):
                        logger.success("Модель готова к использованию!")
                        installed_count += 1
                    else:
                        logger.error("Модель загружена, но не может быть загружена")
                else:
                    logger.error("Не удалось загрузить модель")
                    logger.info(
                        f"Попробуйте загрузить вручную: python -m spacy download {model_name}"
                    )
            else:
                logger.info("Загрузка отменена")

    # Итоговая информация
    logger.info(f"\n{'=' * 50}")
    logger.info("ИТОГИ УСТАНОВКИ:")
    logger.info(f"Установлено моделей: {installed_count} из {len(models_to_install)}")

    if installed_count == len(models_to_install):
        logger.success("✅ Все модели успешно установлены!")
    elif installed_count > 0:
        logger.warning("⚠️ Установлены не все модели")
    else:
        logger.error("❌ Ни одна модель не установлена")

    # Дополнительная информация
    logger.info("\n📚 Доступные модели SpaCy:")
    logger.info("Русские:")
    logger.info("  - ru_core_news_sm (маленькая, ~15 МБ)")
    logger.info("  - ru_core_news_md (средняя, ~45 МБ)")
    logger.info("  - ru_core_news_lg (большая, ~500 МБ)")
    logger.info("\nАнглийские:")
    logger.info("  - en_core_web_sm (маленькая, ~15 МБ)")
    logger.info("  - en_core_web_md (средняя, ~40 МБ)")
    logger.info("  - en_core_web_lg (большая, ~400 МБ)")
    logger.info("  - en_core_web_trf (transformer, ~450 МБ)")


if __name__ == "__main__":
    main()


========================================
FILE: tests\__init__.py
========================================



========================================
FILE: tests\test_core_functionality.py
========================================
"""Тесты основной функциональности (ИСПРАВЛЕННАЯ ВЕРСИЯ)"""

import tempfile
from pathlib import Path
from unittest.mock import Mock, patch

import pytest

from semantic_search.core.doc2vec_trainer import Doc2VecTrainer
from semantic_search.core.document_processor import DocumentProcessor
from semantic_search.core.search_engine import SemanticSearchEngine
from semantic_search.utils.validators import DataValidator, ValidationError


class TestDocumentProcessor:
    """Тесты обработчика документов"""

    def test_processor_initialization(self):
        """Тест инициализации процессора"""
        processor = DocumentProcessor()
        assert processor is not None
        assert processor.file_extractor is not None
        assert processor.text_processor is not None

    def test_empty_directory_processing(self):
        """Тест обработки пустой директории"""
        with tempfile.TemporaryDirectory() as temp_dir:
            processor = DocumentProcessor()
            docs = list(processor.process_documents(Path(temp_dir)))
            assert len(docs) == 0

    @patch("semantic_search.core.document_processor.FileExtractor")
    def test_document_processing_with_mock(self, mock_extractor):
        """Тест обработки с мокированием"""
        # Настройка мока
        mock_instance = Mock()
        mock_extractor.return_value = mock_instance

        # Используем относительный путь вместо абсолютного
        test_path = Path("test_documents")
        mock_instance.find_documents.return_value = [test_path / "test_doc.pdf"]
        mock_instance.extract_text.return_value = "Test document content " * 20

        processor = DocumentProcessor()
        processor.file_extractor = mock_instance

        docs = list(processor.process_documents(test_path))
        assert len(docs) > 0
        assert docs[0].relative_path == "test_doc.pdf"


class TestValidators:
    """Тесты валидаторов"""

    def test_text_validation_success(self):
        """Тест успешной валидации текста"""
        text = DataValidator.validate_text("Hello world", min_length=5)
        assert text == "Hello world"

    def test_text_validation_failure(self):
        """Тест неудачной валидации текста"""
        with pytest.raises(ValidationError):
            DataValidator.validate_text("Hi", min_length=10)

    def test_search_params_validation(self):
        """Тест валидации параметров поиска"""
        params = DataValidator.validate_search_params(
            query="test query", top_k=10, similarity_threshold=0.5
        )

        assert params["query"] == "test query"
        assert params["top_k"] == 10
        assert params["similarity_threshold"] == 0.5

    def test_model_params_validation(self):
        """Тест валидации параметров модели"""
        params = DataValidator.validate_model_params(vector_size=100, epochs=20)

        assert params["vector_size"] == 100
        assert params["epochs"] == 20


@pytest.fixture
def sample_corpus():
    """Фикстура с образцом корпуса"""
    return [
        (["машинное", "обучение", "алгоритм"], "doc1.pdf", {"tokens_count": 3}),
        (
            ["нейронная", "сеть", "глубокое", "обучение"],
            "doc2.pdf",
            {"tokens_count": 4},
        ),
        (["анализ", "данных", "статистика"], "doc3.pdf", {"tokens_count": 3}),
    ]


class TestDoc2VecTrainer:
    """Тесты тренера Doc2Vec"""

    def test_trainer_initialization(self):
        """Тест инициализации тренера"""
        trainer = Doc2VecTrainer()
        assert trainer is not None
        assert trainer.model is None

    def test_tagged_documents_creation(self, sample_corpus):
        """Тест создания TaggedDocument объектов"""
        trainer = Doc2VecTrainer()

        if not hasattr(trainer, "create_tagged_documents"):
            pytest.skip("Gensim не доступен")

        tagged_docs = trainer.create_tagged_documents(sample_corpus)
        assert len(tagged_docs) == 3
        assert tagged_docs[0].tags == ["doc1.pdf"]


class TestSearchEngine:
    """Тесты поискового движка"""

    def test_search_engine_initialization(self):
        """Тест инициализации поискового движка"""
        engine = SemanticSearchEngine()
        assert engine is not None
        assert engine.model is None

    def test_empty_query_handling(self):
        """Тест обработки пустого запроса"""
        engine = SemanticSearchEngine()
        results = engine.search("")
        assert len(results) == 0

    def test_search_without_model(self):
        """Тест поиска без модели"""
        engine = SemanticSearchEngine()
        results = engine.search("test query")
        assert len(results) == 0


# Бенчмарк тесты
class TestPerformance:
    """Тесты производительности"""

    def test_text_processing_speed(self, benchmark):
        """Бенчмарк скорости обработки текста"""
        from semantic_search.utils.text_utils import TextProcessor

        processor = TextProcessor()
        test_text = "Это тестовый текст для проверки скорости обработки. " * 100

        result = benchmark(processor.preprocess_text, test_text)
        assert isinstance(result, list)
        assert len(result) > 0


========================================
FILE: config\app_config.json
========================================
{
  "text_processing": {
    "min_text_length": 100,
    "max_text_length": 5000000,
    "min_tokens_count": 10,
    "min_token_length": 2,
    "min_sentence_length": 10,
    "remove_stop_words": true,
    "lemmatize": true,
    "max_file_size_mb": 100,
    "chunk_size": 800000,
    "spacy_max_length": 3000000
  },
  "doc2vec": {
    "vector_size": 150,
    "window": 10,
    "min_count": 2,
    "epochs": 40,
    "workers": 15,
    "seed": 42,
    "dm": 1,
    "negative": 5,
    "hs": 0,
    "sample": 0.0001
  },
  "search": {
    "default_top_k": 10,
    "max_top_k": 100,
    "similarity_threshold": 0.1,
    "enable_caching": true,
    "cache_size": 1000,
    "enable_filtering": true
  },
  "gui": {
    "window_title": "Semantic Document Search",
    "window_size": [
      1400,
      900
    ],
    "theme": "default",
    "font_size": 10,
    "enable_dark_theme": false,
    "auto_save_settings": true
  },
  "summarization": {
    "default_sentences_count": 5,
    "max_sentences_count": 20,
    "min_sentence_length": 15,
    "use_textrank": true,
    "damping_factor": 0.85,
    "max_iterations": 100
  },
  "performance": {
    "enable_monitoring": true,
    "log_slow_operations": true,
    "slow_operation_threshold": 5.0,
    "memory_warning_threshold": 80,
    "enable_profiling": false
  }
}

================================================================================
EXPORT SUMMARY:
----------------------------------------
Total files included: 44
Export file: project_export.txt
Export size: 422,671 bytes
================================================================================
