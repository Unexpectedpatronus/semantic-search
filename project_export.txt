================================================================================
SEMANTIC SEARCH PROJECT EXPORT
Date: 2025-06-22 19:35:43
Root: C:\Users\evgen\Evgeny\Dev_projects\Dev_Python\diplom\semantic-search
================================================================================

PROJECT STRUCTURE:
----------------------------------------

[root]
  - pyproject.toml
  - README.md
  - LICENSE
  - .gitignore

src\semantic_search/
  - __init__.py
  - config.py
  - main.py

src\semantic_search\core/
  - __init__.py
  - doc2vec_trainer.py
  - document_processor.py
  - search_engine.py
  - text_summarizer.py

src\semantic_search\gui/
  - __init__.py
  - evaluation_widget.py
  - main_window.py

src\semantic_search\utils/
  - __init__.py
  - cache_manager.py
  - file_utils.py
  - logging_config.py
  - notification_system.py
  - performance_monitor.py
  - statistics.py
  - task_manager.py
  - text_utils.py
  - validators.py

src\semantic_search\evaluation/
  - __init__.py
  - baselines.py
  - comparison.py
  - metrics.py

scripts/
  - build.py
  - cache_clear.py
  - check_dependencies.py
  - cleanup.py
  - demo_evaluation.py
  - export_project.py
  - inspect_corpus.py
  - list_models.py
  - print_project_tree.py
  - run_semantic_search.bat
  - run_semantic_search.sh
  - setup_spacy.py

tests/
  - __init__.py
  - test_core_functionality.py

config/
  - app_config.json

================================================================================
FILE CONTENTS:
================================================================================


========================================
FILE: pyproject.toml
========================================
[project]
name = "semantic-search"
version = "0.1.0"
description = "–î–µ—Å–∫—Ç–æ–ø–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Doc2Vec"
authors = [
    {name = "Evgeny Odintsov",email = "ev1genial@gmail.com"}
]
readme = "README.md"
requires-python = ">=3.10,<3.13"
dependencies = [
    "pymupdf (>=1.26.0,<2.0.0)",
    "pywin32 (>=310,<311); sys_platform == 'win32'",
    "python-docx (>=1.1.2,<2.0.0)",
    "spacy (>=3.8.7,<4.0.0)",
    "gensim (>=4.3.3,<5.0.0)",
    "pyqt6 (>=6.9.1,<7.0.0)",
    "loguru (>=0.7.3,<0.8.0)",
    "click (>=8.2.1,<9.0.0)",
    "scikit-learn (>=1.7.0,<2.0.0)",
    "psutil (>=7.0.0,<8.0.0)",
    "matplotlib (>=3.10.3,<4.0.0)",
    "seaborn (>=0.13.2,<0.14.0)"
]

[project.optional-dependencies]
openai = ["openai (>=1.88.0,<2.0.0)"]

[project.scripts]
semantic-search = "semantic_search.main:main"
semantic-search-cli = "semantic_search.main:cli_mode"

[tool.poetry]
packages = [{include = "semantic_search", from = "src"}]

[tool.poetry.group.dev.dependencies]
ruff = "^0.11.13"
pytest = "^8.4.0"
pytest-qt = "^4.4.0"
pyinstaller = "^6.14.1"
pytest-cov = "^6.2.1"
pytest-benchmark = "^5.1.0"
mypy = "^1.16.1"
pre-commit = "^4.2.0"

[build-system]
requires = ["poetry-core>=2.0.0,<3.0.0"]
build-backend = "poetry.core.masonry.api"

[tool.pytest.ini_options]
testpaths = ["tests"]


========================================
FILE: README.md
========================================
# Semantic Document Search

–î–µ—Å–∫—Ç–æ–ø–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –ª–æ–∫–∞–ª—å–Ω–æ–π –±–∞–∑–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ Doc2Vec.

## üöÄ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- üîç **–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫** - –Ω–∞—Ö–æ–¥–∏—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —Å–º—ã—Å–ª—É, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
- üìÑ **–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤** - PDF, DOCX, DOC
- üìù **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –≤—ã–∂–∏–º–æ–∫** - —ç–∫—Å—Ç—Ä–∞–∫—Ç–∏–≤–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
- üß† **–û–±—É—á–µ–Ω–∏–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π** - —Å–æ–∑–¥–∞–≤–∞–π—Ç–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –≤–∞—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
- üíª **–£–¥–æ–±–Ω—ã–π –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å** - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –∏ –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω—ã–π
- üöÄ **–í—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å** - –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ

## üìã –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

- Python 3.10 - 3.12
- Windows 10/11 (–¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ .doc —Ñ–∞–π–ª–æ–≤)
- –ú–∏–Ω–∏–º—É–º 4 –ì–ë –û–ó–£
- 500 –ú–ë —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞ –Ω–∞ –¥–∏—Å–∫–µ

## üõ†Ô∏è –£—Å—Ç–∞–Ω–æ–≤–∫–∞

### 1. –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
```bash
git clone https://github.com/yourusername/semantic-search.git
cd semantic-search
```

### 2. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Poetry (–µ—Å–ª–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)
```bash
# Windows (PowerShell)
(Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | py -

# Linux/MacOS
curl -sSL https://install.python-poetry.org | python3 -
```

### 3. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ Poetry
poetry install
```

### 4. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ SpaCy
```bash
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞
poetry run python scripts/setup_spacy.py

# –ò–ª–∏ –≤—Ä—É—á–Ω—É—é
poetry run python -m spacy download ru_core_news_sm
```

## üöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è

### –ì—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
```bash
poetry run semantic-search
```

### –ö–æ–º–∞–Ω–¥–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
```bash
poetry run semantic-search-cli --help
```

## üìñ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### –ì—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å

1. **–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏**
   - –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É "–û–±—É—á–µ–Ω–∏–µ"
   - –í—ã–±–µ—Ä–∏—Ç–µ –ø–∞–ø–∫—É —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
   - –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
   - –ù–∞–∂–º–∏—Ç–µ "–ù–∞—á–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ"

2. **–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤**
   - –í—ã–±–µ—Ä–∏—Ç–µ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ —Å–ø–∏—Å–∫–∞
   - –í–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
   - –ù–∞–∂–º–∏—Ç–µ "–ü–æ–∏—Å–∫"
   - –ö–ª–∏–∫–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞

3. **–°–æ–∑–¥–∞–Ω–∏–µ –≤—ã–∂–∏–º–æ–∫**
   - –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É "–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è"
   - –í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª –¥–æ–∫—É–º–µ–Ω—Ç–∞
   - –£–∫–∞–∂–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
   - –ù–∞–∂–º–∏—Ç–µ "–°–æ–∑–¥–∞—Ç—å –≤—ã–∂–∏–º–∫—É"

### –ö–æ–º–∞–Ω–¥–Ω–∞—è —Å—Ç—Ä–æ–∫–∞

#### –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
```bash
# –ë–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ
poetry run semantic-search-cli train -d /path/to/documents

# –° –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
poetry run semantic-search-cli train -d /path/to/documents \
    --model my_model \
    --vector-size 200 \
    --epochs 50
```

#### –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
```bash
poetry run semantic-search-cli search -d /path/to/documents \
    -q "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏" \
    --top-k 5
```

#### –°–æ–∑–¥–∞–Ω–∏–µ –≤—ã–∂–∏–º–æ–∫
```bash
# –î–ª—è –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
poetry run semantic-search-cli summarize-file -f document.pdf -s 5

# –î–ª—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ø–∞–ø–∫–µ
poetry run semantic-search-cli summarize-batch -d /path/to/documents \
    -s 3 -o /path/to/summaries
```

#### –ü—Ä–æ—Å–º–æ—Ç—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
```bash
poetry run semantic-search-cli stats -d /path/to/documents -m my_model
```

#### –ö–æ–º–∞–Ω–¥—ã –∫–æ–Ω—Ñ–∏–≥–∞
```bash
# –ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ–∫—É—â—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
poetry run semantic-search-cli config --show

# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
poetry run semantic-search-cli config --set text_processing.max_text_length 5000000

# –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
poetry run semantic-search-cli config --reload
```

## üî¨ –û—Ü–µ–Ω–∫–∞ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å OpenAI

–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –≤–∫–ª—é—á–∞–µ—Ç –º–æ–¥—É–ª—å –¥–ª—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞ –º–µ–∂–¥—É –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é Doc2Vec –∏ OpenAI embeddings.

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π OpenAI
poetry install -E openai

# –ò–ª–∏ –æ—Ç–¥–µ–ª—å–Ω–æ
pip install openai pandas
```

### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ OpenAI API

```bash
# Windows
setx OPENAI_API_KEY "your_api_key_here"

# Linux/MacOS
export OPENAI_API_KEY="your_api_key_here"
```

### –ó–∞–ø—É—Å–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è

#### –ß–µ—Ä–µ–∑ GUI

1. –û—Ç–∫—Ä–æ–π—Ç–µ –≤–∫–ª–∞–¥–∫—É "üìä –û—Ü–µ–Ω–∫–∞ –º–µ—Ç–æ–¥–æ–≤"
2. –í–≤–µ–¥–∏—Ç–µ OpenAI API –∫–ª—é—á
3. –í—ã–±–µ—Ä–∏—Ç–µ –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤
4. –ù–∞–∂–º–∏—Ç–µ "–ó–∞–ø—É—Å—Ç–∏—Ç—å —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ"

#### –ß–µ—Ä–µ–∑ CLI

```bash
# –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ (3 —Ç–µ—Å—Ç–∞)
poetry run semantic-search-cli evaluate --test-cases quick

# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ (5 —Ç–µ—Å—Ç–æ–≤)
poetry run semantic-search-cli evaluate --test-cases standard

# –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ (10 —Ç–µ—Å—Ç–æ–≤)
poetry run semantic-search-cli evaluate --test-cases extended

# –° —É–∫–∞–∑–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ –∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
poetry run semantic-search-cli evaluate \
    --model my_model \
    --test-cases standard \
    --output-dir ./evaluation_results
```

#### –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç

```bash
# –ó–∞–ø—É—Å–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
poetry run python scripts/demo_evaluation.py
```

### –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏

–°–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏:

- **MAP (Mean Average Precision)** - –æ—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
- **MRR (Mean Reciprocal Rank)** - –ø–æ–∑–∏—Ü–∏—è –ø–µ—Ä–≤–æ–≥–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
- **Precision@k** - —Ç–æ—á–Ω–æ—Å—Ç—å –≤ —Ç–æ–ø-k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
- **Recall@k** - –ø–æ–ª–Ω–æ—Ç–∞ –≤ —Ç–æ–ø-k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
- **NDCG@k** - –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π –≤—ã–∏–≥—Ä—ã—à
- **–°–∫–æ—Ä–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞** - —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –∑–∞–ø—Ä–æ—Å
- **–≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å** - —Å—Ç–æ–∏–º–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏

–ü–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏ —Å–æ–∑–¥–∞—é—Ç—Å—è:

```
data/evaluation_results/
‚îú‚îÄ‚îÄ comparison_results.csv      # –¢–∞–±–ª–∏—Ü–∞ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
‚îú‚îÄ‚îÄ comparison_report.txt       # –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç
‚îú‚îÄ‚îÄ detailed_results.json       # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
‚îî‚îÄ‚îÄ plots/
    ‚îú‚îÄ‚îÄ comparison_plots.png    # –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏
    ‚îî‚îÄ‚îÄ *_detailed.png          # –î–µ—Ç–∞–ª—å–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –ø–æ –º–µ—Ç–æ–¥–∞–º
```

### –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

#### –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ Doc2Vec:
- ‚úÖ **–°–∫–æ—Ä–æ—Å—Ç—å**: –í 10-100 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ OpenAI
- ‚úÖ **–ê–≤—Ç–æ–Ω–æ–º–Ω–æ—Å—Ç—å**: –†–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
- ‚úÖ **–°—Ç–æ–∏–º–æ—Å—Ç—å**: –ë–µ—Å–ø–ª–∞—Ç–Ω–æ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
- ‚úÖ **–ö–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å**: –î–∞–Ω–Ω—ã–µ –æ—Å—Ç–∞—é—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω—ã–º–∏
- ‚úÖ **–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è**: –õ—É—á—à–µ –¥–ª—è —É–∑–∫–æ—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ—Ä–ø—É—Å–æ–≤

#### –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ OpenAI:
- ‚úÖ **–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å**: –†–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
- ‚úÖ **–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç—å**: –ü–æ–¥–¥–µ—Ä–∂–∫–∞ 100+ —è–∑—ã–∫–æ–≤
- ‚úÖ **–û–±–Ω–æ–≤–ª–µ–Ω–∏—è**: –ü–æ—Å—Ç–æ—è–Ω–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–∏

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–±–æ—Ä—É –º–µ—Ç–æ–¥–∞

| –°—Ü–µ–Ω–∞—Ä–∏–π | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|----------|--------------|
| –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ | Doc2Vec |
| –í—ã—Å–æ–∫–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞ (>1000 –∑–∞–ø—Ä–æ—Å–æ–≤/–¥–µ–Ω—å) | Doc2Vec |
| –ö–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ | Doc2Vec |
| –†–∞–±–æ—Ç–∞ –±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ | Doc2Vec |
| –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ | OpenAI |
| –ú–∞–ª—ã–π –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö | OpenAI |
| –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã | OpenAI |

### –≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ

–ü—Ä–∏ —Ç–∏–ø–∏—á–Ω–æ–π –Ω–∞–≥—Ä—É–∑–∫–µ –≤ 1000 –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –¥–µ–Ω—å:
- **OpenAI**: ~$1.50/–º–µ—Å—è—Ü ($18/–≥–æ–¥)
- **Doc2Vec**: $0 (–ø–æ—Å–ª–µ –µ–¥–∏–Ω–æ—Ä–∞–∑–æ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è)
- **–≠–∫–æ–Ω–æ–º–∏—è**: $18/–≥–æ–¥ –Ω–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –Ω–∞–≥—Ä—É–∑–∫–µ

–ü—Ä–∏ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ (10,000+ –∑–∞–ø—Ä–æ—Å–æ–≤/–¥–µ–Ω—å) —ç–∫–æ–Ω–æ–º–∏—è –º–æ–∂–µ—Ç –¥–æ—Å—Ç–∏–≥–∞—Ç—å —Å–æ—Ç–µ–Ω –¥–æ–ª–ª–∞—Ä–æ–≤ –≤ –º–µ—Å—è—Ü.

## üèóÔ∏è –°–±–æ—Ä–∫–∞ –≤ –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–π —Ñ–∞–π–ª

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ PyInstaller (–µ—Å–ª–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)
poetry add --group dev pyinstaller

# –°–±–æ—Ä–∫–∞ –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª
poetry run python scripts/build.py --onefile --windowed

# –°–±–æ—Ä–∫–∞ –≤ –ø–∞–ø–∫—É
poetry run python scripts/build.py
```

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

```bash
# –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤
poetry run pytest

# –° –ø–æ–∫—Ä—ã—Ç–∏–µ–º –∫–æ–¥–∞
poetry run pytest --cov=semantic_search

# –¢–æ–ª—å–∫–æ —é–Ω–∏—Ç-—Ç–µ—Å—Ç—ã
poetry run pytest tests/test_core_functionality.py

# –ë–µ–Ω—á–º–∞—Ä–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
poetry run pytest tests/ -k "performance" --benchmark-only
```

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
semantic-search/
‚îú‚îÄ‚îÄ README.md                            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–µ–∫—Ç–µ
‚îú‚îÄ‚îÄ config/                              # –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
‚îÇ   ‚îî‚îÄ‚îÄ app_config.json
‚îú‚îÄ‚îÄ data/                                # –î–∞–Ω–Ω—ã–µ –∏ –º–æ–¥–µ–ª–∏
‚îÇ   ‚îú‚îÄ‚îÄ cache/                           # –ö—ç—à
‚îÇ   ‚îú‚îÄ‚îÄ models/                          # –û–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
‚îÇ   ‚îî‚îÄ‚îÄ temp/                            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
‚îú‚îÄ‚îÄ logs/                                # –õ–æ–≥–∏
‚îú‚îÄ‚îÄ pyproject.toml                       # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Poetry
‚îú‚îÄ‚îÄ scripts/                             # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∫—Ä–∏–ø—Ç—ã
‚îÇ   ‚îú‚îÄ‚îÄ build.py
‚îÇ   ‚îú‚îÄ‚îÄ print_project_tree.py
‚îÇ   ‚îî‚îÄ‚îÄ setup_spacy.py
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ semantic_search/
‚îÇ       ‚îú‚îÄ‚îÄ config.py                    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
‚îÇ       ‚îú‚îÄ‚îÄ core/                        # –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ doc2vec_trainer.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ document_processor.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ search_engine.py
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ text_summarizer.py
‚îÇ       ‚îú‚îÄ‚îÄ gui/                         # –ì—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ main_window.py
‚îÇ       ‚îú‚îÄ‚îÄ main.py                      # –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞
‚îÇ       ‚îî‚îÄ‚îÄ utils/                       # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥—É–ª–∏
‚îÇ           ‚îú‚îÄ‚îÄ cache_manager.py
‚îÇ           ‚îú‚îÄ‚îÄ file_utils.py
‚îÇ           ‚îú‚îÄ‚îÄ logging_config.py
‚îÇ           ‚îú‚îÄ‚îÄ notification_system.py
‚îÇ           ‚îú‚îÄ‚îÄ performance_monitor.py
‚îÇ           ‚îú‚îÄ‚îÄ statistics.py
‚îÇ           ‚îú‚îÄ‚îÄ task_manager.py
‚îÇ           ‚îú‚îÄ‚îÄ text_utils.py
‚îÇ           ‚îî‚îÄ‚îÄ validators.py
‚îî‚îÄ‚îÄ tests/                               # –¢–µ—Å—Ç—ã
    ‚îî‚îÄ‚îÄ test_core_functionality.py
```

## ‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–µ—Ç —Ñ–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ `config/app_config.json` –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ.

### –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:

```json
{
  "text_processing": {
    "min_text_length": 100,
    "max_text_length": 5000000,
    "min_tokens_count": 10,
    "min_token_length": 2,
    "min_sentence_length": 10,
    "remove_stop_words": true,
    "lemmatize": true,
    "max_file_size_mb": 100,
    "chunk_size": 800000,
    "spacy_max_length": 3000000
  },
  "doc2vec": {
    "vector_size": 150,
    "window": 10,
    "min_count": 2,
    "epochs": 40,
    "workers": 15,
    "seed": 42,
    "dm": 1,
    "negative": 5,
    "hs": 0,
    "sample": 0.0001
  },
  "search": {
    "default_top_k": 10,
    "max_top_k": 100,
    "similarity_threshold": 0.1,
    "enable_caching": true,
    "cache_size": 1000,
    "enable_filtering": true
  }
}
```

## üîß –†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º

### SpaCy –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
```bash
# –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –º–æ–¥–µ–ª—å
poetry run python -m spacy download ru_core_news_sm --force
```

### –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ .doc —Ñ–∞–π–ª–æ–≤
- –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω Microsoft Word
- –ò–ª–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–π—Ç–µ .doc —Ñ–∞–π–ª—ã –≤ .docx

### –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
- –£–º–µ–Ω—å—à–∏—Ç–µ —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ–Ω—å—à—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤
- –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–π—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —á–∞—Å—Ç—è–º–∏

## ü§ù –í–∫–ª–∞–¥ –≤ –ø—Ä–æ–µ–∫—Ç

1. –§–æ—Ä–∫–Ω–∏—Ç–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
2. –°–æ–∑–¥–∞–π—Ç–µ –≤–µ—Ç–∫—É –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ (`git checkout -b feature/AmazingFeature`)
3. –ó–∞–∫–æ–º–º–∏—Ç—å—Ç–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è (`git commit -m 'Add some AmazingFeature'`)
4. –ó–∞–ø—É—à—å—Ç–µ –≤ –≤–µ—Ç–∫—É (`git push origin feature/AmazingFeature`)
5. –û—Ç–∫—Ä–æ–π—Ç–µ Pull Request

## üìù –õ–∏—Ü–µ–Ω–∑–∏—è

–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç –ª–∏—Ü–µ–Ω–∑–∏—Ä–æ–≤–∞–Ω –ø–æ–¥ MIT License - —Å–º. —Ñ–∞–π–ª LICENSE –¥–ª—è –¥–µ—Ç–∞–ª–µ–π.

## üë§ –ê–≤—Ç–æ—Ä

**Evgeny Odintsov**
- Email: ev1genial@gmail.com
- GitHub: [@unexpectedpatronus](https://github.com/Unexpectedpatronus)

## üôè –ë–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç–∏

- [Gensim](https://radimrehurek.com/gensim/) - –∑–∞ –æ—Ç–ª–∏—á–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é Doc2Vec
- [SpaCy](https://spacy.io/) - –∑–∞ –º–æ—â–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã NLP
- [PyQt6](https://www.riverbankcomputing.com/software/pyqt/) - –∑–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Å–æ–∑–¥–∞–Ω–∏—è GUI

## üìà Roadmap

- [ ] –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
- [ ] –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
- [ ] –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –æ–±–ª–∞—á–Ω—ã–º–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞–º–∏
- [ ] –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
- [ ] –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –¥—Ä—É–≥–∏—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ (TXT, RTF, ODT)
- [ ] –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- [ ] –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤
- [ ] API –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –¥—Ä—É–≥–∏–º–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è–º–∏


========================================
FILE: LICENSE
========================================
[Binary file - 1093 bytes]


========================================
FILE: .gitignore
========================================
[Binary file - 4564 bytes]


========================================
FILE: src\semantic_search\__init__.py
========================================
"""Semantic Search - –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º"""

__version__ = "1.0.0"
__author__ = "Evgeny Odintsov"
__email__ = "ev1genial@gmail.com"

from semantic_search.core.doc2vec_trainer import Doc2VecTrainer
from semantic_search.core.document_processor import DocumentProcessor
from semantic_search.core.search_engine import SearchResult, SemanticSearchEngine
from semantic_search.core.text_summarizer import TextSummarizer

__all__ = [
    "Doc2VecTrainer",
    "DocumentProcessor",
    "SemanticSearchEngine",
    "SearchResult",
    "TextSummarizer",
]


========================================
FILE: src\semantic_search\config.py
========================================
"""–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""

import json
import multiprocessing
from dataclasses import asdict, dataclass, field
from pathlib import Path
from typing import Any, Dict, Optional

from loguru import logger

# –ë–∞–∑–æ–≤—ã–µ –ø—É—Ç–∏
PROJECT_ROOT = Path(__file__).parent.parent.parent
SRC_DIR = PROJECT_ROOT / "src"
DATA_DIR = PROJECT_ROOT / "data"
MODELS_DIR = DATA_DIR / "models"
TEMP_DIR = DATA_DIR / "temp"
LOGS_DIR = PROJECT_ROOT / "logs"
CACHE_DIR = DATA_DIR / "cache"
CONFIG_DIR = PROJECT_ROOT / "config"
EVALUATION_RESULTS_DIR = DATA_DIR / "evaluation_results"

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
for dir_path in [
    DATA_DIR,
    MODELS_DIR,
    TEMP_DIR,
    LOGS_DIR,
    CACHE_DIR,
    CONFIG_DIR,
    EVALUATION_RESULTS_DIR,
]:
    dir_path.mkdir(exist_ok=True, parents=True)


@dataclass
class AppConfig:
    """–ì–ª–∞–≤–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    text_processing: Dict[str, Any] = field(default_factory=dict)

    # –ú–æ–¥–µ–ª—å Doc2Vec
    doc2vec: Dict[str, Any] = field(default_factory=dict)

    # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Doc2Vec
    doc2vec_presets: Dict[str, Dict[str, Any]] = field(default_factory=dict)

    # –ü–æ–∏—Å–∫
    search: Dict[str, Any] = field(default_factory=dict)

    # GUI
    gui: Dict[str, Any] = field(default_factory=dict)

    # –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
    summarization: Dict[str, Any] = field(default_factory=dict)

    # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
    performance: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        if not self.text_processing:
            self.text_processing = {
                "min_text_length": 100,
                "max_text_length": 5_000_000,
                "min_tokens_count": 10,
                "min_token_length": 2,
                "min_sentence_length": 10,
                "remove_stop_words": True,
                "lemmatize": True,
                "max_file_size_mb": 100,
                "chunk_size": 800_000,
                "spacy_max_length": 3_000_000,
                # –ù–û–í–û–ï: –ú–æ–¥–µ–ª–∏ SpaCy –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤
                "spacy_models": {
                    "ru": "ru_core_news_sm",  # –†—É—Å—Å–∫–∞—è –º–æ–¥–µ–ª—å
                    "en": "en_core_web_sm",  # –ê–Ω–≥–ª–∏–π—Å–∫–∞—è –º–æ–¥–µ–ª—å
                },
            }

        if not self.doc2vec:
            # –û–ë–ù–û–í–õ–ï–ù–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            self.doc2vec = {
                "vector_size": 300,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç–∏
                "window": 15,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                "min_count": 3,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —à—É–º–∞
                "epochs": 30,  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ —Å–∫–æ—Ä–æ—Å—Ç—å/–∫–∞—á–µ—Å—Ç–≤–æ
                "workers": max(1, multiprocessing.cpu_count() - 1),
                "seed": 42,
                "dm": 1,  # Distributed Memory
                "dm_concat": 0,  # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)
                "dm_mean": 1,  # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å–ª–æ–≤
                "negative": 10,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
                "hs": 0,  # Hierarchical Softmax –æ—Ç–∫–ª—é—á–µ–Ω
                "sample": 1e-5,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤
                "alpha": 0.025,  # –ù–∞—á–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
                "min_alpha": 0.0001,  # –ö–æ–Ω–µ—á–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
            }

        if not self.doc2vec_presets:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            self.doc2vec_presets = {
                "fast": {
                    "vector_size": 200,
                    "window": 10,
                    "min_count": 5,
                    "epochs": 15,
                    "negative": 5,
                    "sample": 1e-4,
                },
                "balanced": {
                    "vector_size": 300,
                    "window": 15,
                    "min_count": 3,
                    "epochs": 30,
                    "negative": 10,
                    "sample": 1e-5,
                },
                "quality": {
                    "vector_size": 400,
                    "window": 20,
                    "min_count": 2,
                    "epochs": 50,
                    "negative": 15,
                    "sample": 1e-5,
                },
            }

        if not self.search:
            self.search = {
                "default_top_k": 10,
                "max_top_k": 100,
                "similarity_threshold": 0.1,
                "enable_caching": True,
                "cache_size": 1000,
                "enable_filtering": True,
            }

        if not self.gui:
            self.gui = {
                "window_title": "Semantic Document Search",
                "window_size": (1400, 900),
                "theme": "default",
                "font_size": 10,
                "enable_dark_theme": False,
                "auto_save_settings": True,
            }

        if not self.summarization:
            self.summarization = {
                "default_sentences_count": 5,
                "max_sentences_count": 20,
                "min_sentence_length": 15,
                "use_textrank": True,
                "damping_factor": 0.85,
                "max_iterations": 100,
                "filter_short_sentences": True,  # –í–∫–ª—é—á–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
                "max_digit_ratio": 0.5,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–æ–ª—è —Ü–∏—Ñ—Ä –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏
                "min_meaningful_words": 2,  # –ú–∏–Ω–∏–º—É–º –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤ (–¥–ª–∏–Ω–Ω–µ–µ 3 —Å–∏–º–≤–æ–ª–æ–≤)
            }

        if not self.performance:
            self.performance = {
                "enable_monitoring": True,
                "log_slow_operations": True,
                "slow_operation_threshold": 5.0,  # —Å–µ–∫—É–Ω–¥—ã
                "memory_warning_threshold": 80,  # –ø—Ä–æ—Ü–µ–Ω—Ç
                "enable_profiling": False,
                # –ù–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–æ–º
                "task_manager_workers": min(8, multiprocessing.cpu_count()),
                "cpu_task_manager_workers": max(1, multiprocessing.cpu_count() - 2),
                "adaptive_workers": True,  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è
                "min_workers_per_task": 2,  # –ú–∏–Ω–∏–º—É–º –≤–æ—Ä–∫–µ—Ä–æ–≤ –Ω–∞ –∑–∞–¥–∞—á—É
            }


class ConfigManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""

    def __init__(self, config_file: Optional[Path] = None):
        self.config_file = config_file or (CONFIG_DIR / "app_config.json")
        self._config = None

    @property
    def config(self) -> AppConfig:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å –ª–µ–Ω–∏–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π"""
        if self._config is None:
            self._config = self.load_config()
        return self._config

    def load_config(self) -> AppConfig:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
        if self.config_file.exists():
            try:
                with open(self.config_file, "r", encoding="utf-8") as f:
                    config_data = json.load(f)

                logger.info(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {self.config_file}")
                return AppConfig(**config_data)

            except Exception as e:
                logger.warning(
                    f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"
                )

        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        default_config = AppConfig()
        self.save_config(default_config)
        return default_config

    def save_config(self, config: AppConfig) -> bool:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –≤ —Ñ–∞–π–ª"""
        try:
            with open(self.config_file, "w", encoding="utf-8") as f:
                json.dump(asdict(config), f, indent=2, ensure_ascii=False)

            logger.info(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {self.config_file}")
            return True

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            return False

    def reload_config(self) -> AppConfig:
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        self._config = None
        return self.config

    def reset_to_defaults(self) -> AppConfig:
        """–°–±—Ä–æ—Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∫ –∑–Ω–∞—á–µ–Ω–∏—è–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        self._config = AppConfig()
        self.save_config(self._config)
        logger.info("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–±—Ä–æ—à–µ–Ω–∞ –∫ –∑–Ω–∞—á–µ–Ω–∏—è–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
        return self._config

    def update_config(self, **kwargs) -> bool:
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        try:
            config_dict = asdict(self.config)

            for key, value in kwargs.items():
                if key in config_dict:
                    if isinstance(config_dict[key], dict) and isinstance(value, dict):
                        config_dict[key].update(value)
                    else:
                        config_dict[key] = value

            self._config = AppConfig(**config_dict)
            return self.save_config(self._config)

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            return False


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
config_manager = ConfigManager()

# –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
SUPPORTED_EXTENSIONS = {".pdf", ".docx", ".doc"}

# –û–ë–ù–û–í–õ–ï–ù–û: –¢–µ–ø–µ—Ä—å –º–æ–¥–µ–ª–∏ SpaCy –±–µ—Ä—É—Ç—Å—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
SPACY_MODELS: dict = config_manager.config.text_processing.get(
    "spacy_models", {"ru": "ru_core_news_sm", "en": "en_core_web_sm"}
)

# –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –æ—Å—Ç–∞–≤–ª—è–µ–º SPACY_MODEL –∫–∞–∫ —Ä—É—Å—Å–∫—É—é –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
SPACY_MODEL = SPACY_MODELS.get("ru", "ru_core_news_sm")

TEXT_PROCESSING_CONFIG = config_manager.config.text_processing
DOC2VEC_CONFIG = config_manager.config.doc2vec
DOC2VEC_PRESETS = config_manager.config.doc2vec_presets
SEARCH_CONFIG = config_manager.config.search
GUI_CONFIG = config_manager.config.gui
SUMMARIZATION_CONFIG = config_manager.config.summarization
PERFORMANCE_CONFIG = config_manager.config.performance


========================================
FILE: src\semantic_search\main.py
========================================
"""–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ"""

import sys
import time
from pathlib import Path
from typing import Optional

import click
from loguru import logger

from semantic_search.config import GUI_CONFIG, SPACY_MODEL
from semantic_search.core.doc2vec_trainer import Doc2VecTrainer
from semantic_search.core.document_processor import DocumentProcessor
from semantic_search.core.search_engine import SemanticSearchEngine
from semantic_search.core.text_summarizer import TextSummarizer
from semantic_search.utils.file_utils import FileExtractor
from semantic_search.utils.logging_config import setup_logging
from semantic_search.utils.notification_system import notification_manager
from semantic_search.utils.performance_monitor import PerformanceMonitor
from semantic_search.utils.statistics import (
    calculate_model_statistics,
    calculate_statistics_from_processed_docs,
    format_statistics_for_display,
)
from semantic_search.utils.task_manager import task_manager
from semantic_search.utils.text_utils import check_spacy_model_availability
from semantic_search.utils.validators import DataValidator, FileValidator

performance_monitor = PerformanceMonitor()


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""

    notification_manager.start()

    try:
        setup_logging()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ SpaCy —Å —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ–º
        spacy_info = check_spacy_model_availability()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å—Ç–∞–Ω–æ–≤–∫—É –º–æ–¥–µ–ª–µ–π
        if not spacy_info["spacy_installed"]:
            notification_manager.error(
                "SpaCy –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω",
                "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ SpaCy –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º",
                "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: pip install spacy",
            )
        else:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏
            if not (spacy_info["ru_model_loadable"] or spacy_info["en_model_loadable"]):
                notification_manager.warning(
                    "SpaCy –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã",
                    "–ù–∏ –æ–¥–Ω–∞ —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞",
                    "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: poetry run python scripts/setup_spacy.py",
                )
            else:
                # –ò–Ω—Ñ–æ—Ä–º–∏—Ä—É–µ–º –æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö
                models_status = []
                if spacy_info["ru_model_loadable"]:
                    models_status.append("—Ä—É—Å—Å–∫–∞—è")
                if spacy_info["en_model_loadable"]:
                    models_status.append("–∞–Ω–≥–ª–∏–π—Å–∫–∞—è")

                notification_manager.success(
                    "SpaCy –≥–æ—Ç–æ–≤", f"–ó–∞–≥—Ä—É–∂–µ–Ω—ã –º–æ–¥–µ–ª–∏: {', '.join(models_status)}"
                )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å PyQt6
        try:
            from PyQt6.QtWidgets import QApplication

            from semantic_search.gui.main_window import MainWindow
        except ImportError as e:
            logger.error(f"PyQt6 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {e}")
            notification_manager.error(
                "–û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞",
                "PyQt6 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω",
                "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: poetry install",
            )
            print("\n‚ùå PyQt6 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
            print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∫–æ–º–∞–Ω–¥–æ–π: poetry install")
            print("\n–í—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CLI —Ä–µ–∂–∏–º:")
            print("poetry run semantic-search-cli --help")
            sys.exit(1)

        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è Qt
        app = QApplication(sys.argv)
        app.setApplicationName(GUI_CONFIG["window_title"])
        app.setOrganizationName("Semantic Search")

        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å—Ç–∏–ª—å
        app.setStyle("Fusion")  # –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å—Ç–∏–ª—å

        # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≥–ª–∞–≤–Ω–æ–≥–æ –æ–∫–Ω–∞
        main_window = MainWindow()
        main_window.show()

        logger.info("–ì–ª–∞–≤–Ω–æ–µ –æ–∫–Ω–æ —Å–æ–∑–¥–∞–Ω–æ –∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–æ")

        # –ó–∞–ø—É—Å–∫ —Ü–∏–∫–ª–∞ —Å–æ–±—ã—Ç–∏–π
        exit_code = app.exec()
        logger.info(f"–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —Å –∫–æ–¥–æ–º: {exit_code}")
        sys.exit(exit_code)

    except Exception as e:
        notification_manager.error(
            "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞", "–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è", str(e)
        )
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        print("\n–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –¥–ª—è –¥–µ—Ç–∞–ª–µ–π")
        sys.exit(1)
    finally:
        notification_manager.stop()
        task_manager.shutdown()


@click.group()
def cli():
    """Semantic Document Search CLI"""
    setup_logging()


@cli.command()
@click.option("--documents", "-d", required=True, help="–ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏")
@click.option("--model", "-m", default="doc2vec_model", help="–ò–º—è –º–æ–¥–µ–ª–∏")
@click.option("--vector-size", default=150, help="–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤")
@click.option("--epochs", default=40, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è")
@click.option("--async-mode", "-a", is_flag=True, help="–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ")
def train(documents: str, model: str, vector_size: int, epochs: int, async_mode: bool):
    """
    –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å Doc2Vec –Ω–∞ –∫–æ—Ä–ø—É—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

    Args:
        documents: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
        model: –ò–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        vector_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤
        epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
    """
    try:
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        documents_path = DataValidator.validate_directory_path(Path(documents))
        model_params = DataValidator.validate_model_params(
            vector_size=vector_size, epochs=epochs
        )

        logger.info("–í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æ")

    except Exception as e:
        click.echo(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
        return

    def train_task(progress_tracker=None):
        """–ó–∞–¥–∞—á–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"""

        # –ù–∞—á–∏–Ω–∞–µ–º –æ—Ç—Å—á–µ—Ç –æ–±—â–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏
        start_time = time.time()

        with performance_monitor.measure_operation("document_processing"):
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            processor = DocumentProcessor()
            processed_docs = []

            file_extractor = FileExtractor()
            file_paths = file_extractor.find_documents(documents_path)

            if progress_tracker:
                progress_tracker.total_steps = len(file_paths) + epochs + 2
                progress_tracker.update(0, "–ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

            for i, doc in enumerate(processor.process_documents(documents_path)):
                processed_docs.append(doc)
                if progress_tracker:
                    progress_tracker.update(
                        i + 1, f"–û–±—Ä–∞–±–æ—Ç–∞–Ω –¥–æ–∫—É–º–µ–Ω—Ç: {doc.relative_path}"
                    )

            if not processed_docs:
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã")

            corpus = [
                (doc.tokens, doc.relative_path, doc.metadata) for doc in processed_docs
            ]

            if progress_tracker:
                progress_tracker.update(message="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–∏...")

        with performance_monitor.measure_operation("model_training"):
            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            trainer = Doc2VecTrainer()

            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            trained_model = trainer.train_model(
                corpus, vector_size=vector_size, epochs=epochs
            )

            if trained_model:
                # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–µ–µ –≤—Ä–µ–º—è
                training_time = time.time() - start_time
                trainer.training_metadata["training_time_formatted"] = (
                    f"{training_time:.1f}—Å ({training_time / 60:.1f}–º)"
                )
                trainer.training_metadata["training_date"] = time.strftime(
                    "%Y-%m-%d %H:%M:%S", time.localtime(start_time)
                )
                trainer.training_metadata["corpus_size"] = len(processed_docs)
                trainer.save_model(trained_model, model)

                if progress_tracker:
                    progress_tracker.finish(
                        f"–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∑–∞ {training_time / 60:.1f} –º–∏–Ω—É—Ç"
                    )

                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                stats = calculate_statistics_from_processed_docs(processed_docs)
                return {
                    "model_saved": True,
                    "documents_processed": len(processed_docs),
                    "vocabulary_size": len(trained_model.wv.key_to_index),
                    "training_time": training_time,
                    "statistics": stats,
                }
            else:
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å")

    if async_mode:
        # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
        notification_manager.start()

        task_id = task_manager.submit_task(
            train_task,
            name=f"–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ {model}",
            description=f"–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –∏–∑ {documents_path}",
            track_progress=True,
            total_steps=100,  # –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤
        )

        click.echo(f"üîÑ –ó–∞–¥–∞—á–∞ –æ–±—É—á–µ–Ω–∏—è –∑–∞–ø—É—â–µ–Ω–∞ (ID: {task_id})")
        click.echo("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É 'status' –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞")

        # –ü–æ–¥–ø–∏—Å–∫–∞ –Ω–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏
        def console_notification_handler(notification):
            if notification.type.value == "success":
                click.echo(f"‚úÖ {notification.title}: {notification.message}")
            elif notification.type.value == "error":
                click.echo(f"‚ùå {notification.title}: {notification.message}")
            elif notification.type.value == "warning":
                click.echo(f"‚ö†Ô∏è {notification.title}: {notification.message}")

        notification_manager.subscribe(console_notification_handler)

    else:
        # –°–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
        try:
            with performance_monitor.measure_operation("full_training"):
                result = train_task()

            click.echo("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
            click.echo(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {result['documents_processed']}")
            click.echo(f"üìö –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {result['vocabulary_size']:,}")

            # –í—ã–≤–æ–¥–∏–º –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            stats_display = format_statistics_for_display(result["statistics"])
            click.echo(f"\n{stats_display}")

        except Exception as e:
            click.echo(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")


@cli.command()
@click.option("--documents", "-d", required=True, help="–ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏")
@click.option("--query", "-q", required=True, help="–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å")
@click.option("--model", "-m", default="doc2vec_model", help="–ò–º—è –º–æ–¥–µ–ª–∏")
@click.option("--top-k", "-k", default=10, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
def search(documents: str, query: str, model: str, top_k: int):
    """
    –ü–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º

    Args:
        documents: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        model: –ò–º—è –º–æ–¥–µ–ª–∏
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –≤—ã–≤–æ–¥–∞
    """
    logger.info(f"–†–µ–∂–∏–º –ø–æ–∏—Å–∫–∞: {query}")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    trainer = Doc2VecTrainer()
    loaded_model = trainer.load_model(model)

    if loaded_model is None:
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å")
        click.echo("–°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å –∫–æ–º–∞–Ω–¥–æ–π: train")
        return

    # –ü–æ–∏—Å–∫
    search_engine = SemanticSearchEngine(loaded_model, trainer.corpus_info)
    results = search_engine.search(query, top_k=top_k)

    if results:
        click.echo(f"\nüîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è '{query}':")
        click.echo("=" * 50)
        for i, result in enumerate(results, 1):
            click.echo(f"{i}. {result.doc_id}")
            click.echo(f"   üìä –°—Ö–æ–¥—Å—Ç–≤–æ: {result.similarity:.3f}")
            if result.metadata:
                tokens_count = result.metadata.get("tokens_count", "N/A")
                file_size = result.metadata.get("file_size", 0)
                click.echo(f"   üìù –¢–æ–∫–µ–Ω–æ–≤: {tokens_count}, –†–∞–∑–º–µ—Ä: {file_size} –±–∞–π—Ç")
            click.echo()
    else:
        click.echo(f"‚ùå –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}'")


@cli.command()
@click.option("--documents", "-d", help="–ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏")
@click.option("--model", "-m", default="doc2vec_model", help="–ò–º—è –º–æ–¥–µ–ª–∏")
def stats(documents: Optional[str], model: str):
    """
    –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –º–æ–¥–µ–ª–∏ –∏ –∫–æ—Ä–ø—É—Å–∞

    Args:
        documents: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        model: –ò–º—è –º–æ–¥–µ–ª–∏
    """
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ—Ä–ø—É—Å–∞
    if documents:
        documents_path = Path(documents)
        if documents_path.exists():
            processor = DocumentProcessor()
            processed_docs = list(processor.process_documents(documents_path))

            if processed_docs:
                stats_data = calculate_statistics_from_processed_docs(processed_docs)
                click.echo(format_statistics_for_display(stats_data))
            else:
                click.echo("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã")
        else:
            click.echo(f"‚ùå –ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {documents_path}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–æ–¥–µ–ª–∏
    trainer = Doc2VecTrainer()
    if trainer.load_model(model):
        model_info = trainer.get_model_info()
        click.echo(f"\n{calculate_model_statistics(model_info)}")
    else:
        click.echo(f"\n‚ùå –ú–æ–¥–µ–ª—å '{model}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")


# –ö–û–ú–ê–ù–î–´ –°–£–ú–ú–ê–†–ò–ó–ê–¶–ò–ò


@cli.command()
@click.option("--file", "-f", required=True, help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏")
@click.option("--model", "-m", default="doc2vec_model", help="–ò–º—è Doc2Vec –º–æ–¥–µ–ª–∏")
@click.option("--sentences", "-s", default=5, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –≤—ã–∂–∏–º–∫–µ")
@click.option(
    "--min-length", "-l", default=15, help="–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ —Å–∏–º–≤–æ–ª–∞—Ö"
)
@click.option(
    "--min-words", "-w", default=5, help="–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏"
)
@click.option(
    "--no-filter", is_flag=True, help="–û—Ç–∫–ª—é—á–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"
)
@click.option("--output", "-o", help="–§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—ã–∂–∏–º–∫–∏")
def summarize_file(
    file: str,
    model: str,
    sentences: int,
    min_length: int,
    min_words: int,
    no_filter: bool,
    output: Optional[str],
) -> None:
    """
    –°–æ–∑–¥–∞—Ç—å –≤—ã–∂–∏–º–∫—É –∏–∑ —Ñ–∞–π–ª–∞ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π

    Args:
        file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        model: –ò–º—è Doc2Vec –º–æ–¥–µ–ª–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        sentences: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –≤—ã–∂–∏–º–∫–µ
        min_length: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        min_words: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏
        no_filter: –û—Ç–∫–ª—é—á–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        output: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—ã–∂–∏–º–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    """
    file_path = Path(file)
    if not file_path.exists():
        click.echo(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        return

    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Doc2Vec
    trainer = Doc2VecTrainer()
    loaded_model = trainer.load_model(model)

    if loaded_model is None:
        click.echo("‚ö†Ô∏è –ú–æ–¥–µ–ª—å Doc2Vec –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∞–∑–æ–≤–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è")
        summarizer = TextSummarizer()
    else:
        click.echo("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Å Doc2Vec")
        summarizer = TextSummarizer(loaded_model)

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    if not no_filter:
        summarizer.min_summary_sentence_length = min_length
        summarizer.min_words_in_sentence = min_words
        click.echo(f"üìè –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: –º–∏–Ω–∏–º—É–º {min_length} —Å–∏–º–≤–æ–ª–æ–≤ –∏ {min_words} —Å–ª–æ–≤")
    else:
        summarizer.min_summary_sentence_length = 1
        summarizer.min_words_in_sentence = 1
        click.echo("üìã –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –æ—Ç–∫–ª—é—á–µ–Ω–∞")

    logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ –≤—ã–∂–∏–º–∫–∏ —Ñ–∞–π–ª–∞: {file_path}")

    # –°–æ–∑–¥–∞–Ω–∏–µ –≤—ã–∂–∏–º–∫–∏
    try:
        summary = summarizer.summarize_file(str(file_path), sentences_count=sentences)

        if not summary:
            click.echo(
                "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≤—ã–∂–∏–º–∫—É. –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:\n"
                "   - –í—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ\n"
                "   - –§–∞–π–ª –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞\n"
                "   –ü–æ–ø—Ä–æ–±—É–π—Ç–µ --no-filter –∏–ª–∏ —É–º–µ–Ω—å—à–∏—Ç–µ --min-length"
            )
            return

        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ –∫–æ–Ω—Å–æ–ª—å
        click.echo(f"\nüìÑ –í—ã–∂–∏–º–∫–∞ —Ñ–∞–π–ª–∞: {file_path.name}")
        click.echo("=" * 60)

        for i, sentence in enumerate(summary, 1):
            click.echo(f"{i}. {sentence.strip()}")
            click.echo()  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –º–µ–∂–¥—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        try:
            from semantic_search.utils.file_utils import FileExtractor

            extractor = FileExtractor()
            original_text = extractor.extract_text(file_path)

            if original_text:
                stats = summarizer.get_summary_statistics(original_text, summary)

                click.echo("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏:")
                click.echo("-" * 30)
                click.echo(
                    f"  üìë –ò—Å—Ö–æ–¥–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {stats['original_sentences_count']}"
                )

                if "valid_original_sentences_count" in stats and not no_filter:
                    filtered = (
                        stats["original_sentences_count"]
                        - stats["valid_original_sentences_count"]
                    )
                    click.echo(f"  üîΩ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –∫–æ—Ä–æ—Ç–∫–∏—Ö: {filtered}")
                    click.echo(
                        f"  ‚úÖ –í–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {stats['valid_original_sentences_count']}"
                    )

                click.echo(
                    f"  üìÑ –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –≤—ã–∂–∏–º–∫–µ: {stats['summary_sentences_count']}"
                )
                click.echo(f"  üìâ –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è: {stats['compression_ratio']:.1%}")
                click.echo(f"  üî§ –ò—Å—Ö–æ–¥–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤: {stats['original_chars_count']:,}")
                click.echo(f"  ‚úÇÔ∏è –°–∏–º–≤–æ–ª–æ–≤ –≤ –≤—ã–∂–∏–º–∫–µ: {stats['summary_chars_count']:,}")
                click.echo(
                    f"  üìä –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞: {stats['chars_compression_ratio']:.1%}"
                )

                if "avg_sentence_length" in stats:
                    click.echo(
                        f"  üìè –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: {stats['avg_sentence_length']:.1f} —Å–ª–æ–≤"
                    )

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
        if output:
            output_path = Path(output)
            try:
                with open(output_path, "w", encoding="utf-8") as f:
                    f.write(f"–í—ã–∂–∏–º–∫–∞ —Ñ–∞–π–ª–∞: {file_path.name}\n")
                    f.write(f"–°–æ–∑–¥–∞–Ω–æ: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                    if not no_filter:
                        f.write(
                            f"–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: –º–∏–Ω. {min_length} —Å–∏–º–≤–æ–ª–æ–≤, {min_words} —Å–ª–æ–≤\n"
                        )
                    f.write("=" * 60 + "\n\n")

                    for i, sentence in enumerate(summary, 1):
                        f.write(f"{i}. {sentence.strip()}\n\n")

                    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ —Ñ–∞–π–ª
                    if "stats" in locals():
                        f.write("\n" + "=" * 60 + "\n")
                        f.write("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–£–ú–ú–ê–†–ò–ó–ê–¶–ò–ò\n")
                        f.write("=" * 60 + "\n")
                        f.write(
                            f"–ò—Å—Ö–æ–¥–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {stats['original_sentences_count']}\n"
                        )

                        if "valid_original_sentences_count" in stats and not no_filter:
                            f.write(
                                f"–í–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {stats['valid_original_sentences_count']}\n"
                            )
                            f.write(
                                f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {stats['original_sentences_count'] - stats['valid_original_sentences_count']}\n"
                            )

                        f.write(
                            f"–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –≤—ã–∂–∏–º–∫–µ: {stats['summary_sentences_count']}\n"
                        )
                        f.write(
                            f"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è: {stats['compression_ratio']:.1%}\n"
                        )
                        f.write(
                            f"–ò—Å—Ö–æ–¥–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤: {stats['original_chars_count']:,}\n"
                        )
                        f.write(
                            f"–°–∏–º–≤–æ–ª–æ–≤ –≤ –≤—ã–∂–∏–º–∫–µ: {stats['summary_chars_count']:,}\n"
                        )
                        f.write(
                            f"–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞: {stats['chars_compression_ratio']:.1%}\n"
                        )

                click.echo(f"üíæ –í—ã–∂–∏–º–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {output_path}")

            except Exception as e:
                click.echo(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {e}")
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤—ã–∂–∏–º–∫–∏: {e}")

    except Exception as e:
        click.echo(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤—ã–∂–∏–º–∫–∏: {e}")
        logger.error(f"–û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")


@cli.command()
@click.option("--text", "-t", required=True, help="–¢–µ–∫—Å—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏")
@click.option("--model", "-m", default="doc2vec_model", help="–ò–º—è Doc2Vec –º–æ–¥–µ–ª–∏")
@click.option("--sentences", "-s", default=5, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –≤—ã–∂–∏–º–∫–µ")
@click.option("--output", "-o", help="–§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—ã–∂–∏–º–∫–∏")
def summarize_text(
    text: str, model: str, sentences: int, output: Optional[str]
) -> None:
    """
    –°–æ–∑–¥–∞—Ç—å –≤—ã–∂–∏–º–∫—É –∏–∑ —Ç–µ–∫—Å—Ç–∞

    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ (—Å—Ç—Ä–æ–∫–∞)
        model: –ò–º—è Doc2Vec –º–æ–¥–µ–ª–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        sentences: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –≤—ã–∂–∏–º–∫–µ
        output: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—ã–∂–∏–º–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    """
    # –ë–∞–∑–æ–≤–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    if not text or len(text.strip()) < 100:
        click.echo("‚ùå –¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ (–º–∏–Ω–∏–º—É–º 100 —Å–∏–º–≤–æ–ª–æ–≤)")
        return

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
    temp_processor = TextSummarizer()
    original_sentences = temp_processor.text_processor.split_into_sentences(text)

    if len(original_sentences) <= sentences:
        click.echo(
            f"‚ö†Ô∏è –í —Ç–µ–∫—Å—Ç–µ –≤—Å–µ–≥–æ {len(original_sentences)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, —á—Ç–æ –º–µ–Ω—å—à–µ –∏–ª–∏ —Ä–∞–≤–Ω–æ –∑–∞–ø—Ä–æ—à–µ–Ω–Ω–æ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É ({sentences})"
        )
        click.echo("–í—ã–≤–æ–¥–∏–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç:")
        for i, sentence in enumerate(original_sentences, 1):
            click.echo(f"{i}. {sentence.strip()}")
        return

    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Doc2Vec
    trainer = Doc2VecTrainer()
    loaded_model = trainer.load_model(model)

    if loaded_model is None:
        click.echo("‚ö†Ô∏è –ú–æ–¥–µ–ª—å Doc2Vec –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∞–∑–æ–≤–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è")
        summarizer = TextSummarizer()
    else:
        click.echo("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Å Doc2Vec")
        summarizer = TextSummarizer(loaded_model)

    logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –≤—ã–∂–∏–º–∫–∏ —Ç–µ–∫—Å—Ç–∞")

    try:
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤—ã–∂–∏–º–∫–∏
        summary = summarizer.summarize_text(text, sentences_count=sentences)

        if not summary:
            click.echo("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≤—ã–∂–∏–º–∫—É")
            return

        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        click.echo("\nüìÑ –í—ã–∂–∏–º–∫–∞ —Ç–µ–∫—Å—Ç–∞:")
        click.echo("=" * 60)

        for i, sentence in enumerate(summary, 1):
            click.echo(f"{i}. {sentence.strip()}")
            click.echo()  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –º–µ–∂–¥—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        stats = summarizer.get_summary_statistics(text, summary)

        click.echo("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏:")
        click.echo("-" * 30)
        click.echo(f"  üìë –ò—Å—Ö–æ–¥–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {stats['original_sentences_count']}")
        click.echo(f"  üìÑ –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –≤—ã–∂–∏–º–∫–µ: {stats['summary_sentences_count']}")
        click.echo(
            f"  üìâ –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {stats['compression_ratio']:.1%}"
        )
        click.echo(f"  üî§ –ò—Å—Ö–æ–¥–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤: {stats['original_chars_count']:,}")
        click.echo(f"  ‚úÇÔ∏è –°–∏–º–≤–æ–ª–æ–≤ –≤ –≤—ã–∂–∏–º–∫–µ: {stats['summary_chars_count']:,}")
        click.echo(f"  üìä –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞: {stats['chars_compression_ratio']:.1%}")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
        if output:
            output_path = Path(output)
            try:
                with open(output_path, "w", encoding="utf-8") as f:
                    f.write("–í—ã–∂–∏–º–∫–∞ —Ç–µ–∫—Å—Ç–∞\n")
                    f.write("=" * 60 + "\n\n")

                    for i, sentence in enumerate(summary, 1):
                        f.write(f"{i}. {sentence.strip()}\n\n")

                    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                    f.write("\n" + "=" * 60 + "\n")
                    f.write("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–£–ú–ú–ê–†–ò–ó–ê–¶–ò–ò\n")
                    f.write("=" * 60 + "\n")
                    f.write(
                        f"–ò—Å—Ö–æ–¥–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {stats['original_sentences_count']}\n"
                    )
                    f.write(
                        f"–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –≤—ã–∂–∏–º–∫–µ: {stats['summary_sentences_count']}\n"
                    )
                    f.write(
                        f"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {stats['compression_ratio']:.1%}\n"
                    )
                    f.write(f"–ò—Å—Ö–æ–¥–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤: {stats['original_chars_count']:,}\n")
                    f.write(f"–°–∏–º–≤–æ–ª–æ–≤ –≤ –≤—ã–∂–∏–º–∫–µ: {stats['summary_chars_count']:,}\n")
                    f.write(
                        f"–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞: {stats['chars_compression_ratio']:.1%}\n"
                    )

                click.echo(f"üíæ –í—ã–∂–∏–º–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {output_path}")

            except Exception as e:
                click.echo(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {e}")
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤—ã–∂–∏–º–∫–∏: {e}")

    except Exception as e:
        click.echo(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤—ã–∂–∏–º–∫–∏: {e}")
        logger.error(f"–û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞: {e}")


@cli.command()
@click.option("--documents", "-d", required=True, help="–ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏")
@click.option("--model", "-m", default="doc2vec_model", help="–ò–º—è Doc2Vec –º–æ–¥–µ–ª–∏")
@click.option(
    "--sentences",
    "-s",
    default=3,
    help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –≤—ã–∂–∏–º–∫–µ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞",
)
@click.option(
    "--min-length", "-l", default=15, help="–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ —Å–∏–º–≤–æ–ª–∞—Ö"
)
@click.option(
    "--min-words", "-w", default=5, help="–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏"
)
@click.option(
    "--no-filter", is_flag=True, help="–û—Ç–∫–ª—é—á–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"
)
@click.option("--output-dir", "-o", help="–ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—ã–∂–∏–º–æ–∫")
@click.option(
    "--extensions", default="pdf,docx,doc", help="–†–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)"
)
@click.option(
    "--max-files",
    default=0,
    help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (0 = –≤—Å–µ)",
)
def summarize_batch(
    documents: str,
    model: str,
    sentences: int,
    min_length: int,
    min_words: int,
    no_filter: bool,
    output_dir: Optional[str],
    extensions: str,
    max_files: int,
) -> None:
    """
    –°–æ–∑–¥–∞—Ç—å –≤—ã–∂–∏–º–∫–∏ –¥–ª—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ø–∞–ø–∫–µ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π

    Args:
        documents: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
        model: –ò–º—è Doc2Vec –º–æ–¥–µ–ª–∏
        sentences: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –∫–∞–∂–¥–æ–π –≤—ã–∂–∏–º–∫–µ
        min_length: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        min_words: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
        no_filter: –û—Ç–∫–ª—é—á–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é
        output_dir: –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—ã–∂–∏–º–æ–∫
        extensions: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
        max_files: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤
    """
    documents_path = Path(documents)
    if not documents_path.exists():
        click.echo(f"‚ùå –ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {documents_path}")
        return

    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Doc2Vec
    trainer = Doc2VecTrainer()
    loaded_model = trainer.load_model(model)

    if loaded_model is None:
        click.echo("‚ö†Ô∏è –ú–æ–¥–µ–ª—å Doc2Vec –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∞–∑–æ–≤–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è")
        summarizer = TextSummarizer()
    else:
        click.echo("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Å Doc2Vec")
        summarizer = TextSummarizer(loaded_model)

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    if not no_filter:
        summarizer.min_summary_sentence_length = min_length
        summarizer.min_words_in_sentence = min_words
        click.echo(f"üìè –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: –º–∏–Ω–∏–º—É–º {min_length} —Å–∏–º–≤–æ–ª–æ–≤ –∏ {min_words} —Å–ª–æ–≤")
    else:
        summarizer.min_summary_sentence_length = 1
        summarizer.min_words_in_sentence = 1
        click.echo("üìã –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞")

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π
    allowed_extensions = {f".{ext.strip().lower()}" for ext in extensions.split(",")}
    click.echo(f"üîç –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º–∏: {allowed_extensions}")

    # –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤
    all_files = []
    for file_path in documents_path.rglob("*"):
        if file_path.is_file() and file_path.suffix.lower() in allowed_extensions:
            all_files.append(file_path)

    if not all_files:
        click.echo(f"‚ùå –§–∞–π–ª—ã —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º–∏ {allowed_extensions} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return

    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ–∞–π–ª–æ–≤
    if max_files > 0 and len(all_files) > max_files:
        all_files = all_files[:max_files]
        click.echo(f"üìÅ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–æ {max_files} —Ñ–∞–π–ª–æ–≤ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö")

    click.echo(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(all_files)}")

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞–ø–∫–∏ –¥–ª—è –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    if output_dir:
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True, parents=True)
        click.echo(f"üíæ –í—ã–∂–∏–º–∫–∏ –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_path}")
    else:
        click.echo("üì∫ –í—ã–∂–∏–º–∫–∏ –±—É–¥—É—Ç –≤—ã–≤–µ–¥–µ–Ω—ã —Ç–æ–ª—å–∫–æ –≤ –∫–æ–Ω—Å–æ–ª—å")

    successful = 0
    failed = 0
    filtered_out = 0

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤
    for i, file_path in enumerate(all_files, 1):
        click.echo(f"\nüîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ {i}/{len(all_files)}: {file_path.name}")

        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ –≤—ã–∂–∏–º–∫–∏
            summary = summarizer.summarize_file(
                str(file_path), sentences_count=sentences
            )

            if not summary:
                click.echo(
                    "   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≤—ã–∂–∏–º–∫—É (–≤–æ–∑–º–æ–∂–Ω–æ, –≤—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ)"
                )
                filtered_out += 1
                continue

            # –ö—Ä–∞—Ç–∫–∏–π –≤—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å
            click.echo(f"   ‚úÖ –°–æ–∑–¥–∞–Ω–∞ –≤—ã–∂–∏–º–∫–∞: {len(summary)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∫–∞–∫ –ø—Ä–µ–≤—å—é
            if summary:
                preview = (
                    summary[0][:100] + "..." if len(summary[0]) > 100 else summary[0]
                )
                click.echo(f"   üëÅÔ∏è –ü—Ä–µ–≤—å—é: {preview}")

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
            if output_dir:
                summary_filename = f"{file_path.stem}_summary.txt"
                summary_path = output_path / summary_filename

                try:
                    with open(summary_path, "w", encoding="utf-8") as f:
                        f.write(f"–í—ã–∂–∏–º–∫–∞ —Ñ–∞–π–ª–∞: {file_path.name}\n")
                        f.write(f"–ò—Å—Ö–æ–¥–Ω—ã–π –ø—É—Ç—å: {file_path}\n")
                        f.write(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {len(summary)}\n")
                        if not no_filter:
                            f.write(
                                f"–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: –º–∏–Ω. {min_length} —Å–∏–º–≤–æ–ª–æ–≤, {min_words} —Å–ª–æ–≤\n"
                            )
                        f.write("=" * 60 + "\n\n")

                        for j, sentence in enumerate(summary, 1):
                            f.write(f"{j}. {sentence.strip()}\n\n")

                    click.echo(f"   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {summary_filename}")
                except Exception as save_error:
                    click.echo(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {save_error}")
                    failed += 1
                    continue

            successful += 1

        except Exception as e:
            click.echo(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_path}: {e}")
            failed += 1

    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    click.echo("\nüìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞–∫–µ—Ç–Ω–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏:")
    click.echo("=" * 50)
    click.echo(f"  ‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {successful}")
    click.echo(f"  ‚ö†Ô∏è –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ (–∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è): {filtered_out}")
    click.echo(f"  ‚ùå –û—à–∏–±–æ–∫: {failed}")
    click.echo(f"  üìÅ –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {len(all_files)}")
    click.echo(f"  üìà –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {(successful / len(all_files) * 100):.1f}%")

    if output_dir and successful > 0:
        click.echo(f"  üíæ –í—ã–∂–∏–º–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_path}")

    if filtered_out > 0 and not no_filter:
        click.echo(
            "\nüí° –°–æ–≤–µ—Ç: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --no-filter –∏–ª–∏ —É–º–µ–Ω—å—à–∏—Ç–µ --min-length –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤ —Å –∫–æ—Ä–æ—Ç–∫–∏–º–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏"
        )


@cli.command()
def status():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –≤—ã–ø–æ–ª–Ω—è—é—â–∏—Ö—Å—è –∑–∞–¥–∞—á"""

    tasks = task_manager.get_all_tasks()

    if not tasks:
        click.echo("üì≠ –ê–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á –Ω–µ—Ç")
        return

    click.echo("üìã –°—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á:")
    click.echo("=" * 60)

    for task in tasks:
        status_icon = {
            "pending": "‚è≥",
            "running": "üîÑ",
            "completed": "‚úÖ",
            "failed": "‚ùå",
            "cancelled": "‚èπÔ∏è",
        }.get(task.status.value, "‚ùì")

        click.echo(f"{status_icon} {task.name}")
        click.echo(f"   ID: {task.id}")
        click.echo(f"   –°—Ç–∞—Ç—É—Å: {task.status.value}")

        if task.progress > 0:
            progress_bar = "‚ñà" * int(task.progress * 20) + "‚ñë" * (
                20 - int(task.progress * 20)
            )
            click.echo(f"   –ü—Ä–æ–≥—Ä–µ—Å—Å: [{progress_bar}] {task.progress:.1%}")

        if task.duration:
            click.echo(f"   –í—Ä–µ–º—è: {task.duration:.1f}—Å")

        if task.error:
            click.echo(f"   –û—à–∏–±–∫–∞: {task.error}")

        click.echo()


@cli.command()
@click.argument("task_id")
def cancel(task_id: str):
    """–û—Ç–º–µ–Ω–∞ –∑–∞–¥–∞—á–∏"""

    if task_manager.cancel_task(task_id):
        click.echo(f"‚úÖ –ó–∞–¥–∞—á–∞ {task_id} –æ—Ç–º–µ–Ω–µ–Ω–∞")
    else:
        click.echo(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–º–µ–Ω–∏—Ç—å –∑–∞–¥–∞—á—É {task_id}")


@cli.command()
@click.option(
    "--max-keep", default=50, help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è"
)
def cleanup(max_keep: int):
    """–û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á"""

    before_count = len(task_manager.get_all_tasks())
    task_manager.cleanup_finished_tasks(max_keep)
    after_count = len(task_manager.get_all_tasks())

    removed = before_count - after_count
    click.echo(f"üßπ –£–¥–∞–ª–µ–Ω–æ {removed} –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á")


@cli.command()
@click.option("--documents", "-d", help="–ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏")
@click.option("--output", "-o", help="–§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞")
def system_info(documents: Optional[str], output: Optional[str]):
    """–°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞"""

    info_lines = []

    # –°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    system_info = performance_monitor.get_system_info()
    info_lines.extend(
        [
            "üñ•Ô∏è –°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:",
            f"   CPU: {system_info['cpu_count']} —è–¥–µ—Ä, –∑–∞–≥—Ä—É–∑–∫–∞ {system_info['cpu_percent']}%",
            f"   –û–ó–£: {system_info['memory_available']:.1f}/{system_info['memory_total']:.1f} –ì–ë —Å–≤–æ–±–æ–¥–Ω–æ",
            f"   –î–∏—Å–∫: {100 - system_info['disk_usage']:.1f}% —Å–≤–æ–±–æ–¥–Ω–æ",
            "",
        ]
    )

    # –°—Ç–∞—Ç—É—Å SpaCy
    spacy_info = check_spacy_model_availability()
    spacy_status = "‚úÖ –ì–æ—Ç–æ–≤" if spacy_info["model_loadable"] else "‚ùå –ù–µ –≥–æ—Ç–æ–≤"
    info_lines.extend(
        [
            "üß† –Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å:",
            f"   SpaCy: {spacy_status}",
            f"   –ú–æ–¥–µ–ª—å: {SPACY_MODEL}",
            "",
        ]
    )

    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
    if documents:
        try:
            docs_path = Path(documents)
            if docs_path.exists():
                file_extractor = FileExtractor()
                found_files = file_extractor.find_documents(docs_path)

                # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤
                valid_files = 0
                invalid_files = 0
                total_size = 0

                for file_path in found_files:
                    validation = FileValidator.validate_document_file(file_path)
                    if validation["valid"]:
                        valid_files += 1
                        total_size += validation["file_info"]["size"]
                    else:
                        invalid_files += 1

                info_lines.extend(
                    [
                        "üìÅ –ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:",
                        f"   –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {len(found_files)}",
                        f"   –í–∞–ª–∏–¥–Ω—ã—Ö: {valid_files}",
                        f"   –° –æ—à–∏–±–∫–∞–º–∏: {invalid_files}",
                        f"   –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size / 1024 / 1024:.1f} –ú–ë",
                        "",
                    ]
                )

        except Exception as e:
            info_lines.extend(["üìÅ –ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:", f"   ‚ùå –û—à–∏–±–∫–∞: {e}", ""])

    # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
    if performance_monitor.metrics:
        info_lines.extend(
            [
                "‚ö° –ü–æ—Å–ª–µ–¥–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏:",
            ]
        )

        for op_name, metrics in list(performance_monitor.metrics.items())[-5:]:
            info_lines.append(f"   {op_name}: {metrics['duration']:.2f}—Å")

        info_lines.append("")

    # –ê–∫—Ç–∏–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏
    running_tasks = task_manager.get_running_tasks()
    if running_tasks:
        info_lines.extend(
            [
                "üîÑ –ê–∫—Ç–∏–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏:",
            ]
        )

        for task in running_tasks:
            info_lines.append(f"   {task.name}: {task.progress:.1%}")

        info_lines.append("")

    report = "\n".join(info_lines)

    # –í—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å
    click.echo(report)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
    if output:
        try:
            with open(output, "w", encoding="utf-8") as f:
                f.write(f"–°–∏—Å—Ç–µ–º–Ω—ã–π –æ—Ç—á–µ—Ç - {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("=" * 60 + "\n\n")
                f.write(report)

            click.echo(f"üíæ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output}")

        except Exception as e:
            click.echo(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {e}")


@cli.command()
@click.option("--show", is_flag=True, help="–ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ–∫—É—â—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é")
@click.option("--reset", is_flag=True, help="–°–±—Ä–æ—Å–∏—Ç—å –∫ –∑–Ω–∞—á–µ–Ω–∏—è–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
@click.option("--reload", is_flag=True, help="–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ —Ñ–∞–π–ª–∞")
@click.option(
    "--set", nargs=2, multiple=True, help="–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä: --set key value"
)
def config(show: bool, reset: bool, reload: bool, set: tuple):
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""

    from semantic_search.config import config_manager

    if reset:
        if click.confirm(
            "–í—ã —É–≤–µ—Ä–µ–Ω—ã, —á—Ç–æ —Ö–æ—Ç–∏—Ç–µ —Å–±—Ä–æ—Å–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∫ –∑–Ω–∞—á–µ–Ω–∏—è–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é?"
        ):
            config_manager.reset_to_defaults()
            click.echo("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–±—Ä–æ—à–µ–Ω–∞ –∫ –∑–Ω–∞—á–µ–Ω–∏—è–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
        else:
            click.echo("‚ùå –°–±—Ä–æ—Å –æ—Ç–º–µ–Ω–µ–Ω")
        return

    if reload:
        config_manager.reload_config()
        click.echo("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ —Ñ–∞–π–ª–∞")

    if set:
        for key, value in set:
            # –ü—ã—Ç–∞–µ–º—Å—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ç–∏–ø
            try:
                # –ß–∏—Å–ª–∞
                if value.isdigit():
                    value = int(value)
                elif value.replace(".", "", 1).isdigit():
                    value = float(value)
                # –ë—É–ª–µ–≤—ã –∑–Ω–∞—á–µ–Ω–∏—è
                elif value.lower() in ("true", "false"):
                    value = value.lower() == "true"
                # –ß–∏—Å–ª–∞ —Å –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è–º–∏
                elif "_" in value and value.replace("_", "").isdigit():
                    value = int(value.replace("_", ""))

            except Exception:
                pass  # –û—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ —Å—Ç—Ä–æ–∫—É

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ–∫—Ü–∏—é –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä
            if "." in key:
                section, param = key.split(".", 1)
                config_manager.update_config(**{section: {param: value}})
                click.echo(f"‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ: {section}.{param} = {value}")
            else:
                click.echo("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∫–ª—é—á–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: section.parameter")

    if show or (not reset and not reload and not set):
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–µ–∫—É—â—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        current_config = config_manager.config

        click.echo("\nüìã –¢–µ–∫—É—â–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
        click.echo("=" * 60)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        click.echo("\nüìù –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ (text_processing):")
        for key, value in current_config.text_processing.items():
            if isinstance(value, int) and value > 1000:
                click.echo(f"  {key}: {value:,}")
            else:
                click.echo(f"  {key}: {value}")

        # Doc2Vec
        click.echo("\nüß† –ü–∞—Ä–∞–º–µ—Ç—Ä—ã Doc2Vec (doc2vec):")
        for key, value in current_config.doc2vec.items():
            click.echo(f"  {key}: {value}")

        # –ü–æ–∏—Å–∫
        click.echo("\nüîç –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞ (search):")
        for key, value in current_config.search.items():
            click.echo(f"  {key}: {value}")

        # GUI
        click.echo("\nüíª –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ (gui):")
        for key, value in current_config.gui.items():
            click.echo(f"  {key}: {value}")

        # –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
        click.echo("\nüìÑ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ (summarization):")
        for key, value in current_config.summarization.items():
            click.echo(f"  {key}: {value}")

        click.echo("\nüí° –ü—Ä–∏–º–µ—Ä—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:")
        click.echo(
            "  semantic-search-cli config --set text_processing.max_text_length 10000000"
        )
        click.echo("  semantic-search-cli config --set doc2vec.vector_size 200")
        click.echo("  semantic-search-cli config --set search.default_top_k 20")


"""–î–æ–±–∞–≤–∏—Ç—å –≤ main.py –ø–æ—Å–ª–µ –¥—Ä—É–≥–∏—Ö CLI –∫–æ–º–∞–Ω–¥"""


@cli.command()
@click.option("--model", "-m", default="doc2vec_model", help="–ò–º—è Doc2Vec –º–æ–¥–µ–ª–∏")
@click.option("--openai-key", envvar="OPENAI_API_KEY", help="OpenAI API key")
@click.option(
    "--test-cases",
    type=click.Choice(["quick", "standard", "extended"]),
    default="standard",
    help="–ù–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤",
)
@click.option("--output-dir", "-o", help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
def evaluate(model: str, openai_key: str, test_cases: str, output_dir: Optional[str]):
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Doc2Vec —Å OpenAI embeddings

    Args:
        model: –ò–º—è Doc2Vec –º–æ–¥–µ–ª–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        openai_key: API –∫–ª—é—á OpenAI (–∏–ª–∏ –∏–∑ OPENAI_API_KEY)
        test_cases: –†–∞–∑–º–µ—Ä –Ω–∞–±–æ—Ä–∞ —Ç–µ—Å—Ç–æ–≤ (quick/standard/extended)
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    if not openai_key:
        click.echo("‚ùå OpenAI API key –Ω–µ –Ω–∞–π–¥–µ–Ω")
        click.echo(
            "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è OPENAI_API_KEY –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ --openai-key"
        )
        return

    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    click.echo(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model}...")
    trainer = Doc2VecTrainer()
    loaded_model = trainer.load_model(model)

    if not loaded_model:
        click.echo(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å '{model}'")
        return

    if not trainer.corpus_info:
        click.echo("‚ùå –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ—Ä–ø—É—Å–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return

    # –ò–º–ø–æ—Ä—Ç –º–æ–¥—É–ª–µ–π –æ—Ü–µ–Ω–∫–∏
    from semantic_search.core.search_engine import SemanticSearchEngine
    from semantic_search.evaluation.baselines import (
        Doc2VecSearchAdapter,
        OpenAISearchBaseline,
    )
    from semantic_search.evaluation.comparison import SearchComparison

    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞
    search_engine = SemanticSearchEngine(loaded_model, trainer.corpus_info)

    # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    comparison = SearchComparison()

    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤
    click.echo("üß™ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤...")
    default_cases = comparison.create_default_test_cases()

    if test_cases == "quick":
        test_cases_list = default_cases[:3]
        click.echo("   –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç: 3 –∑–∞–ø—Ä–æ—Å–∞")
    elif test_cases == "extended":
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã
        from semantic_search.evaluation.comparison import QueryTestCase

        extra_cases = [
            QueryTestCase(
                query="–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π CNN",
                relevant_docs={"cnn_tutorial.pdf", "image_classification.pdf"},
                relevance_scores={"cnn_tutorial.pdf": 3, "image_classification.pdf": 3},
            ),
            QueryTestCase(
                query="—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è dropout L1 L2",
                relevant_docs={"regularization.pdf", "overfitting.pdf"},
                relevance_scores={"regularization.pdf": 3, "overfitting.pdf": 2},
            ),
            QueryTestCase(
                query="word embeddings word2vec GloVe",
                relevant_docs={"word2vec_paper.pdf", "embeddings_tutorial.pdf"},
                relevance_scores={
                    "word2vec_paper.pdf": 3,
                    "embeddings_tutorial.pdf": 3,
                },
            ),
            QueryTestCase(
                query="precision recall F1 score ROC AUC",
                relevant_docs={"ml_metrics.pdf", "evaluation_methods.pdf"},
                relevance_scores={"ml_metrics.pdf": 3, "evaluation_methods.pdf": 3},
            ),
            QueryTestCase(
                query="backpropagation gradient descent",
                relevant_docs={"backpropagation.pdf", "optimization.pdf"},
                relevance_scores={"backpropagation.pdf": 3, "optimization.pdf": 2},
            ),
        ]
        test_cases_list = default_cases + extra_cases
        click.echo("   –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ç–µ—Å—Ç: 10 –∑–∞–ø—Ä–æ—Å–æ–≤")
    else:  # standard
        test_cases_list = default_cases
        click.echo("   –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ç–µ—Å—Ç: 5 –∑–∞–ø—Ä–æ—Å–æ–≤")

    comparison.test_cases = test_cases_list

    # –°–æ–∑–¥–∞–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤
    click.echo("\nüîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞...")
    doc2vec_adapter = Doc2VecSearchAdapter(search_engine, trainer.corpus_info)

    try:
        openai_baseline = OpenAISearchBaseline(api_key=openai_key)
        click.echo("‚úÖ OpenAI baseline –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    except Exception as e:
        click.echo(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ OpenAI: {e}")
        return

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
    click.echo("\nüìö –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è OpenAI...")
    documents = []
    max_docs = min(50, len(trainer.corpus_info))  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏

    for i, (tokens, doc_id, metadata) in enumerate(trainer.corpus_info[:max_docs]):
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Ç–æ–∫–µ–Ω–æ–≤
        text = " ".join(tokens[:500])  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 500 —Ç–æ–∫–µ–Ω–æ–≤
        documents.append((doc_id, text, metadata))

    click.echo(f"   –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–ª—è OpenAI
    click.echo("\nüîÑ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ OpenAI API...")
    click.echo("   (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç)")

    try:
        with click.progressbar(length=100, label="–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è") as bar:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º callback –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            original_index = openai_baseline.index

            def index_with_progress(docs):
                result = original_index(docs)
                bar.update(100)
                return result

            openai_baseline.index = index_with_progress
            openai_baseline.index(documents)
            openai_baseline.index = original_index

        click.echo("‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    except Exception as e:
        click.echo(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")
        return

    # –û—Ü–µ–Ω–∫–∞ –º–µ—Ç–æ–¥–æ–≤
    click.echo("\nüìä –û—Ü–µ–Ω–∫–∞ –º–µ—Ç–æ–¥–æ–≤...")

    # Doc2Vec
    click.echo("\n1Ô∏è‚É£ –û—Ü–µ–Ω–∫–∞ Doc2Vec...")
    doc2vec_results = comparison.evaluate_method(
        doc2vec_adapter, top_k=10, verbose=False
    )
    click.echo(f"   MAP: {doc2vec_results['aggregated']['MAP']:.3f}")
    click.echo(f"   MRR: {doc2vec_results['aggregated']['MRR']:.3f}")
    click.echo(
        f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∑–∞–ø—Ä–æ—Å–∞: {doc2vec_results['aggregated']['avg_query_time']:.3f}—Å"
    )

    # OpenAI
    click.echo("\n2Ô∏è‚É£ –û—Ü–µ–Ω–∫–∞ OpenAI embeddings...")
    openai_results = comparison.evaluate_method(
        openai_baseline, top_k=10, verbose=False
    )
    click.echo(f"   MAP: {openai_results['aggregated']['MAP']:.3f}")
    click.echo(f"   MRR: {openai_results['aggregated']['MRR']:.3f}")
    click.echo(
        f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∑–∞–ø—Ä–æ—Å–∞: {openai_results['aggregated']['avg_query_time']:.3f}—Å"
    )

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    click.echo("\nüìà –°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    click.echo("=" * 60)

    # MAP —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    doc2vec_map = doc2vec_results["aggregated"]["MAP"]
    openai_map = openai_results["aggregated"]["MAP"]
    map_improvement = (
        ((doc2vec_map - openai_map) / openai_map * 100) if openai_map > 0 else 0
    )

    click.echo("\nüìä Mean Average Precision (MAP):")
    click.echo(f"   Doc2Vec: {doc2vec_map:.3f}")
    click.echo(f"   OpenAI:  {openai_map:.3f}")

    if map_improvement > 0:
        click.echo(f"   ‚úÖ Doc2Vec –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç OpenAI –Ω–∞ {map_improvement:.1f}%")
    else:
        click.echo(f"   ‚ùå OpenAI –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Doc2Vec –Ω–∞ {-map_improvement:.1f}%")

    # –°–∫–æ—Ä–æ—Å—Ç—å
    doc2vec_time = doc2vec_results["aggregated"]["avg_query_time"]
    openai_time = openai_results["aggregated"]["avg_query_time"]
    speed_ratio = openai_time / doc2vec_time if doc2vec_time > 0 else float("inf")

    click.echo("\n‚ö° –°–∫–æ—Ä–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞:")
    click.echo(f"   Doc2Vec: {doc2vec_time:.3f}—Å –Ω–∞ –∑–∞–ø—Ä–æ—Å")
    click.echo(f"   OpenAI:  {openai_time:.3f}—Å –Ω–∞ –∑–∞–ø—Ä–æ—Å")
    click.echo(f"   ‚úÖ Doc2Vec –±—ã—Å—Ç—Ä–µ–µ –≤ {speed_ratio:.1f} —Ä–∞–∑")

    # –î—Ä—É–≥–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    click.echo("\nüìè –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:")

    for metric in ["precision@10", "recall@10", "f1@10"]:
        doc2vec_val = doc2vec_results["aggregated"].get(f"avg_{metric}", 0)
        openai_val = openai_results["aggregated"].get(f"avg_{metric}", 0)
        click.echo(f"   {metric.upper()}:")
        click.echo(f"      Doc2Vec: {doc2vec_val:.3f}")
        click.echo(f"      OpenAI:  {openai_val:.3f}")

    # –≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
    click.echo("\nüí∞ –≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å:")

    # –†–∞—Å—á–µ—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç–∏
    queries_per_day = 1000
    avg_tokens_per_query = 50
    openai_cost_per_1k_tokens = 0.0001  # $0.0001 –∑–∞ 1K —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è ada-002

    daily_queries_cost = (
        queries_per_day * avg_tokens_per_query / 1000
    ) * openai_cost_per_1k_tokens
    # –°—Ç–æ–∏–º–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (–ø—Ä–∏–º–µ—Ä–Ω–æ 200 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç)
    indexing_cost = (len(documents) * 200 / 1000) * openai_cost_per_1k_tokens

    monthly_cost = daily_queries_cost * 30 + indexing_cost
    yearly_cost = monthly_cost * 12

    click.echo(f"   –ü—Ä–∏ {queries_per_day} –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –¥–µ–Ω—å:")
    click.echo(
        f"   - –°—Ç–æ–∏–º–æ—Å—Ç—å OpenAI: ~${monthly_cost:.2f}/–º–µ—Å—è—Ü (${yearly_cost:.2f}/–≥–æ–¥)"
    )
    click.echo("   - –°—Ç–æ–∏–º–æ—Å—Ç—å Doc2Vec: $0 (–ø–æ—Å–ª–µ –µ–¥–∏–Ω–æ—Ä–∞–∑–æ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è)")
    click.echo(f"   - üíµ –≠–∫–æ–Ω–æ–º–∏—è: ${yearly_cost:.2f} –≤ –≥–æ–¥")

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤
    click.echo("\nüìÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤ –∏ –≥—Ä–∞—Ñ–∏–∫–æ–≤...")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if output_dir:
        results_dir = Path(output_dir)
    else:
        from semantic_search.config import EVALUATION_RESULTS_DIR

        results_dir = EVALUATION_RESULTS_DIR

    results_dir.mkdir(exist_ok=True, parents=True)

    # –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
    comparison.compare_methods([doc2vec_adapter, openai_baseline], save_results=True)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
    try:
        comparison.plot_comparison(save_plots=True)
        click.echo("‚úÖ –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
    except Exception as e:
        click.echo(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏: {e}")

    # –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç
    report_path = results_dir / "comparison_report.txt"
    comparison.generate_report(report_path)

    click.echo(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {results_dir}")
    click.echo("   üìä comparison_results.csv - —Ç–∞–±–ª–∏—Ü–∞ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏")
    click.echo("   üìù comparison_report.txt - —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç")
    click.echo("   üìà plots/ - –≥—Ä–∞—Ñ–∏–∫–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
    click.echo("   üóÇÔ∏è detailed_results.json - –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")

    # –û—Å–Ω–æ–≤–Ω—ã–µ –≤—ã–≤–æ–¥—ã
    click.echo("\nüéØ –û–°–ù–û–í–ù–´–ï –í–´–í–û–î–´:")
    click.echo("=" * 60)

    if map_improvement > 0:
        click.echo(
            f"‚úÖ Doc2Vec –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –õ–£–ß–®–ï–ï –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ (+{map_improvement:.1f}% MAP)"
        )
        click.echo("   –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    else:
        click.echo(
            f"‚ö†Ô∏è OpenAI –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ (+{-map_improvement:.1f}% MAP)"
        )
        click.echo("   –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å Doc2Vec")

    click.echo(f"\n‚úÖ Doc2Vec —Ä–∞–±–æ—Ç–∞–µ—Ç –ó–ù–ê–ß–ò–¢–ï–õ–¨–ù–û –ë–´–°–¢–†–ï–ï (–≤ {speed_ratio:.1f} —Ä–∞–∑)")
    click.echo("   –∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è")

    click.echo(f"\n‚úÖ Doc2Vec –≠–ö–û–ù–û–ú–ò–ß–ï–°–ö–ò –í–´–ì–û–î–ù–ï–ï (—ç–∫–æ–Ω–æ–º–∏—è ${yearly_cost:.0f}/–≥–æ–¥)")
    click.echo("   –ø—Ä–∏ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏")

    click.echo("\nüìå –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: Doc2Vec –æ–ø—Ç–∏–º–∞–ª–µ–Ω –¥–ª—è:")
    click.echo("   ‚Ä¢ –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ—Ä–ø—É—Å–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    click.echo("   ‚Ä¢ –í—ã—Å–æ–∫–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏ (–º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤)")
    click.echo("   ‚Ä¢ –†–∞–±–æ—Ç—ã –±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞")
    click.echo("   ‚Ä¢ –ö–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")


def cli_mode():
    """–ó–∞–ø—É—Å–∫ CLI —Ä–µ–∂–∏–º–∞"""
    cli()


if __name__ == "__main__":
    if len(sys.argv) > 1:
        cli_mode()
    else:
        main()


========================================
FILE: src\semantic_search\core\__init__.py
========================================
"""–û—Å–Ω–æ–≤–Ω—ã–µ –º–æ–¥—É–ª–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –∏ –ø–æ–∏—Å–∫–æ–º"""

from .doc2vec_trainer import Doc2VecTrainer
from .document_processor import DocumentProcessor, ProcessedDocument
from .search_engine import SearchResult, SemanticSearchEngine
from .text_summarizer import TextSummarizer

__all__ = [
    "Doc2VecTrainer",
    "DocumentProcessor",
    "ProcessedDocument",
    "SemanticSearchEngine",
    "SearchResult",
    "TextSummarizer",
]


========================================
FILE: src\semantic_search\core\doc2vec_trainer.py
========================================
"""–ú–æ–¥—É–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ Doc2Vec"""

from __future__ import annotations

import json
import pickle
from pathlib import Path
from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple

from loguru import logger

if TYPE_CHECKING:
    from gensim.models.doc2vec import Doc2Vec, TaggedDocument
try:
    from gensim.models.doc2vec import Doc2Vec, TaggedDocument

    GENSIM_AVAILABLE = True
except ImportError:
    logger.error("Gensim –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install gensim")
    GENSIM_AVAILABLE = False

from semantic_search.config import DOC2VEC_CONFIG, MODELS_DIR


class Doc2VecTrainer:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—å—é Doc2Vec"""

    def __init__(self):
        self.model: Optional[Doc2Vec] = None
        self.config = DOC2VEC_CONFIG
        self.corpus_info: Optional[List[Tuple[List[str], str, dict]]] = None
        self.training_metadata: Dict[str, Any] = {}
        self.documents_base_path: Optional[Path] = None

    def create_tagged_documents(
        self, corpus: List[Tuple[List[str], str, dict]]
    ) -> List[TaggedDocument]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ TaggedDocument –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è gensim

        Args:
            corpus: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (tokens, doc_id, metadata)

        Returns:
            –°–ø–∏—Å–æ–∫ TaggedDocument –æ–±—ä–µ–∫—Ç–æ–≤
        """
        if not GENSIM_AVAILABLE:
            raise ImportError("Gensim –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")

        tagged_docs = [
            TaggedDocument(words=tokens, tags=[doc_id]) for tokens, doc_id, _ in corpus
        ]
        logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(tagged_docs)} TaggedDocument –æ–±—ä–µ–∫—Ç–æ–≤")
        return tagged_docs

    def _get_training_params(
        self,
        vector_size: Optional[int],
        window: Optional[int],
        min_count: Optional[int],
        epochs: Optional[int],
        workers: Optional[int],
        dm: Optional[int],
        negative: Optional[int],
        hs: Optional[int],
        sample: Optional[float],
        dm_concat: Optional[int] = None,
        dm_mean: Optional[int] = None,
        alpha: Optional[float] = None,
        min_alpha: Optional[float] = None,
    ) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –Ω–æ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        params = {
            "vector_size": vector_size or self.config["vector_size"],
            "window": window or self.config["window"],
            "min_count": min_count or self.config["min_count"],
            "epochs": epochs or self.config["epochs"],
            "workers": workers or self.config["workers"],
            "seed": self.config["seed"],
            "dm": dm if dm is not None else self.config.get("dm", 1),
            "negative": negative
            if negative is not None
            else self.config.get("negative", 10),
            "hs": hs if hs is not None else self.config.get("hs", 0),
            "sample": sample if sample is not None else self.config.get("sample", 1e-5),
        }

        # –ù–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if dm_concat is not None:
            params["dm_concat"] = dm_concat
        elif "dm_concat" in self.config:
            params["dm_concat"] = self.config["dm_concat"]

        if dm_mean is not None:
            params["dm_mean"] = dm_mean
        elif "dm_mean" in self.config:
            params["dm_mean"] = self.config["dm_mean"]

        if alpha is not None:
            params["alpha"] = alpha
        elif "alpha" in self.config:
            params["alpha"] = self.config["alpha"]

        if min_alpha is not None:
            params["min_alpha"] = min_alpha
        elif "min_alpha" in self.config:
            params["min_alpha"] = self.config["min_alpha"]

        return params

    def _train_standard(
        self,
        corpus: List[Tuple[List[str], str, dict]],
        vector_size: Optional[int] = None,
        window: Optional[int] = None,
        min_count: Optional[int] = None,
        epochs: Optional[int] = None,
        workers: Optional[int] = None,
        dm: Optional[int] = None,
        negative: Optional[int] = None,
        hs: Optional[int] = None,
        sample: Optional[float] = None,
    ) -> Optional[Doc2Vec]:
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Doc2Vec

        Args:
            corpus: –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å
            vector_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ config)
            window: –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            min_count: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤–∞
            epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
            workers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤
            dm: Distributed Memory (1) –∏–ª–∏ Distributed Bag of Words (0)
            negative: –†–∞–∑–º–µ—Ä negative sampling (–µ—Å–ª–∏ hs=0)
            hs: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Hierarchical Softmax (1) –∏–ª–∏ negative sampling (0)
            sample: –ü–æ—Ä–æ–≥ –¥–ª—è downsampling –≤—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã—Ö —Å–ª–æ–≤

        Returns:
            –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Doc2Vec –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        if not GENSIM_AVAILABLE:
            logger.error("Gensim –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")
            return None
        if not corpus:
            logger.error("–ö–æ—Ä–ø—É—Å –ø—É—Å—Ç, –æ–±—É—á–µ–Ω–∏–µ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ")
            return None

        params = self._get_training_params(
            vector_size, window, min_count, epochs, workers, dm, negative, hs, sample
        )

        logger.info("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
        tagged_docs = self.create_tagged_documents(corpus)

        logger.info("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Doc2Vec —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:")
        for k, v in params.items():
            logger.info(f"  {k}: {v}")

        try:
            model = Doc2Vec(tagged_docs, **params)

            self.model = model
            self.corpus_info = corpus

            logger.info("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
            logger.info(
                f"–°–ª–æ–≤–∞—Ä—å —Å–æ–¥–µ—Ä–∂–∏—Ç {len(model.wv.key_to_index)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤"
            )
            logger.info(f"–û–±—É—á–µ–Ω–æ –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(model.dv)}")

            return model

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
            return None

    def train_model(
        self,
        corpus: List[Tuple[List[str], str, dict]],
        vector_size: Optional[int] = None,
        window: Optional[int] = None,
        min_count: Optional[int] = None,
        epochs: Optional[int] = None,
        workers: Optional[int] = None,
        dm: Optional[int] = None,
        negative: Optional[int] = None,
        hs: Optional[int] = None,
        sample: Optional[float] = None,
        dm_concat: Optional[int] = None,
        dm_mean: Optional[int] = None,
        alpha: Optional[float] = None,
        min_alpha: Optional[float] = None,
        preset: Optional[str] = None,
    ) -> Optional[Doc2Vec]:
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Doc2Vec —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–æ–¥ –æ–±—ä—ë–º –∫–æ—Ä–ø—É—Å–∞.

        –î–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –∫–æ—Ä–ø—É—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ.
        –î–ª—è –±–æ–ª—å—à–∏—Ö –∫–æ—Ä–ø—É—Å–æ–≤ (> 10 000 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤) –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –ø–æ—ç–ø–æ—Ö–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ.

        Args:
            corpus: –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å, –≥–¥–µ –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç ‚Äî –∫–æ—Ä—Ç–µ–∂ (—Ç–æ–∫–µ–Ω—ã, —Ç–µ–≥, –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)
            vector_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ self.config)
            window: –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ self.config)
            min_count: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ self.config)
            epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ self.config)
            workers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ self.config)
            dm: Distributed Memory (1) –∏–ª–∏ Distributed Bag of Words (0)
            negative: –†–∞–∑–º–µ—Ä negative sampling
            hs: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Hierarchical Softmax
            sample: –ü–æ—Ä–æ–≥ –¥–ª—è downsampling
            preset: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ—Å–µ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–∫ ('fast', 'balanced', 'quality')

        Returns:
            –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Doc2Vec –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        if not GENSIM_AVAILABLE:
            logger.error("Gensim –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")
            return None
        if not corpus:
            logger.error("–ö–æ—Ä–ø—É—Å –ø—É—Å—Ç, –æ–±—É—á–µ–Ω–∏–µ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ")
            return None

        if preset and preset in self.config.get("doc2vec_presets", {}):
            preset_config: Dict = self.config["doc2vec_presets"][preset]
            logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–µ—Å–µ—Ç '{preset}'")

            # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–µ—Å–µ—Ç–∞ (–µ—Å–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä –Ω–µ —É–∫–∞–∑–∞–Ω —è–≤–Ω–æ)
            vector_size = vector_size or preset_config.get("vector_size")
            window = window or preset_config.get("window")
            min_count = min_count or preset_config.get("min_count")
            epochs = epochs or preset_config.get("epochs")
            negative = negative or preset_config.get("negative")
            sample = sample or preset_config.get("sample")

        if len(corpus) > 10000:
            # –î–ª—è –±–æ–ª—å—à–∏—Ö –∫–æ—Ä–ø—É—Å–æ–≤ - –ø–æ—ç–ø–æ—Ö–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ
            logger.info("–ë–æ–ª—å—à–æ–π –∫–æ—Ä–ø—É—Å –æ–±–Ω–∞—Ä—É–∂–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—ç–ø–æ—Ö–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ...")

            params = self._get_training_params(
                vector_size,
                window,
                min_count,
                epochs,
                workers,
                dm,
                negative,
                hs,
                sample,
            )

            logger.info("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
            tagged_docs = self.create_tagged_documents(corpus)

            logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:")
            for k, v in params.items():
                logger.info(f"  {k}: {v}")

            try:
                model = Doc2Vec(**params)
                model.build_vocab(tagged_docs)
                logger.info(f"–°–ª–æ–≤–∞—Ä—å –ø–æ—Å—Ç—Ä–æ–µ–Ω: {len(model.wv.key_to_index)} —Å–ª–æ–≤")

                for epoch in range(params["epochs"]):
                    logger.info(f"–≠–ø–æ—Ö–∞ {epoch + 1}/{params['epochs']}...")
                    model.train(
                        tagged_docs, total_examples=model.corpus_count, epochs=1
                    )

                self.model = model
                self.corpus_info = corpus

                logger.info("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
                logger.info(
                    f"–°–ª–æ–≤–∞—Ä—å —Å–æ–¥–µ—Ä–∂–∏—Ç {len(model.wv.key_to_index)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤"
                )
                logger.info(f"–û–±—É—á–µ–Ω–æ –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(model.dv)}")

                return model

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
                return None
        else:
            # –î–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –∫–æ—Ä–ø—É—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
            return self._train_standard(
                corpus,
                vector_size=vector_size,
                window=window,
                min_count=min_count,
                epochs=epochs,
                workers=workers,
                dm=dm,
                negative=negative,
                hs=hs,
                sample=sample,
            )

    def save_model(
        self, model: Optional[Doc2Vec] = None, model_name: str = "doc2vec_model"
    ) -> bool:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∏—Å–∫

        Args:
            model: –ú–æ–¥–µ–ª—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é self.model)
            model_name: –ò–º—è —Ñ–∞–π–ª–∞ –º–æ–¥–µ–ª–∏

        Returns:
            True –µ—Å–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ
        """
        model_to_save = model or self.model

        if model_to_save is None:
            logger.error("–ù–µ—Ç –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return False

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∏–º—è –º–æ–¥–µ–ª–∏ –Ω–µ –ø—É—Å—Ç–æ–µ
        if not model_name or model_name.strip() == "":
            logger.error("–ò–º—è –º–æ–¥–µ–ª–∏ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
            return False

        try:
            if model_name.endswith(".model"):
                model_name = model_name[:-6]
            model_path = MODELS_DIR / f"{model_name}.model"
            model_to_save.save(str(model_path))

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–∫–∂–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ—Ä–ø—É—Å–µ
            if self.corpus_info:
                corpus_path = MODELS_DIR / f"{model_name}_corpus_info.pkl"
                with open(corpus_path, "wb") as f:
                    pickle.dump(self.corpus_info, f)
                logger.info(f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ—Ä–ø—É—Å–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {corpus_path}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
            if self.training_metadata:
                metadata_path = MODELS_DIR / f"{model_name}_metadata.json"
                with open(metadata_path, "w", encoding="utf-8") as f:
                    json.dump(self.training_metadata, f, indent=2, ensure_ascii=False)
                logger.info(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_path}")

            logger.info(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
            return True

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False

    def load_model(self, model_name: str = "doc2vec_model") -> Optional[Doc2Vec]:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –¥–∏—Å–∫–∞

        Args:
            model_name: –ò–º—è —Ñ–∞–π–ª–∞ –º–æ–¥–µ–ª–∏

        Returns:
            –ó–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Doc2Vec –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        if not GENSIM_AVAILABLE:
            logger.error("Gensim –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏")
            return None

        if not model_name or model_name.strip() == "":
            logger.error("–ò–º—è –º–æ–¥–µ–ª–∏ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
            return None

        try:
            if model_name.endswith(".model"):
                model_name = model_name[:-6]

            model_path = MODELS_DIR / f"{model_name}.model"

            if not model_path.exists():
                logger.error(f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_path}")
                return None

            model = Doc2Vec.load(str(model_path))
            self.model = model

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ—Ä–ø—É—Å–µ –µ—Å–ª–∏ –µ—Å—Ç—å
            corpus_path = MODELS_DIR / f"{model_name}_corpus_info.pkl"
            if corpus_path.exists():
                try:
                    with open(corpus_path, "rb") as f:
                        self.corpus_info = pickle.load(f)
                    logger.info("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ—Ä–ø—É—Å–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ—Ä–ø—É—Å–µ: {e}")
                    self.corpus_info = []

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è –µ—Å–ª–∏ –µ—Å—Ç—å
            metadata_path = MODELS_DIR / f"{model_name}_metadata.json"
            if metadata_path.exists():
                try:
                    with open(metadata_path, "r", encoding="utf-8") as f:
                        self.training_metadata = json.load(f)
                    logger.info("–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—ã–π –ø—É—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                    if "documents_base_path" in self.training_metadata:
                        self.documents_base_path = Path(
                            self.training_metadata["documents_base_path"]
                        )
                        logger.info(
                            f"–ë–∞–∑–æ–≤—ã–π –ø—É—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {self.documents_base_path}"
                        )

                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø—É—Ç–∏
                        if not self.documents_base_path.exists():
                            logger.warning(
                                f"–ë–∞–∑–æ–≤—ã–π –ø—É—Ç—å –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {self.documents_base_path}"
                            )
                    else:
                        logger.warning("–ë–∞–∑–æ–≤—ã–π –ø—É—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö")

                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è: {e}")
                    self.training_metadata = {}

            logger.info(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path}")
            logger.info(f"–í–µ–∫—Ç–æ—Ä–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(model.dv)}")
            logger.info(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤: {model.vector_size}")

            return model

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            return None

    def get_model_info(self) -> dict:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –º–æ–¥–µ–ª–∏
        """
        if self.model is None:
            return {"status": "no_model"}

        info = {
            "status": "loaded",
            "vector_size": self.model.vector_size,
            "vocabulary_size": len(self.model.wv.key_to_index),
            "documents_count": len(self.model.dv),
            "window": self.model.window,
            "min_count": self.model.min_count,
            "epochs": self.model.epochs,
            "dm": self.model.dm,
            "dm_mean": getattr(self.model, "dm_mean", None),
            "dm_concat": getattr(self.model, "dm_concat", None),
            "negative": self.model.negative,
            "hs": self.model.hs,
            "sample": self.model.sample,
            "workers": self.model.workers,
        }

        info["training_time_formatted"] = self.training_metadata.get(
            "training_time_formatted", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
        )

        info["training_date"] = self.training_metadata.get(
            "training_date", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
        )

        return info


========================================
FILE: src\semantic_search\core\document_processor.py
========================================
"""–û—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥—É–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""

from pathlib import Path
from typing import Generator, List, NamedTuple

from loguru import logger

from semantic_search.config import TEXT_PROCESSING_CONFIG
from semantic_search.utils.file_utils import FileExtractor
from semantic_search.utils.text_utils import TextProcessor


class ProcessedDocument(NamedTuple):
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""

    file_path: Path
    relative_path: str
    raw_text: str
    tokens: List[str]
    metadata: dict


class DocumentProcessor:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""

    def __init__(self):
        self.file_extractor = FileExtractor()
        self.text_processor = TextProcessor()
        self.config = TEXT_PROCESSING_CONFIG

    def process_documents(
        self, root_path: Path
    ) -> Generator[ProcessedDocument, None, None]:
        """
        –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        Args:
            root_path: –ü—É—Ç—å –∫ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏

        Yields:
            ProcessedDocument –æ–±—ä–µ–∫—Ç—ã
        """

        if not root_path.exists():
            raise FileNotFoundError(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {root_path}")

        if not root_path.is_dir():
            raise NotADirectoryError(f"–ü—É—Ç—å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–µ–π: {root_path}")

        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤: {root_path}")

        file_paths = self.file_extractor.find_documents(root_path)

        if not file_paths:
            logger.warning("–î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return

        processed_count = 0
        skipped_count = 0

        for i, file_path in enumerate(file_paths, 1):
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {i}/{len(file_paths)}: {file_path.name}")

            try:
                file_size = file_path.stat().st_size
                max_file_size_bytes = (
                    self.config.get("max_file_size_mb", 100) * 1024 * 1024
                )

                if file_size > max_file_size_bytes:
                    logger.warning(
                        f"–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({file_size / 1024 / 1024:.1f}MB): {file_path}"
                    )
                    skipped_count += 1
                    continue

                raw_text = self.file_extractor.extract_text(file_path)

                if len(raw_text) < self.config["min_text_length"]:
                    logger.warning(
                        f"–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π ({len(raw_text)} —Å–∏–º–≤–æ–ª–æ–≤): {file_path}"
                    )
                    skipped_count += 1
                    continue

                max_text_length = self.config.get("max_text_length", 5_000_000)

                if len(raw_text) > max_text_length:
                    logger.info(
                        f"–¢–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω —Å {len(raw_text):,} –¥–æ {max_text_length:,} —Å–∏–º–≤–æ–ª–æ–≤"
                    )
                    raw_text = raw_text[:max_text_length]

                tokens = self.text_processor.preprocess_text(raw_text)

                if len(tokens) < self.config["min_tokens_count"]:
                    logger.warning(f"–°–ª–∏—à–∫–æ–º –º–∞–ª–æ —Ç–æ–∫–µ–Ω–æ–≤ ({len(tokens)}): {file_path}")
                    skipped_count += 1
                    continue

                relative_path = str(file_path.relative_to(root_path))
                relative_path = relative_path.replace("\\", "/")

                metadata = {
                    "file_size": file_path.stat().st_size,
                    "extension": file_path.suffix,
                    "tokens_count": len(tokens),
                    "text_length": len(raw_text),
                }

                processed_count += 1
                yield ProcessedDocument(
                    file_path=file_path,
                    relative_path=relative_path,
                    raw_text=raw_text,
                    tokens=tokens,
                    metadata=metadata,
                )

            except PermissionError:
                logger.error(f"–ù–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ —Ñ–∞–π–ª—É: {file_path}")
                skipped_count += 1
                continue
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_path}: {e}")
                skipped_count += 1
                continue

        logger.info(
            f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –£—Å–ø–µ—à–Ω–æ: {processed_count}, –ü—Ä–æ–ø—É—â–µ–Ω–æ: {skipped_count}"
        )


========================================
FILE: src\semantic_search\core\search_engine.py
========================================
"""–ú–æ–¥—É–ª—å –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞ (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)"""

from __future__ import annotations

import json
from pathlib import Path
from typing import TYPE_CHECKING, Any, Dict, List, Optional

if TYPE_CHECKING:
    from gensim.models.doc2vec import Doc2Vec
from loguru import logger

from semantic_search.config import CACHE_DIR, SEARCH_CONFIG
from semantic_search.utils.cache_manager import CacheManager
from semantic_search.utils.text_utils import TextProcessor


class SearchResult:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø–æ–∏—Å–∫–∞"""

    def __init__(self, doc_id: str, similarity: float, metadata: Optional[Dict] = None):
        self.doc_id = doc_id
        self.similarity = similarity
        self.metadata = metadata or {}
        self.file_path = Path(doc_id)  # doc_id —ç—Ç–æ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å

    def __repr__(self):
        return f"SearchResult(doc_id='{self.doc_id}', similarity={self.similarity:.3f})"


class SemanticSearchEngine:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º"""

    def __init__(
        self,
        model: Optional[Doc2Vec] = None,
        corpus_info: Optional[List] = None,
        documents_base_path: Optional[Path] = None,
    ):
        self.model = model
        self.corpus_info = corpus_info or []
        self.documents_base_path = documents_base_path  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–∑–æ–≤—ã–π –ø—É—Ç—å
        self.text_processor = TextProcessor()
        self.config = SEARCH_CONFIG
        self.cache_manager = CacheManager(CACHE_DIR)

        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
        self._metadata_index = dict()
        if self.corpus_info:
            for tokens, doc_id, metadata in self.corpus_info:
                self._metadata_index[doc_id] = metadata

        if self.documents_base_path:
            logger.info(
                f"SearchEngine –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –±–∞–∑–æ–≤—ã–º –ø—É—Ç–µ–º: {self.documents_base_path}"
            )

    def set_model(
        self,
        model: Doc2Vec,
        corpus_info: Optional[List] = None,
        documents_base_path: Optional[Path] = None,
    ):
        """
        –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞

        Args:
            model: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Doc2Vec
            corpus_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ—Ä–ø—É—Å–µ
            documents_base_path: –ë–∞–∑–æ–≤—ã–π –ø—É—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        self.model = model
        if corpus_info:
            self.corpus_info = corpus_info
            self._metadata_index = dict()
            for tokens, doc_id, metadata in corpus_info:
                self._metadata_index[doc_id] = metadata

        if documents_base_path:
            self.documents_base_path = documents_base_path
            logger.info(f"–£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –±–∞–∑–æ–≤—ã–π –ø—É—Ç—å: {self.documents_base_path}")

        logger.info("–ü–æ–∏—Å–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")

    def _search_base(
        self,
        query: str,
        top_k: Optional[int] = None,
        similarity_threshold: Optional[float] = None,
    ) -> List[SearchResult]:
        """
        –ë–∞–∑–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ config)
            similarity_threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        """
        if self.model is None:
            logger.error("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return []

        if not query.strip():
            logger.warning("–ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å")
            return []

        top_k = top_k or self.config["default_top_k"]
        similarity_threshold = (
            similarity_threshold or self.config["similarity_threshold"]
        )

        try:
            logger.info(f"–ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")

            # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –∑–∞–ø—Ä–æ—Å–∞
            query_tokens = self.text_processor.preprocess_text(query)

            if not query_tokens:
                logger.warning("–ó–∞–ø—Ä–æ—Å –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–Ω–∞—á–∏–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤")
                return []

            logger.info(
                f"–¢–æ–∫–µ–Ω—ã –∑–∞–ø—Ä–æ—Å–∞: {query_tokens[:10]}..."
            )  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10

            # –ü–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_vector = self.model.infer_vector(query_tokens)

            # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            similar_docs = self.model.dv.most_similar([query_vector], topn=top_k)

            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–æ—Ä–æ–≥—É —Å—Ö–æ–∂–µ—Å—Ç–∏ –∏ —Å–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            results = []
            for doc_id, similarity in similar_docs:
                if similarity >= similarity_threshold:
                    metadata = self._metadata_index.get(doc_id, {})
                    results.append(SearchResult(doc_id, similarity, metadata))

            logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}")
            return results

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
            return []

    def search_with_filters(
        self,
        query: str,
        top_k: Optional[int] = None,
        file_extensions: Optional[set] = None,
        date_range: Optional[tuple] = None,
        min_file_size: Optional[int] = None,
        max_file_size: Optional[int] = None,
    ) -> List[SearchResult]:
        """–ü–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏"""

        # –ë–∞–∑–æ–≤—ã–π –ø–æ–∏—Å–∫
        results = self._search_base(query, top_k=top_k or 100)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
        filtered_results = []
        for result in results:
            metadata = result.metadata

            # –§–∏–ª—å—Ç—Ä –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
            if file_extensions and metadata.get("extension") not in file_extensions:
                continue

            # –§–∏–ª—å—Ç—Ä –ø–æ —Ä–∞–∑–º–µ—Ä—É —Ñ–∞–π–ª–∞
            file_size = metadata.get("file_size", 0)
            if min_file_size and file_size < min_file_size:
                continue
            if max_file_size and file_size > max_file_size:
                continue

            filtered_results.append(result)

            if len(filtered_results) >= (top_k or self.config["default_top_k"]):
                break

        return filtered_results

    def make_cache_key(
        self,
        query: str,
        top_k: Optional[int],
        file_extensions: Optional[set],
        date_range: Optional[tuple],
        min_file_size: Optional[int],
        max_file_size: Optional[int],
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –∫—ç—à-–∫–ª—é—á–∞ –¥–ª—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        key_data = {
            "query": query.strip().lower(),
            "top_k": top_k,
            "file_extensions": sorted(file_extensions) if file_extensions else None,
            "date_range": date_range,
            "min_file_size": min_file_size,
            "max_file_size": max_file_size,
        }
        return json.dumps(key_data, sort_keys=True, ensure_ascii=False)

    def search(
        self,
        query: str,
        top_k: Optional[int] = None,
        file_extensions: Optional[set] = None,
        date_range: Optional[tuple] = None,
        min_file_size: Optional[int] = None,
        max_file_size: Optional[int] = None,
    ) -> List[SearchResult]:
        """
        –ü–æ–∏—Å–∫ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ñ–∏–ª—å—Ç—Ä–æ–≤

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            file_extensions: –§–∏–ª—å—Ç—Ä –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º —Ñ–∞–π–ª–æ–≤
            date_range: –§–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–µ (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω–æ –æ—Å—Ç–∞–≤–ª–µ–Ω–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
            min_file_size: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
            max_file_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        """
        if not self.config.get("enable_caching", True):
            return self.search_with_filters(
                query,
                top_k=top_k,
                file_extensions=file_extensions,
                date_range=date_range,
                min_file_size=min_file_size,
                max_file_size=max_file_size,
            )

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –∫–ª—é—á
        raw_key = self.make_cache_key(
            query, top_k, file_extensions, date_range, min_file_size, max_file_size
        )
        cache_key = f"search:{raw_key}"

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cached_result = self.cache_manager.get(cache_key)
        if cached_result:
            logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–ª—É—á–µ–Ω –∏–∑ –∫—ç—à–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query}")
            return cached_result

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        results = self.search_with_filters(
            query,
            top_k=top_k,
            file_extensions=file_extensions,
            date_range=date_range,
            min_file_size=min_file_size,
            max_file_size=max_file_size,
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        self.cache_manager.set(cache_key, results)

        return results

    def search_similar_to_document(
        self, doc_id: str, top_k: Optional[int] = None
    ) -> List[SearchResult]:
        """
        –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –ø–æ—Ö–æ–∂–∏—Ö –Ω–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç

        Args:
            doc_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        if self.model is None:
            logger.error("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return []

        top_k = top_k or self.config["default_top_k"]

        try:
            if doc_id not in self.model.dv:
                logger.error(f"–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –º–æ–¥–µ–ª–∏: {doc_id}")
                return []

            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            similar_docs = self.model.dv.most_similar(
                doc_id, topn=top_k + 1
            )  # +1 —á—Ç–æ–±—ã –∏—Å–∫–ª—é—á–∏—Ç—å —Å–∞–º –¥–æ–∫—É–º–µ–Ω—Ç

            results = []
            for similar_doc_id, similarity in similar_docs:
                if similar_doc_id != doc_id:  # –ò—Å–∫–ª—é—á–∞–µ–º —Å–∞–º –¥–æ–∫—É–º–µ–Ω—Ç
                    metadata = self._metadata_index.get(similar_doc_id, {})
                    results.append(SearchResult(similar_doc_id, similarity, metadata))

            return results[:top_k]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ top_k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return []

    def get_document_vector(self, doc_id: str) -> Optional[list]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞

        Args:
            doc_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞

        Returns:
            –í–µ–∫—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–ª–∏ None
        """
        if self.model is None or doc_id not in self.model.dv:
            return None

        return self.model.dv[doc_id].tolist()

    def get_search_statistics(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–∏—Å–∫–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        if self.model is None:
            return {"status": "no_model"}

        return {
            "status": "ready",
            "documents_count": len(self.model.dv),
            "vocabulary_size": len(self.model.wv.key_to_index),
            "vector_size": self.model.vector_size,
            "indexed_documents": list(self.model.dv.key_to_index.keys())[
                :10
            ],  # –ü–µ—Ä–≤—ã–µ 10
        }


========================================
FILE: src\semantic_search\core\text_summarizer.py
========================================
"""–ú–æ–¥—É–ª—å –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤"""

from __future__ import annotations

from pathlib import Path
from typing import TYPE_CHECKING, List, Optional, Tuple

if TYPE_CHECKING:
    from gensim.models.doc2vec import Doc2Vec

import numpy as np
from loguru import logger

from semantic_search.config import SUMMARIZATION_CONFIG, TEXT_PROCESSING_CONFIG
from semantic_search.utils.text_utils import TextProcessor

try:
    from sklearn.metrics.pairwise import cosine_similarity

    SKLEARN_AVAILABLE = True
except ImportError:
    logger.warning("scikit-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
    SKLEARN_AVAILABLE = False


class TextSummarizer:
    """–ö–ª–∞—Å—Å –¥–ª—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–∏–≤–Ω–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤"""

    def __init__(self, doc2vec_model: Optional[Doc2Vec] = None):
        self.model = doc2vec_model
        self.text_processor = TextProcessor()
        self.config = SUMMARIZATION_CONFIG
        self.chunk_size = TEXT_PROCESSING_CONFIG.get("chunk_size", 500_000)

        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –≤ –≤—ã–∂–∏–º–∫—É
        self.min_summary_sentence_length = self.config.get("min_sentence_length", 15)
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏
        self.min_words_in_sentence = self.config.get("min_words_in_sentence", 5)

    def set_model(self, model: Doc2Vec):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ Doc2Vec"""
        self.model = model
        logger.info("–ú–æ–¥–µ–ª—å –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")

    def _filter_sentence(self, sentence: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞, –ø–æ–¥—Ö–æ–¥–∏—Ç –ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –≤ –≤—ã–∂–∏–º–∫—É

        Args:
            sentence: –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏

        Returns:
            True –µ—Å–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–∏—Ç, False –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å
        """
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        cleaned_sentence = sentence.strip()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        if len(cleaned_sentence) < self.min_summary_sentence_length:
            return False

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤
        words = cleaned_sentence.split()
        if len(words) < self.min_words_in_sentence:
            return False

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ–≥–æ –∑–Ω–∞—á–∏–º–æ–≥–æ —Å–ª–æ–≤–∞ (–Ω–µ —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥–ª–æ–≥–∏/—Å–æ—é–∑—ã)
        meaningful_words = [w for w in words if len(w) > 3]
        if len(meaningful_words) < 2:
            return False

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Ü–∏—Ñ—Ä (–≤–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ —Ç–∞–±–ª–∏—Ü–∞ –∏–ª–∏ —Å–ø–∏—Å–æ–∫)
        digit_ratio = sum(c.isdigit() for c in cleaned_sentence) / len(cleaned_sentence)
        if digit_ratio > 0.5:
            return False

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, "............")
        for char in cleaned_sentence:
            if cleaned_sentence.count(char * 5) > 0:  # 5 –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –ø–æ–¥—Ä—è–¥
                return False

        return True

    def _sentence_to_vector(self, sentence_tokens: List[str]) -> Optional[np.ndarray]:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –≤–µ–∫—Ç–æ—Ä

        Args:
            sentence_tokens: –¢–æ–∫–µ–Ω—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è

        Returns:
            –í–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        """
        if self.model is None or not sentence_tokens:
            return None

        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º infer_vector –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            vector = self.model.infer_vector(sentence_tokens)
            return vector
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: {e}")
            return None

    def _calculate_sentence_scores(
        self, sentences: List[str]
    ) -> List[Tuple[str, float]]:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –º–µ—Ç–æ–¥–æ–º TextRank
        —Å —É—á–µ—Ç–æ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π

        Args:
            sentences: –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π

        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, –æ—Ü–µ–Ω–∫–∞)
        """
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–µ—Ä–µ–¥ –æ—Ü–µ–Ω–∫–æ–π
        filtered_sentences = []
        sentence_indices = []

        for i, sentence in enumerate(sentences):
            if self._filter_sentence(sentence):
                filtered_sentences.append(sentence)
                sentence_indices.append(i)

        if not filtered_sentences:
            logger.warning("–í—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã –∫–∞–∫ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–∞–º—ã–µ –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –µ—Å–ª–∏ –≤—Å–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã
            sorted_by_length = sorted(
                enumerate(sentences), key=lambda x: len(x[1]), reverse=True
            )
            return [(sent, 1.0) for _, sent in sorted_by_length[:5]]

        if not SKLEARN_AVAILABLE or self.model is None:
            # Fallback: –æ—Ü–µ–Ω–∫–∞ –ø–æ –¥–ª–∏–Ω–µ –∏ –ø–æ–∑–∏—Ü–∏–∏
            scored_sentences = []
            for i, sent in enumerate(filtered_sentences):
                # –£—á–∏—Ç—ã–≤–∞–µ–º –¥–ª–∏–Ω—É –∏ –ø–æ–∑–∏—Ü–∏—é (–Ω–∞—á–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ –≤–∞–∂–Ω–µ–µ)
                position_score = 1.0 - (sentence_indices[i] / len(sentences))
                length_score = min(
                    len(sent.split()) / 20, 1.0
                )  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ 20 —Å–ª–æ–≤–∞–º
                score = position_score * 0.3 + length_score * 0.7
                scored_sentences.append((sent, score))
            return scored_sentences

        # –ü–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        sentence_vectors = []
        valid_sentences = []

        for sentence in filtered_sentences:
            tokens = self.text_processor.preprocess_text(sentence)
            if tokens:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å –∑–Ω–∞—á–∏–º—ã–µ —Ç–æ–∫–µ–Ω—ã
                vector = self._sentence_to_vector(tokens)
                if vector is not None:
                    sentence_vectors.append(vector)
                    valid_sentences.append(sentence)

        if len(sentence_vectors) < 2:
            # –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            return [(sent, 1.0) for sent in valid_sentences]

        try:
            # –í—ã—á–∏—Å–ª—è–µ–º –º–∞—Ç—Ä–∏—Ü—É —Å—Ö–æ–∂–µ—Å—Ç–∏
            similarity_matrix = cosine_similarity(sentence_vectors)

            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–æ—Å—Ç–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º PageRank
            scores = self._pagerank_algorithm(similarity_matrix)

            # –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º –æ—Ü–µ–Ω–∫–∏ —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏
            scored_sentences = list(zip(valid_sentences, scores))

            return scored_sentences

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –æ—Ü–µ–Ω–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {e}")
            # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–π –æ—Ü–µ–Ω–∫–µ
            return [(sent, 1.0) for sent in valid_sentences]

    def _pagerank_algorithm(
        self, similarity_matrix: np.ndarray, damping: float = 0.85, max_iter: int = 100
    ) -> List[float]:
        """
        –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º PageRank –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π

        Args:
            similarity_matrix: –ú–∞—Ç—Ä–∏—Ü–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            damping: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∑–∞—Ç—É—Ö–∞–Ω–∏—è
            max_iter: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π

        Returns:
            –°–ø–∏—Å–æ–∫ –æ—Ü–µ–Ω–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        """
        n = similarity_matrix.shape[0]

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: —Ä–∞–≤–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è –≤—Å–µ—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        scores = np.ones(n) / n

        # –°–æ–∑–¥–∞–µ–º –ø–µ—Ä–µ—Ö–æ–¥–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É
        # –ó–∞–º–µ–Ω—è–µ–º –Ω—É–ª–∏ –Ω–∞ –º–∞–ª–µ–Ω—å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
        similarity_matrix = np.where(similarity_matrix == 0, 1e-8, similarity_matrix)

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å—Ç—Ä–æ–∫–∏ –º–∞—Ç—Ä–∏—Ü—ã
        row_sums = similarity_matrix.sum(axis=1)
        transition_matrix = similarity_matrix / row_sums[:, np.newaxis]

        # –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º PageRank
        for _ in range(max_iter):
            new_scores = (1 - damping) / n + damping * np.dot(
                transition_matrix.T, scores
            )

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å
            if np.allclose(scores, new_scores, atol=1e-6):
                break

            scores = new_scores

        return scores.tolist()

    def summarize_text(
        self,
        text: str,
        sentences_count: Optional[int] = None,
        min_sentence_length: Optional[int] = None,
    ) -> List[str]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫—Å—Ç—Ä–∞–∫—Ç–∏–≤–Ω–æ–π –≤—ã–∂–∏–º–∫–∏ —Ç–µ–∫—Å—Ç–∞

        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            sentences_count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –≤—ã–∂–∏–º–∫–µ
            min_sentence_length: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏)

        Returns:
            –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤—ã–∂–∏–º–∫–∏
        """
        if not text.strip():
            logger.warning("–ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏")
            return []

        sentences_count = sentences_count or self.config["default_sentences_count"]

        # –í—Ä–µ–º–µ–Ω–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞
        if min_sentence_length is not None:
            original_min_length = self.min_summary_sentence_length
            self.min_summary_sentence_length = min_sentence_length

        logger.info(
            f"–ù–∞—á–∏–Ω–∞–µ–º —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ –¥–ª–∏–Ω–æ–π {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ (—Ü–µ–ª—å: {sentences_count} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π)"
        )

        # –î–ª—è –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥
        if len(text) > 1_000_000:
            logger.warning(
                f"–¢–µ–∫—Å—Ç –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤), –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥"
            )
            result = self._summarize_long_text(
                text, sentences_count, self.min_summary_sentence_length
            )

            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É
            if min_sentence_length is not None:
                self.min_summary_sentence_length = original_min_length

            return result

        sentences = self.text_processor.split_into_sentences(text)

        if not sentences:
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è")

            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É
            if min_sentence_length is not None:
                self.min_summary_sentence_length = original_min_length

            return []

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–µ—Ä–µ–¥ –ø—Ä–æ–≤–µ—Ä–∫–æ–π
        valid_sentences = [s for s in sentences if self._filter_sentence(s)]

        if len(valid_sentences) <= sentences_count:
            logger.info(
                f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π ({len(valid_sentences)}) –º–µ–Ω—å—à–µ –∏–ª–∏ —Ä–∞–≤–Ω–æ —Ç—Ä–µ–±—É–µ–º–æ–º—É"
            )

            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É
            if min_sentence_length is not None:
                self.min_summary_sentence_length = original_min_length

            return valid_sentences

        # –í—ã—á–∏—Å–ª—è–µ–º –æ—Ü–µ–Ω–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (—É–∂–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö)
        scored_sentences = self._calculate_sentence_scores(sentences)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –æ—Ü–µ–Ω–∫–µ (—É–±—ã–≤–∞–Ω–∏–µ)
        scored_sentences.sort(key=lambda x: x[1], reverse=True)

        # –ë–µ—Ä–µ–º —Ç–æ–ø-N –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        top_sentences = scored_sentences[:sentences_count]

        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        summary_sentences = []

        # –°–æ–∑–¥–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        top_sentences_set = {sent for sent, _ in top_sentences}

        for original_sent in sentences:
            if original_sent in top_sentences_set:
                summary_sentences.append(original_sent)
                if len(summary_sentences) >= sentences_count:
                    break

        logger.info(f"–°–æ–∑–¥–∞–Ω–∞ –≤—ã–∂–∏–º–∫–∞ –∏–∑ {len(summary_sentences)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")

        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É
        if min_sentence_length is not None:
            self.min_summary_sentence_length = original_min_length

        return summary_sentences

    def _summarize_long_text(
        self, text: str, sentences_count: int, min_sentence_length: int
    ) -> List[str]:
        """
        –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤

        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            sentences_count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            min_sentence_length: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è

        Returns:
            –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤—ã–∂–∏–º–∫–∏
        """

        chunks = [
            text[i : i + self.chunk_size] for i in range(0, len(text), self.chunk_size)
        ]

        all_important_sentences = []

        for i, chunk in enumerate(chunks):
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Å—Ç–∏ {i + 1}/{len(chunks)}")

            chunk_sentences = self.text_processor.split_into_sentences(chunk)

            if not chunk_sentences:
                continue

            # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            valid_chunk_sentences = [
                s for s in chunk_sentences if self._filter_sentence(s)
            ]

            if not valid_chunk_sentences:
                continue

            # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞ –≤—ã–±–∏—Ä–∞–µ–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            chunk_sentence_count = max(1, sentences_count // len(chunks))
            if i == 0:  # –ü–µ—Ä–≤—ã–π —á–∞–Ω–∫ –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –±–æ–ª—å—à–µ –≤–∞–∂–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
                chunk_sentence_count = max(2, chunk_sentence_count)

            # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è + —Å–∞–º—ã–µ –¥–ª–∏–Ω–Ω—ã–µ
            important_sentences = []

            # –ü–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —á–∞–Ω–∫–∞ (–µ—Å–ª–∏ –æ–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω–æ–µ)
            if valid_chunk_sentences:
                important_sentences.append(valid_chunk_sentences[0])

            # –ü–æ—Å–ª–µ–¥–Ω–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —á–∞–Ω–∫–∞
            if len(valid_chunk_sentences) > 1:
                important_sentences.append(valid_chunk_sentences[-1])

            # –°–∞–º—ã–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–¥–ª–∏–Ω–Ω—ã–µ, –Ω–æ –Ω–µ —Å–ª–∏—à–∫–æ–º)
            if len(valid_chunk_sentences) > 2:
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ "–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏" - –Ω–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ –Ω–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ
                middle_sentences = valid_chunk_sentences[1:-1]
                sorted_by_info = sorted(
                    middle_sentences,
                    key=lambda s: min(len(s.split()), 50),  # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ ~50 —Å–ª–æ–≤
                    reverse=True,
                )
                remaining_count = chunk_sentence_count - len(important_sentences)
                important_sentences.extend(sorted_by_info[:remaining_count])

            all_important_sentences.extend(important_sentences[:chunk_sentence_count])

        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        seen = set()
        unique_sentences = []
        for sent in all_important_sentences:
            if sent not in seen and self._filter_sentence(sent):
                seen.add(sent)
                unique_sentences.append(sent)

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç—Ä–µ–±—É–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        return unique_sentences[:sentences_count]

    def summarize_file(self, file_path: str, **kwargs) -> List[str]:
        """
        –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–∞

        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è summarize_text

        Returns:
            –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤—ã–∂–∏–º–∫–∏
        """
        from semantic_search.utils.file_utils import FileExtractor

        try:
            extractor = FileExtractor()
            text = extractor.extract_text(Path(file_path))

            if not text:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞: {file_path}")
                return []

            return self.summarize_text(text, **kwargs)

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return []

    def get_summary_statistics(self, original_text: str, summary: List[str]) -> dict:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏

        Args:
            original_text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            summary: –í—ã–∂–∏–º–∫–∞

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        original_sentences = self.text_processor.split_into_sentences(original_text)

        # –°—á–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ
        valid_original_sentences = [
            s for s in original_sentences if self._filter_sentence(s)
        ]

        stats = {
            "original_sentences_count": len(original_sentences),
            "valid_original_sentences_count": len(valid_original_sentences),
            "summary_sentences_count": len(summary),
            "compression_ratio": len(summary) / len(original_sentences)
            if original_sentences
            else 0,
            "valid_compression_ratio": len(summary) / len(valid_original_sentences)
            if valid_original_sentences
            else 0,
            "original_chars_count": len(original_text),
            "summary_chars_count": sum(len(sent) for sent in summary),
            "chars_compression_ratio": sum(len(sent) for sent in summary)
            / len(original_text)
            if original_text
            else 0,
            "avg_sentence_length": sum(len(sent.split()) for sent in summary)
            / len(summary)
            if summary
            else 0,
        }

        return stats


========================================
FILE: src\semantic_search\gui\__init__.py
========================================
"""GUI –º–æ–¥—É–ª—å –¥–ª—è Semantic Search"""

from .main_window import MainWindow

__all__ = ["MainWindow"]


========================================
FILE: src\semantic_search\gui\evaluation_widget.py
========================================
"""–í–∏–¥–∂–µ—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞ –≤ GUI"""

import os
from typing import List

from loguru import logger
from PyQt6.QtCore import QThread, pyqtSignal
from PyQt6.QtWidgets import (
    QComboBox,
    QGroupBox,
    QHBoxLayout,
    QLabel,
    QLineEdit,
    QMessageBox,
    QProgressBar,
    QPushButton,
    QTextEdit,
    QVBoxLayout,
    QWidget,
)

from semantic_search.config import EVALUATION_RESULTS_DIR
from semantic_search.evaluation.baselines import (
    Doc2VecSearchAdapter,
    OpenAISearchBaseline,
)
from semantic_search.evaluation.comparison import QueryTestCase, SearchComparison


class EvaluationThread(QThread):
    """–ü–æ—Ç–æ–∫ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏"""

    progress = pyqtSignal(int, str)
    finished = pyqtSignal(bool, str)
    result_ready = pyqtSignal(dict)

    def __init__(
        self,
        search_engine,
        corpus_info,
        openai_key: str,
        test_cases: List[QueryTestCase],
    ):
        super().__init__()
        self.search_engine = search_engine
        self.corpus_info = corpus_info
        self.openai_key = openai_key
        self.test_cases = test_cases
        self.comparison = SearchComparison(test_cases)

    def run(self):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        try:
            # –®–∞–≥ 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ Doc2Vec –∞–¥–∞–ø—Ç–µ—Ä–∞
            self.progress.emit(10, "–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ Doc2Vec –º–µ—Ç–æ–¥–∞...")
            doc2vec_adapter = Doc2VecSearchAdapter(self.search_engine, self.corpus_info)

            # –®–∞–≥ 2: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ OpenAI baseline
            self.progress.emit(20, "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI...")
            try:
                openai_baseline = OpenAISearchBaseline(api_key=self.openai_key)
            except Exception as e:
                self.finished.emit(False, f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ OpenAI: {str(e)}")
                return

            # –®–∞–≥ 3: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è OpenAI
            self.progress.emit(30, "–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ OpenAI API...")

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            documents = []
            for tokens, doc_id, metadata in self.corpus_info[
                :50
            ]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 50 –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –¥–ª—è –¥–µ–º–æ
                # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Ç–æ–∫–µ–Ω–æ–≤ (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
                text = " ".join(tokens[:500])  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 500 —Ç–æ–∫–µ–Ω–æ–≤
                documents.append((doc_id, text, metadata))

            try:
                openai_baseline.index(documents)
            except Exception as e:
                self.finished.emit(False, f"–û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ OpenAI: {str(e)}")
                return

            # –®–∞–≥ 4: –û—Ü–µ–Ω–∫–∞ –º–µ—Ç–æ–¥–æ–≤
            self.progress.emit(50, "–û—Ü–µ–Ω–∫–∞ Doc2Vec...")
            doc2vec_results = self.comparison.evaluate_method(
                doc2vec_adapter, top_k=10, verbose=False
            )

            self.progress.emit(70, "–û—Ü–µ–Ω–∫–∞ OpenAI...")
            openai_results = self.comparison.evaluate_method(
                openai_baseline, top_k=10, verbose=False
            )

            # –®–∞–≥ 5: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤ –∏ –≥—Ä–∞—Ñ–∏–∫–æ–≤
            self.progress.emit(85, "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤...")

            # –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
            df_comparison = self.comparison.compare_methods(
                [doc2vec_adapter, openai_baseline], save_results=True
            )

            # –ì—Ä–∞—Ñ–∏–∫–∏
            self.comparison.plot_comparison(save_plots=True)

            # –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç
            report_path = EVALUATION_RESULTS_DIR / "comparison_report.txt"
            report_text = self.comparison.generate_report(report_path)

            # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è GUI
            results = {
                "comparison_df": df_comparison,
                "report_text": report_text,
                "doc2vec_map": doc2vec_results["aggregated"]["MAP"],
                "openai_map": openai_results["aggregated"]["MAP"],
                "doc2vec_time": doc2vec_results["aggregated"]["avg_query_time"],
                "openai_time": openai_results["aggregated"]["avg_query_time"],
            }

            self.progress.emit(100, "–û—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
            self.result_ready.emit(results)
            self.finished.emit(True, "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ –ø–æ—Ç–æ–∫–µ –æ—Ü–µ–Ω–∫–∏: {e}")
            self.finished.emit(False, f"–û—à–∏–±–∫–∞: {str(e)}")


class EvaluationWidget(QWidget):
    """–í–∏–¥–∂–µ—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞"""

    def __init__(self, parent=None):
        super().__init__(parent)
        self.search_engine = None
        self.corpus_info = None
        self.evaluation_thread = None
        self.init_ui()

    def init_ui(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
        layout = QVBoxLayout()

        # –ì—Ä—É–ø–ø–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ OpenAI
        openai_group = QGroupBox("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ OpenAI")
        openai_layout = QVBoxLayout()

        key_layout = QHBoxLayout()
        key_layout.addWidget(QLabel("API Key:"))
        self.api_key_edit = QLineEdit()
        self.api_key_edit.setEchoMode(QLineEdit.EchoMode.Password)
        self.api_key_edit.setPlaceholderText("sk-...")

        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–ª—é—á –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
        env_key = os.getenv("OPENAI_API_KEY")
        if env_key:
            self.api_key_edit.setText(env_key)

        key_layout.addWidget(self.api_key_edit)
        openai_layout.addLayout(key_layout)

        openai_group.setLayout(openai_layout)
        layout.addWidget(openai_group)

        # –ì—Ä—É–ø–ø–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        test_group = QGroupBox("–¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã")
        test_layout = QVBoxLayout()

        self.test_cases_combo = QComboBox()
        self.test_cases_combo.addItem("–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ (5 –∑–∞–ø—Ä–æ—Å–æ–≤)")
        self.test_cases_combo.addItem("–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä (10 –∑–∞–ø—Ä–æ—Å–æ–≤)")
        self.test_cases_combo.addItem("–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç (3 –∑–∞–ø—Ä–æ—Å–∞)")

        test_layout.addWidget(self.test_cases_combo)
        test_group.setLayout(test_layout)
        layout.addWidget(test_group)

        # –ö–Ω–æ–ø–∫–∞ –∑–∞–ø—É—Å–∫–∞
        self.run_button = QPushButton("–ó–∞–ø—É—Å—Ç–∏—Ç—å —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ")
        self.run_button.clicked.connect(self.run_evaluation)
        layout.addWidget(self.run_button)

        # –ü—Ä–æ–≥—Ä–µ—Å—Å
        self.progress_bar = QProgressBar()
        self.progress_bar.setVisible(False)
        layout.addWidget(self.progress_bar)

        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.results_text = QTextEdit()
        self.results_text.setReadOnly(True)
        layout.addWidget(self.results_text)

        self.setLayout(layout)

    def set_search_engine(self, search_engine, corpus_info):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏"""
        self.search_engine = search_engine
        self.corpus_info = corpus_info

    def get_test_cases(self) -> List[QueryTestCase]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤—ã–±–æ—Ä–∞"""
        comparison = SearchComparison()
        default_cases = comparison.create_default_test_cases()

        selected_index = self.test_cases_combo.currentIndex()

        if selected_index == 0:  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –Ω–∞–±–æ—Ä
            return default_cases[:5]
        elif selected_index == 1:  # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã
            extra_cases = [
                QueryTestCase(
                    query="–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π CNN",
                    relevant_docs={"cnn_tutorial.pdf", "image_classification.pdf"},
                    relevance_scores={
                        "cnn_tutorial.pdf": 3,
                        "image_classification.pdf": 3,
                    },
                ),
                QueryTestCase(
                    query="—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏",
                    relevant_docs={"regularization.pdf", "overfitting.pdf"},
                    relevance_scores={"regularization.pdf": 3, "overfitting.pdf": 2},
                ),
                QueryTestCase(
                    query="word2vec –∏ doc2vec –º–æ–¥–µ–ª–∏",
                    relevant_docs={"word2vec_paper.pdf", "doc2vec_tutorial.pdf"},
                    relevance_scores={
                        "word2vec_paper.pdf": 3,
                        "doc2vec_tutorial.pdf": 3,
                    },
                ),
                QueryTestCase(
                    query="–º–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏",
                    relevant_docs={"ml_metrics.pdf", "evaluation_methods.pdf"},
                    relevance_scores={"ml_metrics.pdf": 3, "evaluation_methods.pdf": 3},
                ),
                QueryTestCase(
                    query="–æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏",
                    relevant_docs={"backpropagation.pdf", "neural_networks.pdf"},
                    relevance_scores={
                        "backpropagation.pdf": 3,
                        "neural_networks.pdf": 2,
                    },
                ),
            ]
            return default_cases + extra_cases
        else:  # –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç
            return default_cases[:3]

    def run_evaluation(self):
        """–ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏"""
        if not self.search_engine:
            QMessageBox.warning(self, "–û—à–∏–±–∫–∞", "–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å Doc2Vec")
            return

        api_key = self.api_key_edit.text().strip()
        if not api_key:
            QMessageBox.warning(
                self,
                "–û—à–∏–±–∫–∞",
                "–í–≤–µ–¥–∏—Ç–µ API –∫–ª—é—á OpenAI –∏–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è OPENAI_API_KEY",
            )
            return

        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Å–ª—É—á–∞–∏
        test_cases = self.get_test_cases()

        # –û—Ç–∫–ª—é—á–∞–µ–º –∫–Ω–æ–ø–∫—É –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        self.run_button.setEnabled(False)
        self.progress_bar.setVisible(True)
        self.progress_bar.setValue(0)
        self.results_text.clear()

        # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –ø–æ—Ç–æ–∫
        self.evaluation_thread = EvaluationThread(
            self.search_engine, self.corpus_info, api_key, test_cases
        )

        self.evaluation_thread.progress.connect(self.on_progress)
        self.evaluation_thread.finished.connect(self.on_finished)
        self.evaluation_thread.result_ready.connect(self.on_results_ready)

        self.evaluation_thread.start()

    def on_progress(self, value: int, message: str):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        self.progress_bar.setValue(value)
        self.results_text.append(f"[{value}%] {message}")

    def on_finished(self, success: bool, message: str):
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏"""
        self.run_button.setEnabled(True)
        self.progress_bar.setVisible(False)

        if success:
            self.results_text.append(f"\n‚úÖ {message}")
            QMessageBox.information(self, "–£—Å–ø–µ—Ö", message)
        else:
            self.results_text.append(f"\n‚ùå {message}")
            QMessageBox.critical(self, "–û—à–∏–±–∫–∞", message)

    def on_results_ready(self, results: dict):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø–æ–ª–µ
        self.results_text.append("\n" + "=" * 80)
        self.results_text.append("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–†–ê–í–ù–ï–ù–ò–Ø")
        self.results_text.append("=" * 80)

        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        doc2vec_map = results["doc2vec_map"]
        openai_map = results["openai_map"]
        improvement = (
            ((doc2vec_map - openai_map) / openai_map) * 100 if openai_map > 0 else 0
        )

        self.results_text.append("\nüìä MAP (Mean Average Precision):")
        self.results_text.append(f"   Doc2Vec: {doc2vec_map:.3f}")
        self.results_text.append(f"   OpenAI:  {openai_map:.3f}")

        if improvement > 0:
            self.results_text.append(f"   ‚úÖ Doc2Vec –ª—É—á—à–µ –Ω–∞ {improvement:.1f}%")
        else:
            self.results_text.append(f"   ‚ùå OpenAI –ª—É—á—à–µ –Ω–∞ {-improvement:.1f}%")

        # –°–∫–æ—Ä–æ—Å—Ç—å
        doc2vec_time = results["doc2vec_time"]
        openai_time = results["openai_time"]
        speed_ratio = openai_time / doc2vec_time if doc2vec_time > 0 else 0

        self.results_text.append("\n‚ö° –°–∫–æ—Ä–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞:")
        self.results_text.append(f"   Doc2Vec: {doc2vec_time:.3f}—Å")
        self.results_text.append(f"   OpenAI:  {openai_time:.3f}—Å")
        self.results_text.append(f"   ‚úÖ Doc2Vec –±—ã—Å—Ç—Ä–µ–µ –≤ {speed_ratio:.1f} —Ä–∞–∑")

        # –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç
        self.results_text.append("\n" + results["report_text"])

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö
        self.results_text.append("\nüìÅ –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤:")
        self.results_text.append(f"   {EVALUATION_RESULTS_DIR}")
        self.results_text.append("   - comparison_results.csv")
        self.results_text.append("   - comparison_report.txt")
        self.results_text.append("   - plots/comparison_plots.png")


========================================
FILE: src\semantic_search\gui\main_window.py
========================================
"""–ì–ª–∞–≤–Ω–æ–µ –æ–∫–Ω–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""

import json
import os
import sys
import time
from pathlib import Path
from typing import List, Optional

from loguru import logger
from PyQt6.QtCore import Qt, QThread, pyqtSignal
from PyQt6.QtGui import QAction, QFont
from PyQt6.QtWidgets import (
    QApplication,
    QCheckBox,
    QComboBox,
    QFileDialog,
    QGroupBox,
    QHBoxLayout,
    QLabel,
    QLineEdit,
    QListWidget,
    QListWidgetItem,
    QMainWindow,
    QMessageBox,
    QProgressBar,
    QPushButton,
    QSpinBox,
    QSplitter,
    QStatusBar,
    QTabWidget,
    QTextEdit,
    QToolBar,
    QVBoxLayout,
    QWidget,
)

from semantic_search.config import GUI_CONFIG, MODELS_DIR
from semantic_search.core.doc2vec_trainer import Doc2VecTrainer
from semantic_search.core.document_processor import DocumentProcessor
from semantic_search.core.search_engine import SemanticSearchEngine
from semantic_search.core.text_summarizer import TextSummarizer
from semantic_search.gui.evaluation_widget import EvaluationWidget
from semantic_search.utils.file_utils import FileExtractor
from semantic_search.utils.statistics import (
    calculate_statistics_from_processed_docs,
    format_statistics_for_display,
)

"""–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å TrainingThread –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞ –≤—Ä–µ–º–µ–Ω–∏"""


class TrainingThread(QThread):
    """–ü–æ—Ç–æ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"""

    progress = pyqtSignal(int, str)
    finished = pyqtSignal(bool, str)
    statistics = pyqtSignal(dict)

    def __init__(
        self,
        documents_path: Path,
        model_name: str,
        vector_size: int,
        epochs: int,
        window: int = 15,
        min_count: int = 3,
        dm: int = 1,
        negative: int = 10,
        preset: Optional[str] = None,
    ):
        super().__init__()
        self.documents_path = documents_path
        self.model_name = model_name
        self.vector_size = vector_size
        self.epochs = epochs
        self.window = window
        self.min_count = min_count
        self.dm = dm
        self.negative = negative
        self.preset = preset
        self.is_cancelled = False

    def run(self):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è"""
        try:
            # –ù–∞—á–∏–Ω–∞–µ–º –æ—Ç—Å—á–µ—Ç –æ–±—â–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏
            start_time = time.time()

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            processor = DocumentProcessor()
            processed_docs = []

            self.progress.emit(10, "–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

            file_extractor = FileExtractor()
            file_paths = file_extractor.find_documents(self.documents_path)

            if not file_paths:
                self.finished.emit(False, "–î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            step_size = 40 / len(file_paths)
            current_progress = 10

            for i, doc in enumerate(processor.process_documents(self.documents_path)):
                if self.is_cancelled:
                    self.finished.emit(False, "–û–±—É—á–µ–Ω–∏–µ –æ—Ç–º–µ–Ω–µ–Ω–æ")
                    return

                processed_docs.append(doc)
                current_progress += step_size
                self.progress.emit(
                    int(current_progress),
                    f"–û–±—Ä–∞–±–æ—Ç–∞–Ω –¥–æ–∫—É–º–µ–Ω—Ç {i + 1}/{len(file_paths)}: {doc.relative_path}",
                )

            if not processed_docs:
                self.finished.emit(False, "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã")
                return

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ—Ä–ø—É—Å–∞
            corpus = [
                (doc.tokens, doc.relative_path, doc.metadata) for doc in processed_docs
            ]

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ—Ä–ø—É—Å–∞
            stats = calculate_statistics_from_processed_docs(processed_docs)
            self.statistics.emit(stats)

            # –ê–Ω–∞–ª–∏–∑ —è–∑—ã–∫–æ–≤–æ–≥–æ —Å–æ—Å—Ç–∞–≤–∞ (–Ω–æ–≤–æ–µ)
            self.progress.emit(45, "–ê–Ω–∞–ª–∏–∑ —è–∑—ã–∫–æ–≤–æ–≥–æ —Å–æ—Å—Ç–∞–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
            language_info = self._analyze_language_distribution(corpus)

            logger.info(f"–Ø–∑—ã–∫–æ–≤–æ–π —Å–æ—Å—Ç–∞–≤: {language_info}")

            # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —è–∑—ã–∫–æ–≤–æ–≥–æ —Å–æ—Å—Ç–∞–≤–∞
            adapted_params = self._adapt_params_for_language(language_info)

            # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            final_vector_size = adapted_params.get("vector_size", self.vector_size)
            final_window = adapted_params.get("window", self.window)
            final_min_count = adapted_params.get("min_count", self.min_count)

            if adapted_params:
                self.progress.emit(
                    48, "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞"
                )
                logger.info(f"–ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {adapted_params}")

            trainer = Doc2VecTrainer()

            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            self.progress.emit(
                50,
                f"–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–≤–µ–∫—Ç–æ—Ä—ã: {final_vector_size}, –æ–∫–Ω–æ: {final_window})...",
            )

            model = trainer.train_model(
                corpus,
                vector_size=final_vector_size,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                epochs=self.epochs,
                window=final_window,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                min_count=final_min_count,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                dm=self.dm,
                negative=self.negative,
                sample=1e-5,
                preset=self.preset,
            )

            if model:
                # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–µ–µ –≤—Ä–µ–º—è –≤–∫–ª—é—á–∞—è –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                training_time = time.time() - start_time

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è
                trainer.training_metadata = {
                    "training_time_formatted": f"{training_time:.1f}—Å ({training_time / 60:.1f}–º)",
                    "training_date": time.strftime(
                        "%Y-%m-%d %H:%M:%S", time.localtime(start_time)
                    ),
                    "corpus_size": len(processed_docs),
                    "documents_base_path": str(self.documents_path.absolute()),
                    "vector_size": self.vector_size,
                    "epochs": self.epochs,
                    "window": self.window,
                    "min_count": self.min_count,
                    "dm": self.dm,
                    "negative": self.negative,
                    "preset_used": self.preset,
                    "language_distribution": language_info,
                    "python_version": sys.version,
                    "platform": sys.platform,
                }

                self.progress.emit(90, "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
                success = trainer.save_model(model, self.model_name)

                if success:
                    self.progress.emit(100, "–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
                    self.finished.emit(
                        True,
                        f"–ú–æ–¥–µ–ª—å '{self.model_name}' —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∑–∞ {training_time / 60:.1f} –º–∏–Ω—É—Ç",
                    )
                else:
                    self.finished.emit(False, "–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏")
            else:
                self.finished.emit(False, "–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ –ø–æ—Ç–æ–∫–µ –æ–±—É—á–µ–Ω–∏—è: {e}", exc_info=True)
            self.finished.emit(False, f"–û—à–∏–±–∫–∞: {str(e)}")

    def _analyze_language_distribution(self, corpus):
        """–ê–Ω–∞–ª–∏–∑ —è–∑—ã–∫–æ–≤–æ–≥–æ —Å–æ—Å—Ç–∞–≤–∞ –∫–æ—Ä–ø—É—Å–∞"""
        language_stats = {"russian": 0, "english": 0, "mixed": 0}

        for tokens, doc_id, metadata in corpus[
            :100
        ]:  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 100 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            # –ü–æ–¥—Å—á–µ—Ç –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏—Ö –∏ –ª–∞—Ç–∏–Ω—Å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤
            cyrillic_tokens = sum(
                1 for t in tokens[:200] if any("\u0400" <= c <= "\u04ff" for c in t)
            )
            latin_tokens = sum(1 for t in tokens[:200] if t.isalpha() and t.isascii())

            total = cyrillic_tokens + latin_tokens
            if total > 0:
                cyrillic_ratio = cyrillic_tokens / total

                if cyrillic_ratio > 0.8:
                    language_stats["russian"] += 1
                elif cyrillic_ratio < 0.2:
                    language_stats["english"] += 1
                else:
                    language_stats["mixed"] += 1

        # –≠–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–µ–º –Ω–∞ –≤–µ—Å—å –∫–æ—Ä–ø—É—Å
        sample_size = min(100, len(corpus))
        scale_factor = len(corpus) / sample_size

        return {
            "russian": int(language_stats["russian"] * scale_factor),
            "english": int(language_stats["english"] * scale_factor),
            "mixed": int(language_stats["mixed"] * scale_factor),
            "total": len(corpus),
        }

    def _adapt_params_for_language(self, language_info: dict) -> dict:
        """
        –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —è–∑—ã–∫–æ–≤–æ–≥–æ —Å–æ—Å—Ç–∞–≤–∞

        Args:
            language_info: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —è–∑—ã–∫–æ–≤ –≤ –∫–æ—Ä–ø—É—Å–µ

        Returns:
            –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        """
        total = language_info["total"]
        if total == 0:
            return {}

        # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ
        russian_pct = language_info["russian"] / total
        english_pct = language_info["english"] / total
        mixed_pct = language_info["mixed"] / total

        adapted_params = {}

        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤
        if mixed_pct > 0.3 or (russian_pct > 0.2 and english_pct > 0.2):
            # –ú–Ω–æ–≥–æ —Å–º–µ—à–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–ª–∏ –æ–±–∞ —è–∑—ã–∫–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ
            adapted_params["vector_size"] = min(400, self.vector_size + 50)
            logger.info(
                f"–£–≤–µ–ª–∏—á–µ–Ω–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–æ {adapted_params['vector_size']} –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞"
            )

        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–∞ –æ–∫–Ω–∞
        if english_pct > 0.5:
            # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã —á–∞—Å—Ç–æ –∏–º–µ—é—Ç –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            adapted_params["window"] = max(10, self.window - 2)
        elif mixed_pct > 0.3:
            # –°–º–µ—à–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã —Ç—Ä–µ–±—É—é—Ç –±–æ–ª—å—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            adapted_params["window"] = min(20, self.window + 3)

        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —á–∞—Å—Ç–æ—Ç—ã
        if total < 100:
            # –ú–∞–ª–µ–Ω—å–∫–∏–π –∫–æ—Ä–ø—É—Å - —Å–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥
            adapted_params["min_count"] = max(1, self.min_count - 1)
        elif mixed_pct > 0.3:
            # –°–º–µ—à–∞–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å - –ø–æ–≤—ã—à–∞–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —à—É–º–∞
            adapted_params["min_count"] = self.min_count + 1

        return adapted_params

    def cancel(self):
        """–û—Ç–º–µ–Ω–∞ –æ–±—É—á–µ–Ω–∏—è"""
        self.is_cancelled = True


class SearchThread(QThread):
    """–ü–æ—Ç–æ–∫ –¥–ª—è –ø–æ–∏—Å–∫–∞"""

    results = pyqtSignal(list)
    error = pyqtSignal(str)

    def __init__(self, search_engine: SemanticSearchEngine, query: str, top_k: int):
        super().__init__()
        self.search_engine = search_engine
        self.query = query
        self.top_k = top_k

    def run(self):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞"""
        try:
            results = self.search_engine.search(self.query, top_k=self.top_k)
            self.results.emit(results)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            self.error.emit(str(e))


class MainWindow(QMainWindow):
    """–ì–ª–∞–≤–Ω–æ–µ –æ–∫–Ω–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""

    def __init__(self):
        super().__init__()
        self.current_model = None
        self.search_engine = None
        self.training_thread = None
        self.search_thread = None
        self.summarizer = None

        self.init_ui()
        self.load_models()

    def init_ui(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
        self.setWindowTitle(GUI_CONFIG["window_title"])
        self.setGeometry(100, 100, *GUI_CONFIG["window_size"])

        # –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –≤–∏–¥–∂–µ—Ç
        central_widget = QWidget()
        self.setCentralWidget(central_widget)

        # –û—Å–Ω–æ–≤–Ω–æ–π layout
        main_layout = QVBoxLayout(central_widget)

        # –°–æ–∑–¥–∞–µ–º –º–µ–Ω—é
        self.create_menu_bar()

        # –°–æ–∑–¥–∞–µ–º –ø–∞–Ω–µ–ª—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
        self.create_toolbar()

        # –°–æ–∑–¥–∞–µ–º –≤–∫–ª–∞–¥–∫–∏
        self.tab_widget = QTabWidget()
        main_layout.addWidget(self.tab_widget)

        # –í–∫–ª–∞–¥–∫–∞ –æ–±—É—á–µ–Ω–∏—è
        self.create_training_tab()

        # –í–∫–ª–∞–¥–∫–∞ –ø–æ–∏—Å–∫–∞
        self.create_search_tab()

        # –í–∫–ª–∞–¥–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        self.create_summarization_tab()

        # –í–∫–ª–∞–¥–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.create_statistics_tab()

        # –í–∫–ª–∞–¥–∫–∞ –æ—Ü–µ–Ω–∫–∏ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        self.create_evaluation_tab()

        # –°—Ç–∞—Ç—É—Å –±–∞—Ä
        self.status_bar = QStatusBar()
        self.setStatusBar(self.status_bar)
        self.status_bar.showMessage("–ì–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å—Ç–∏–ª–∏
        self.setStyleSheet("""
            QMainWindow {
                background-color: #f5f5f5;
            }
            QTabWidget::pane {
                border: 1px solid #ddd;
                background-color: white;
            }
            QTabBar::tab {
                padding: 8px 16px;
                margin-right: 2px;
            }
            QTabBar::tab:selected {
                background-color: white;
                border-bottom: 2px solid #0066cc;
            }
            QPushButton {
                padding: 6px 12px;
                border: 1px solid #ddd;
                border-radius: 4px;
                background-color: #0066cc;
                color: white;
                font-weight: bold;
            }
            QPushButton:hover {
                background-color: #0052a3;
            }
            QPushButton:pressed {
                background-color: #004080;
            }
            QLineEdit, QTextEdit, QListWidget {
                border: 1px solid #ddd;
                border-radius: 4px;
                padding: 4px;
            }
            QProgressBar {
                border: 1px solid #ddd;
                border-radius: 4px;
                text-align: center;
            }
            QProgressBar::chunk {
                background-color: #0066cc;
                border-radius: 3px;
            }
        """)

    def create_menu_bar(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–µ–Ω—é"""
        menubar = self.menuBar()

        # –ú–µ–Ω—é –§–∞–π–ª
        file_menu = menubar.addMenu("–§–∞–π–ª")

        exit_action = QAction("–í—ã—Ö–æ–¥", self)
        exit_action.setShortcut("Ctrl+Q")
        exit_action.triggered.connect(self.close)
        file_menu.addAction(exit_action)

        # –ú–µ–Ω—é –ú–æ–¥–µ–ª—å
        model_menu = menubar.addMenu("–ú–æ–¥–µ–ª—å")

        load_model_action = QAction("–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å", self)
        load_model_action.triggered.connect(self.load_model_dialog)
        model_menu.addAction(load_model_action)

        # –ú–µ–Ω—é –ü–æ–º–æ—â—å
        help_menu = menubar.addMenu("–ü–æ–º–æ—â—å")

        about_action = QAction("–û –ø—Ä–æ–≥—Ä–∞–º–º–µ", self)
        about_action.triggered.connect(self.show_about)
        help_menu.addAction(about_action)

    def create_toolbar(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–Ω–µ–ª–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤"""
        toolbar = QToolBar()
        toolbar.setMovable(False)
        self.addToolBar(toolbar)

        # –ö–æ–º–±–æ–±–æ–∫—Å –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏
        self.model_combo = QComboBox()
        self.model_combo.setMinimumWidth(200)
        self.model_combo.currentTextChanged.connect(self.on_model_changed)

        toolbar.addWidget(QLabel("–ú–æ–¥–µ–ª—å: "))
        toolbar.addWidget(self.model_combo)
        toolbar.addSeparator()

        # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä —Å—Ç–∞—Ç—É—Å–∞ –º–æ–¥–µ–ª–∏
        self.model_status_label = QLabel("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        self.model_status_label.setStyleSheet("color: red;")
        toolbar.addWidget(self.model_status_label)

    def create_search_tab(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∫–ª–∞–¥–∫–∏ –ø–æ–∏—Å–∫–∞"""
        search_widget = QWidget()
        layout = QVBoxLayout(search_widget)

        # –ü–∞–Ω–µ–ª—å –ø–æ–∏—Å–∫–∞
        search_panel = QWidget()
        search_layout = QHBoxLayout(search_panel)

        self.search_input = QLineEdit()
        self.search_input.setPlaceholderText("–í–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å...")
        self.search_input.returnPressed.connect(self.perform_search)

        self.search_button = QPushButton("–ü–æ–∏—Å–∫")
        self.search_button.clicked.connect(self.perform_search)

        self.results_count_spin = QSpinBox()
        self.results_count_spin.setMinimum(1)
        self.results_count_spin.setMaximum(100)
        self.results_count_spin.setValue(10)

        search_layout.addWidget(self.search_input)
        search_layout.addWidget(QLabel("–†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:"))
        search_layout.addWidget(self.results_count_spin)
        search_layout.addWidget(self.search_button)

        layout.addWidget(search_panel)

        # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        splitter = QSplitter(Qt.Orientation.Horizontal)

        # –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.results_list = QListWidget()
        self.results_list.itemClicked.connect(self.on_result_selected)
        splitter.addWidget(self.results_list)

        # –ü—Ä–æ—Å–º–æ—Ç—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞
        self.document_viewer = QTextEdit()
        self.document_viewer.setReadOnly(True)
        splitter.addWidget(self.document_viewer)

        splitter.setSizes([400, 600])
        layout.addWidget(splitter)

        self.tab_widget.addTab(search_widget, "üîç –ü–æ–∏—Å–∫")

    def create_training_tab(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∫–ª–∞–¥–∫–∏ –æ–±—É—á–µ–Ω–∏—è"""
        training_widget = QWidget()
        layout = QVBoxLayout(training_widget)

        # –ì—Ä—É–ø–ø–∞ –≤—ã–±–æ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        docs_group = QGroupBox("–î–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        docs_layout = QHBoxLayout()

        self.docs_path_edit = QLineEdit()
        self.docs_path_edit.setPlaceholderText("–ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏...")

        browse_button = QPushButton("–û–±–∑–æ—Ä...")
        browse_button.clicked.connect(self.browse_documents)

        docs_layout.addWidget(self.docs_path_edit)
        docs_layout.addWidget(browse_button)
        docs_group.setLayout(docs_layout)

        layout.addWidget(docs_group)

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        params_group = QGroupBox("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏")
        params_layout = QVBoxLayout()

        # –ò–º—è –º–æ–¥–µ–ª–∏
        name_layout = QHBoxLayout()
        name_layout.addWidget(QLabel("–ò–º—è –º–æ–¥–µ–ª–∏:"))
        self.model_name_edit = QLineEdit("doc2vec_model")
        name_layout.addWidget(self.model_name_edit)
        params_layout.addLayout(name_layout)

        # –í—ã–±–æ—Ä –ø—Ä–µ—Å–µ—Ç–∞
        preset_layout = QHBoxLayout()
        preset_layout.addWidget(QLabel("–ü—Ä–µ—Å–µ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–∫:"))
        self.preset_combo = QComboBox()
        self.preset_combo.addItems(
            [
                "–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)",
                "–ë—ã—Å—Ç—Ä—ã–π (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)",
                "–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π (–º–µ–¥–ª–µ–Ω–Ω—ã–π)",
                "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π",
            ]
        )
        self.preset_combo.currentIndexChanged.connect(self.on_preset_changed)
        preset_layout.addWidget(self.preset_combo)
        params_layout.addLayout(preset_layout)

        # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤
        vector_layout = QHBoxLayout()
        vector_layout.addWidget(QLabel("–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤:"))
        self.vector_size_spin = QSpinBox()
        self.vector_size_spin.setMinimum(50)
        self.vector_size_spin.setMaximum(500)
        self.vector_size_spin.setValue(300)
        vector_layout.addWidget(self.vector_size_spin)
        params_layout.addLayout(vector_layout)

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
        epochs_layout = QHBoxLayout()
        epochs_layout.addWidget(QLabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö:"))
        self.epochs_spin = QSpinBox()
        self.epochs_spin.setMinimum(1)
        self.epochs_spin.setMaximum(100)
        self.epochs_spin.setValue(30)
        epochs_layout.addWidget(self.epochs_spin)
        params_layout.addLayout(epochs_layout)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ)
        advanced_group = QGroupBox("–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)")
        advanced_layout = QVBoxLayout()

        # –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞
        window_layout = QHBoxLayout()
        window_layout.addWidget(QLabel("–†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:"))
        self.window_spin = QSpinBox()
        self.window_spin.setMinimum(5)
        self.window_spin.setMaximum(50)
        self.window_spin.setValue(15)
        window_layout.addWidget(self.window_spin)
        advanced_layout.addLayout(window_layout)

        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞
        min_count_layout = QHBoxLayout()
        min_count_layout.addWidget(QLabel("–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤–∞:"))
        self.min_count_spin = QSpinBox()
        self.min_count_spin.setMinimum(1)
        self.min_count_spin.setMaximum(10)
        self.min_count_spin.setValue(3)
        min_count_layout.addWidget(self.min_count_spin)
        advanced_layout.addLayout(min_count_layout)

        # DM —Ä–µ–∂–∏–º
        dm_layout = QHBoxLayout()
        dm_layout.addWidget(QLabel("–†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è:"))
        self.dm_combo = QComboBox()
        self.dm_combo.addItems(
            ["Distributed Memory (DM)", "Distributed Bag of Words (DBOW)"]
        )
        self.dm_combo.setCurrentIndex(0)
        dm_layout.addWidget(self.dm_combo)
        advanced_layout.addLayout(dm_layout)

        # Negative sampling
        negative_layout = QHBoxLayout()
        negative_layout.addWidget(QLabel("Negative sampling:"))
        self.negative_spin = QSpinBox()
        self.negative_spin.setMinimum(0)
        self.negative_spin.setMaximum(20)
        self.negative_spin.setValue(10)
        negative_layout.addWidget(self.negative_spin)
        advanced_layout.addLayout(negative_layout)

        advanced_group.setLayout(advanced_layout)
        layout.addWidget(advanced_group)

        params_group.setLayout(params_layout)
        layout.addWidget(params_group)

        # –ö–Ω–æ–ø–∫–∞ –æ–±—É—á–µ–Ω–∏—è
        self.train_button = QPushButton("–ù–∞—á–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ")
        self.train_button.clicked.connect(self.start_training)
        layout.addWidget(self.train_button)

        # –ü—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä
        self.training_progress = QProgressBar()
        self.training_progress.setVisible(False)
        layout.addWidget(self.training_progress)

        # –õ–æ–≥ –æ–±—É—á–µ–Ω–∏—è
        self.training_log = QTextEdit()
        self.training_log.setReadOnly(True)
        layout.addWidget(self.training_log)

        self.tab_widget.addTab(training_widget, "üß† –û–±—É—á–µ–Ω–∏–µ")

    def on_preset_changed(self, index):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø—Ä–µ—Å–µ—Ç–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫"""
        if index == 0:  # –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π
            self.vector_size_spin.setValue(300)
            self.epochs_spin.setValue(30)
            self.window_spin.setValue(15)
            self.min_count_spin.setValue(3)
            self.negative_spin.setValue(10)
            self.training_log.append(
                "üìã –í—ã–±—Ä–∞–Ω —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–µ—Å–µ—Ç (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ —Å–ª—É—á–∞–µ–≤)"
            )

        elif index == 1:  # –ë—ã—Å—Ç—Ä—ã–π
            self.vector_size_spin.setValue(200)
            self.epochs_spin.setValue(15)
            self.window_spin.setValue(10)
            self.min_count_spin.setValue(5)
            self.negative_spin.setValue(5)
            self.training_log.append("‚ö° –í—ã–±—Ä–∞–Ω –±—ã—Å—Ç—Ä—ã–π –ø—Ä–µ—Å–µ—Ç (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)")

        elif index == 2:  # –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π
            self.vector_size_spin.setValue(400)
            self.epochs_spin.setValue(50)
            self.window_spin.setValue(20)
            self.min_count_spin.setValue(2)
            self.negative_spin.setValue(15)
            self.training_log.append(
                "üèÜ –í—ã–±—Ä–∞–Ω –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–µ—Å–µ—Ç (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –º–µ–¥–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ)"
            )

        elif index == 3:  # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π
            self.training_log.append(
                "üîß –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —Ä–µ–∂–∏–º - –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—Ä—É—á–Ω—É—é"
            )

    def create_summarization_tab(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∫–ª–∞–¥–∫–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
        summary_widget = QWidget()
        layout = QVBoxLayout(summary_widget)

        # –í—ã–±–æ—Ä —Ñ–∞–π–ª–∞
        file_group = QGroupBox("–í—ã–±–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞")
        file_layout = QHBoxLayout()

        self.summary_file_edit = QLineEdit()
        self.summary_file_edit.setPlaceholderText("–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É...")

        browse_file_button = QPushButton("–û–±–∑–æ—Ä...")
        browse_file_button.clicked.connect(self.browse_summary_file)

        file_layout.addWidget(self.summary_file_edit)
        file_layout.addWidget(browse_file_button)
        file_group.setLayout(file_layout)

        layout.addWidget(file_group)

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        params_group = QGroupBox("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—ã–∂–∏–º–∫–∏")
        params_layout = QVBoxLayout()

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        sentences_layout = QHBoxLayout()
        sentences_layout.addWidget(QLabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π:"))

        self.sentences_spin = QSpinBox()
        self.sentences_spin.setMinimum(1)
        self.sentences_spin.setMaximum(20)
        self.sentences_spin.setValue(5)
        self.sentences_spin.setToolTip("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –≤—ã–∂–∏–º–∫–µ")
        sentences_layout.addWidget(self.sentences_spin)

        sentences_layout.addStretch()
        params_layout.addLayout(sentences_layout)

        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        min_length_layout = QHBoxLayout()
        min_length_layout.addWidget(QLabel("–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è:"))

        self.min_sentence_length_spin = QSpinBox()
        self.min_sentence_length_spin.setMinimum(10)
        self.min_sentence_length_spin.setMaximum(100)
        self.min_sentence_length_spin.setValue(15)
        self.min_sentence_length_spin.setSuffix(" —Å–∏–º–≤–æ–ª–æ–≤")
        self.min_sentence_length_spin.setToolTip(
            "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∫–æ—Ä–æ—á–µ —ç—Ç–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–µ –±—É–¥—É—Ç –≤–∫–ª—é—á–µ–Ω—ã –≤ –≤—ã–∂–∏–º–∫—É"
        )
        min_length_layout.addWidget(self.min_sentence_length_spin)

        min_length_layout.addStretch()
        params_layout.addLayout(min_length_layout)

        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
        min_words_layout = QHBoxLayout()
        min_words_layout.addWidget(QLabel("–ú–∏–Ω–∏–º—É–º —Å–ª–æ–≤ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏:"))

        self.min_words_spin = QSpinBox()
        self.min_words_spin.setMinimum(3)
        self.min_words_spin.setMaximum(20)
        self.min_words_spin.setValue(5)
        self.min_words_spin.setToolTip(
            "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–ª–æ–≤ –±—É–¥—É—Ç –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã"
        )
        min_words_layout.addWidget(self.min_words_spin)

        min_words_layout.addStretch()
        params_layout.addLayout(min_words_layout)

        # –§–ª–∞–∂–æ–∫ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        self.filter_short_checkbox = QCheckBox(
            "–§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ –º–∞–ª–æ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"
        )
        self.filter_short_checkbox.setChecked(True)
        self.filter_short_checkbox.toggled.connect(self.on_filter_toggled)
        params_layout.addWidget(self.filter_short_checkbox)

        params_group.setLayout(params_layout)
        layout.addWidget(params_group)

        # –ö–Ω–æ–ø–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã–∂–∏–º–∫–∏
        button_layout = QHBoxLayout()
        self.summarize_button = QPushButton("–°–æ–∑–¥–∞—Ç—å –≤—ã–∂–∏–º–∫—É")
        self.summarize_button.clicked.connect(self.create_summary)
        button_layout.addWidget(self.summarize_button)

        # –ö–Ω–æ–ø–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—ã–∂–∏–º–∫–∏
        self.save_summary_button = QPushButton("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—ã–∂–∏–º–∫—É")
        self.save_summary_button.clicked.connect(self.save_summary)
        self.save_summary_button.setEnabled(False)
        button_layout.addWidget(self.save_summary_button)

        button_layout.addStretch()
        layout.addLayout(button_layout)

        # –†–µ–∑—É–ª—å—Ç–∞—Ç
        splitter = QSplitter(Qt.Orientation.Vertical)

        # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
        original_group = QGroupBox("–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç")
        original_layout = QVBoxLayout()
        self.original_text = QTextEdit()
        self.original_text.setReadOnly(True)
        original_layout.addWidget(self.original_text)
        original_group.setLayout(original_layout)
        splitter.addWidget(original_group)

        # –í—ã–∂–∏–º–∫–∞
        summary_group = QGroupBox("–í—ã–∂–∏–º–∫–∞")
        summary_layout = QVBoxLayout()
        self.summary_text = QTextEdit()
        self.summary_text.setReadOnly(True)
        summary_layout.addWidget(self.summary_text)
        summary_group.setLayout(summary_layout)
        splitter.addWidget(summary_group)

        layout.addWidget(splitter)

        self.tab_widget.addTab(summary_widget, "üìù –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â—É—é –≤—ã–∂–∏–º–∫—É –¥–ª—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        self.current_summary = []

    def on_filter_toggled(self, checked):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"""
        self.min_sentence_length_spin.setEnabled(checked)
        self.min_words_spin.setEnabled(checked)

        if checked:
            self.status_bar.showMessage("–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤–∫–ª—é—á–µ–Ω–∞")
        else:
            self.status_bar.showMessage(
                "–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞ - –≤—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –±—É–¥—É—Ç —É—á–∏—Ç—ã–≤–∞—Ç—å—Å—è"
            )

    def save_summary(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—ã–∂–∏–º–∫–∏ –≤ —Ñ–∞–π–ª"""
        if not self.current_summary:
            QMessageBox.warning(self, "–û—à–∏–±–∫–∞", "–ù–µ—Ç –≤—ã–∂–∏–º–∫–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return

        file_path, _ = QFileDialog.getSaveFileName(
            self, "–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—ã–∂–∏–º–∫—É", "", "–¢–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã (*.txt);;–í—Å–µ —Ñ–∞–π–ª—ã (*.*)"
        )

        if file_path:
            try:
                with open(file_path, "w", encoding="utf-8") as f:
                    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
                    f.write(f"–í—ã–∂–∏–º–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {self.summary_file_edit.text()}\n")
                    f.write(f"–°–æ–∑–¥–∞–Ω–æ: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write("=" * 60 + "\n\n")

                    # –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤—ã–∂–∏–º–∫–∏
                    for i, sentence in enumerate(self.current_summary, 1):
                        f.write(f"{i}. {sentence.strip()}\n\n")

                    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
                    if hasattr(self, "last_summary_stats"):
                        f.write("\n" + "=" * 60 + "\n")
                        f.write("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–£–ú–ú–ê–†–ò–ó–ê–¶–ò–ò\n")
                        f.write("=" * 60 + "\n")
                        stats = self.last_summary_stats
                        f.write(
                            f"–ò—Å—Ö–æ–¥–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {stats['original_sentences_count']}\n"
                        )
                        f.write(
                            f"–í–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {stats.get('valid_original_sentences_count', '–Ω/–¥')}\n"
                        )
                        f.write(
                            f"–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –≤—ã–∂–∏–º–∫–µ: {stats['summary_sentences_count']}\n"
                        )
                        f.write(
                            f"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è: {stats['compression_ratio']:.1%}\n"
                        )
                        f.write(
                            f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: {stats.get('avg_sentence_length', 0):.1f} —Å–ª–æ–≤\n"
                        )

                QMessageBox.information(
                    self, "–£—Å–ø–µ—Ö", f"–í—ã–∂–∏–º–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤:\n{file_path}"
                )

            except Exception as e:
                QMessageBox.critical(
                    self, "–û—à–∏–±–∫–∞", f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏:\n{str(e)}"
                )

    def create_statistics_tab(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∫–ª–∞–¥–∫–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        stats_widget = QWidget()
        layout = QVBoxLayout(stats_widget)

        # –ö–Ω–æ–ø–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
        refresh_button = QPushButton("–û–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É")
        refresh_button.clicked.connect(self.update_statistics)
        layout.addWidget(refresh_button)

        # –¢–µ–∫—Å—Ç–æ–≤–æ–µ –ø–æ–ª–µ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.statistics_text = QTextEdit()
        self.statistics_text.setReadOnly(True)
        self.statistics_text.setFont(QFont("Consolas", 10))
        layout.addWidget(self.statistics_text)

        self.tab_widget.addTab(stats_widget, "üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")

    def create_evaluation_tab(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∫–ª–∞–¥–∫–∏ –æ—Ü–µ–Ω–∫–∏ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        self.evaluation_widget = EvaluationWidget()
        self.tab_widget.addTab(self.evaluation_widget, "üìö –û—Ü–µ–Ω–∫–∞ –º–µ—Ç–æ–¥–æ–≤")

    def load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        self.model_combo.clear()
        self.model_combo.addItem("–ù–µ –≤—ã–±—Ä–∞–Ω–æ")

        try:
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
            MODELS_DIR.mkdir(exist_ok=True, parents=True)

            # –ò—â–µ–º –º–æ–¥–µ–ª–∏ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            model_files = list(MODELS_DIR.glob("*.model"))

            for model_file in model_files:
                model_name = model_file.stem
                if model_name:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∏–º—è –Ω–µ –ø—É—Å—Ç–æ–µ
                    self.model_combo.addItem(model_name)

            if len(model_files) > 0:
                self.model_combo.setCurrentIndex(1)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π: {e}")
            QMessageBox.warning(
                self, "–û—à–∏–±–∫–∞", f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π: {e}"
            )

    def on_model_changed(self, model_name: str):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"""
        if not model_name or model_name == "–ù–µ –≤—ã–±—Ä–∞–Ω–æ":
            self.current_model = None
            self.search_engine = None
            self.summarizer = None
            self.model_status_label.setText("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            self.model_status_label.setStyleSheet("color: red;")

            # –û—Ç–∫–ª—é—á–∞–µ–º evaluation widget
            if hasattr(self, "evaluation_widget"):
                self.evaluation_widget.set_search_engine(None, None)
            return

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        try:
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}")
            trainer = Doc2VecTrainer()
            model = trainer.load_model(model_name)

            if model:
                self.current_model = model

                # –°–æ–∑–¥–∞–µ–º SearchEngine —Å –±–∞–∑–æ–≤—ã–º –ø—É—Ç–µ–º
                self.search_engine = SemanticSearchEngine(
                    model,
                    trainer.corpus_info,
                    trainer.documents_base_path,  # –ü–µ—Ä–µ–¥–∞–µ–º –±–∞–∑–æ–≤—ã–π –ø—É—Ç—å
                )

                self.summarizer = TextSummarizer(model)

                # –ü–µ—Ä–µ–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ evaluation widget
                if hasattr(self, "evaluation_widget"):
                    self.evaluation_widget.set_search_engine(
                        self.search_engine, trainer.corpus_info
                    )

                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
                status_text = f"–ú–æ–¥–µ–ª—å '{model_name}' –∑–∞–≥—Ä—É–∂–µ–Ω–∞"
                if trainer.documents_base_path:
                    status_text += f" (–±–∞–∑–∞: {trainer.documents_base_path.name})"

                self.model_status_label.setText(status_text)
                self.model_status_label.setStyleSheet("color: green;")

                self.status_bar.showMessage(f"–ú–æ–¥–µ–ª—å '{model_name}' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                if trainer.documents_base_path:
                    logger.info(
                        f"–ë–∞–∑–æ–≤—ã–π –ø—É—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {trainer.documents_base_path}"
                    )
                    logger.info(
                        f"–ü—É—Ç—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {trainer.documents_base_path.exists()}"
                    )
                else:
                    logger.warning("–ë–∞–∑–æ–≤—ã–π –ø—É—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –º–æ–¥–µ–ª–∏")

            else:
                logger.error(f"–ú–æ–¥–µ–ª—å {model_name} –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                self.current_model = None
                self.search_engine = None
                self.summarizer = None
                self.model_status_label.setText("–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏")
                self.model_status_label.setStyleSheet("color: red;")

                if hasattr(self, "evaluation_widget"):
                    self.evaluation_widget.set_search_engine(None, None)

                QMessageBox.warning(
                    self, "–û—à–∏–±–∫–∞", f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å '{model_name}'"
                )
        except Exception as e:
            logger.error(f"–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}", exc_info=True)
            QMessageBox.critical(
                self, "–û—à–∏–±–∫–∞", f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {str(e)}"
            )

    def browse_documents(self):
        """–í—ã–±–æ—Ä –ø–∞–ø–∫–∏ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏"""
        folder = QFileDialog.getExistingDirectory(self, "–í—ã–±–µ—Ä–∏—Ç–µ –ø–∞–ø–∫—É —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏")
        if folder:
            self.docs_path_edit.setText(folder)

    def browse_summary_file(self):
        """–í—ã–±–æ—Ä —Ñ–∞–π–ª–∞ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
        file_path, _ = QFileDialog.getOpenFileName(
            self, "–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª", "", "–î–æ–∫—É–º–µ–Ω—Ç—ã (*.pdf *.docx *.doc);;–í—Å–µ —Ñ–∞–π–ª—ã (*.*)"
        )
        if file_path:
            self.summary_file_edit.setText(file_path)

    def start_training(self):
        """–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        documents_path = self.docs_path_edit.text()
        if not documents_path:
            QMessageBox.warning(self, "–û—à–∏–±–∫–∞", "–í—ã–±–µ—Ä–∏—Ç–µ –ø–∞–ø–∫—É —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏")
            return

        documents_path = Path(documents_path)
        if not documents_path.exists():
            QMessageBox.warning(self, "–û—à–∏–±–∫–∞", "–£–∫–∞–∑–∞–Ω–Ω–∞—è –ø–∞–ø–∫–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            return

        model_name = self.model_name_edit.text().strip()
        if not model_name:
            QMessageBox.warning(self, "–û—à–∏–±–∫–∞", "–í–≤–µ–¥–∏—Ç–µ –∏–º—è –º–æ–¥–µ–ª–∏")
            return

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏
        if "/" in model_name or "\\" in model_name or ":" in model_name:
            QMessageBox.warning(
                self, "–û—à–∏–±–∫–∞", "–ò–º—è –º–æ–¥–µ–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã"
            )
            return

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–∞—è –º–æ–¥–µ–ª—å
        existing_model = MODELS_DIR / f"{model_name}.model"
        if existing_model.exists():
            reply = QMessageBox.question(
                self,
                "–ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ",
                f"–ú–æ–¥–µ–ª—å '{model_name}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ü–µ—Ä–µ–∑–∞–ø–∏—Å–∞—Ç—å?",
                QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No,
            )
            if reply == QMessageBox.StandardButton.No:
                return

        # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—É—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ GUI
        current_params = {
            "vector_size": self.vector_size_spin.value(),
            "window": self.window_spin.value(),
            "min_count": self.min_count_spin.value(),
            "epochs": self.epochs_spin.value(),
            "dm": 1 if self.dm_combo.currentIndex() == 0 else 0,
            "negative": self.negative_spin.value(),
        }

        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –µ—Å–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑–º–µ–Ω–∏–ª–∏—Å—å
        from semantic_search.config import config_manager

        config_changed = False
        current_config = config_manager.config.doc2vec

        for param, value in current_params.items():
            if current_config.get(param) != value:
                config_changed = True
                break

        if config_changed:
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            config_manager.update_config(doc2vec=current_params)
            logger.info("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∞ —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è")

            # –ò–Ω—Ñ–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            self.status_bar.showMessage("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é")

        # –û—Ç–∫–ª—é—á–∞–µ–º –∫–Ω–æ–ø–∫—É
        self.train_button.setEnabled(False)
        self.training_progress.setVisible(True)
        self.training_progress.setValue(0)

        # –û—á–∏—â–∞–µ–º –ª–æ–≥
        self.training_log.clear()
        self.training_log.append("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\n")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.training_log.append("üìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:")
        self.training_log.append(
            f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤: {current_params['vector_size']}"
        )
        self.training_log.append(f"   –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞: {current_params['window']}")
        self.training_log.append(
            f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞: {current_params['min_count']}"
        )
        self.training_log.append(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö: {current_params['epochs']}")
        self.training_log.append(
            f"   –†–µ–∂–∏–º: {'DM' if current_params['dm'] == 1 else 'DBOW'}"
        )
        self.training_log.append(
            f"   Negative sampling: {current_params['negative']}\n"
        )

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–µ—Å–µ—Ç
        preset_index = self.preset_combo.currentIndex()
        preset_map = {0: "balanced", 1: "fast", 2: "quality", 3: None}
        preset = preset_map.get(preset_index)

        # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –ø–æ—Ç–æ–∫
        self.training_thread = TrainingThread(
            documents_path,
            model_name,
            current_params["vector_size"],
            current_params["epochs"],
            window=current_params["window"],
            min_count=current_params["min_count"],
            dm=current_params["dm"],
            negative=current_params["negative"],
            preset=preset,
        )

        self.training_thread.progress.connect(self.on_training_progress)
        self.training_thread.finished.connect(self.on_training_finished)
        self.training_thread.statistics.connect(self.on_training_statistics)

        self.training_thread.start()

    def on_training_progress(self, value: int, message: str):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
        self.training_progress.setValue(value)
        self.training_log.append(message)
        self.status_bar.showMessage(message)

    def on_training_statistics(self, stats: dict):
        """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–æ—Ä–ø—É—Å–∞"""
        stats_text = format_statistics_for_display(stats)
        self.training_log.append("\n" + stats_text + "\n")

    def on_training_finished(self, success: bool, message: str):
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è"""
        self.train_button.setEnabled(True)
        self.training_progress.setVisible(False)

        if success:
            self.training_log.append(f"\n‚úÖ {message}")
            QMessageBox.information(self, "–£—Å–ø–µ—Ö", message)

            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
            self.load_models()

            # –ü—ã—Ç–∞–µ–º—Å—è –≤—ã–±—Ä–∞—Ç—å —Ç–æ–ª—å–∫–æ —á—Ç–æ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
            model_name = self.model_name_edit.text().strip()
            if model_name:
                index = self.model_combo.findText(model_name)
                if index >= 0:
                    self.model_combo.setCurrentIndex(index)
                else:
                    logger.warning(
                        f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –º–æ–¥–µ–ª—å '{model_name}' –≤ —Å–ø–∏—Å–∫–µ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è"
                    )
        else:
            self.training_log.append(f"\n‚ùå {message}")
            QMessageBox.critical(self, "–û—à–∏–±–∫–∞", message)

        self.status_bar.showMessage("–ì–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")

    def perform_search(self):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞"""
        if not self.search_engine:
            QMessageBox.warning(self, "–û—à–∏–±–∫–∞", "–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å")
            return

        query = self.search_input.text().strip()
        if not query:
            QMessageBox.warning(self, "–û—à–∏–±–∫–∞", "–í–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å")
            return

        # –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.results_list.clear()
        self.document_viewer.clear()

        # –û—Ç–∫–ª—é—á–∞–µ–º –∫–Ω–æ–ø–∫—É
        self.search_button.setEnabled(False)
        self.status_bar.showMessage("–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ–∏—Å–∫...")

        # –°–æ–∑–¥–∞–µ–º –ø–æ—Ç–æ–∫ –ø–æ–∏—Å–∫–∞
        self.search_thread = SearchThread(
            self.search_engine, query, self.results_count_spin.value()
        )

        self.search_thread.results.connect(self.on_search_results)
        self.search_thread.error.connect(self.on_search_error)

        self.search_thread.start()

    def on_search_results(self, results: List):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""
        self.search_button.setEnabled(True)

        if not results:
            self.status_bar.showMessage("–†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            QMessageBox.information(
                self, "–ü–æ–∏—Å–∫", "–ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
            )
            return

        self.status_bar.showMessage(f"–ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}")

        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for i, result in enumerate(results, 1):
            item = QListWidgetItem(
                f"{i}. {result.doc_id} (—Å—Ö–æ–∂–µ—Å—Ç—å: {result.similarity:.3f})"
            )
            item.setData(Qt.ItemDataRole.UserRole, result)
            self.results_list.addItem(item)

    def on_search_error(self, error: str):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–∫–∏ –ø–æ–∏—Å–∫–∞"""
        self.search_button.setEnabled(True)
        self.status_bar.showMessage("–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞")
        QMessageBox.critical(self, "–û—à–∏–±–∫–∞", f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {error}")

    def on_result_selected(self, item: QListWidgetItem):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±–æ—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        result = item.data(Qt.ItemDataRole.UserRole)

        try:
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            relative_path = result.doc_id

            logger.info(f"–í—ã–±—Ä–∞–Ω –¥–æ–∫—É–º–µ–Ω—Ç: {relative_path}")

            file_path = None

            # –û—Å–Ω–æ–≤–Ω–æ–π —Å–ø–æ—Å–æ–±: –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π –ø—É—Ç—å –∏–∑ SearchEngine
            if (
                self.search_engine
                and hasattr(self.search_engine, "documents_base_path")
                and self.search_engine.documents_base_path
            ):
                # –°—Ç—Ä–æ–∏–º –ø–æ–ª–Ω—ã–π –ø—É—Ç—å: –±–∞–∑–æ–≤—ã–π –ø—É—Ç—å + –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å
                file_path = self.search_engine.documents_base_path / relative_path

                logger.info(f"–ë–∞–∑–æ–≤—ã–π –ø—É—Ç—å: {self.search_engine.documents_base_path}")
                logger.info(f"–ü–æ–ª–Ω—ã–π –ø—É—Ç—å: {file_path}")
                logger.info(f"–§–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {file_path.exists()}")

                # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–±—É–µ–º —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π —Å–ª–µ—à–µ–π
                if not file_path.exists():
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–ª–µ—à–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–π –û–°
                    normalized_relative = relative_path.replace("/", os.sep).replace(
                        "\\", os.sep
                    )
                    file_path = (
                        self.search_engine.documents_base_path / normalized_relative
                    )

                    if file_path.exists():
                        logger.info("–§–∞–π–ª –Ω–∞–π–¥–µ–Ω –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—É—Ç–µ–π")
            else:
                logger.warning("–ë–∞–∑–æ–≤—ã–π –ø—É—Ç—å –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω –≤ SearchEngine")

            # –ó–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç: –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            if (
                (not file_path or not file_path.exists())
                and result.metadata
                and "full_path" in result.metadata
            ):
                test_path = Path(result.metadata["full_path"])
                if test_path.exists():
                    file_path = test_path
                    logger.info(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω full_path –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {file_path}")

            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            if file_path and file_path.exists():
                extractor = FileExtractor()
                text = extractor.extract_text(file_path)

                if text:
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5000 —Å–∏–º–≤–æ–ª–æ–≤
                    preview = text[:5000]
                    if len(text) > 5000:
                        preview += "\n\n... (—Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω) ..."

                    self.document_viewer.setPlainText(preview)

                    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    metadata_text = "\n\n" + "=" * 60 + "\n"
                    metadata_text += "–ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –î–û–ö–£–ú–ï–ù–¢–ï\n"
                    metadata_text += "=" * 60 + "\n"
                    metadata_text += f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç: {relative_path}\n"
                    metadata_text += f"üìÅ –ü–æ–ª–Ω—ã–π –ø—É—Ç—å: {file_path}\n"
                    metadata_text += f"üìä –°—Ö–æ–∂–µ—Å—Ç—å: {result.similarity:.3f}\n"

                    if self.search_engine and self.search_engine.documents_base_path:
                        metadata_text += f"üìÇ –ë–∞–∑–æ–≤–∞—è –ø–∞–ø–∫–∞ –º–æ–¥–µ–ª–∏: {self.search_engine.documents_base_path}\n"

                    if result.metadata:
                        metadata_text += (
                            f"üíæ –†–∞–∑–º–µ—Ä: {result.metadata.get('file_size', 0):,} –±–∞–π—Ç\n"
                        )
                        metadata_text += (
                            f"üìù –¢–æ–∫–µ–Ω–æ–≤: {result.metadata.get('tokens_count', 0):,}\n"
                        )
                        metadata_text += f"üìë –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ: {result.metadata.get('extension', '–Ω/–¥')}\n"

                    self.document_viewer.append(metadata_text)
                else:
                    self.document_viewer.setPlainText(
                        f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞:\n{file_path}\n\n"
                        f"–í–æ–∑–º–æ–∂–Ω–æ, —Ñ–∞–π–ª –ø–æ–≤—Ä–µ–∂–¥–µ–Ω –∏–ª–∏ –∏–º–µ–µ—Ç –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç."
                    )
            else:
                # –ü–æ–¥—Ä–æ–±–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ
                error_msg = "‚ùå –§–ê–ô–õ –ù–ï –ù–ê–ô–î–ï–ù\n\n"
                error_msg += f"–ò—Å–∫–æ–º—ã–π –¥–æ–∫—É–º–µ–Ω—Ç: {relative_path}\n\n"

                if self.search_engine and hasattr(
                    self.search_engine, "documents_base_path"
                ):
                    if self.search_engine.documents_base_path:
                        error_msg += f"–ë–∞–∑–æ–≤–∞—è –ø–∞–ø–∫–∞ –º–æ–¥–µ–ª–∏: {self.search_engine.documents_base_path}\n"
                        error_msg += f"–û–∂–∏–¥–∞–µ–º—ã–π –ø—É—Ç—å: {self.search_engine.documents_base_path / relative_path}\n"
                        error_msg += f"–ë–∞–∑–æ–≤–∞—è –ø–∞–ø–∫–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {'‚úÖ –î–∞' if self.search_engine.documents_base_path.exists() else '‚ùå –ù–µ—Ç'}\n"
                    else:
                        error_msg += "‚ö†Ô∏è –ë–∞–∑–æ–≤—ã–π –ø—É—Ç—å –Ω–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –º–æ–¥–µ–ª–∏\n"

                error_msg += "\nüìã –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:\n"
                error_msg += "1. –§–∞–π–ª—ã –±—ã–ª–∏ –ø–µ—Ä–µ–º–µ—â–µ–Ω—ã –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏\n"
                error_msg += "2. –ú–æ–¥–µ–ª—å –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –Ω–∞ –¥—Ä—É–≥–æ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–µ\n"
                error_msg += "3. –ò–∑–º–µ–Ω–∏–ª–∞—Å—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–∞–ø–æ–∫\n"

                if not (self.search_engine and self.search_engine.documents_base_path):
                    error_msg += "4. –ú–æ–¥–µ–ª—å –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ —Å—Ç–∞—Ä–æ–π –≤–µ—Ä—Å–∏–µ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –±–∞–∑–æ–≤–æ–≥–æ –ø—É—Ç–∏\n"

                error_msg += "\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:\n"
                error_msg += "‚Ä¢ –ü–µ—Ä–µ–æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å —Å —Ç–µ–∫—É—â–∏–º —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n"
                error_msg += "‚Ä¢ –ò–ª–∏ –ø–µ—Ä–µ–º–µ—Å—Ç–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –∏—Å—Ö–æ–¥–Ω–æ–µ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ\n"

                self.document_viewer.setPlainText(error_msg)

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}", exc_info=True)
            self.document_viewer.setPlainText(
                f"‚ùå –û–®–ò–ë–ö–ê –ü–†–ò –ó–ê–ì–†–£–ó–ö–ï –î–û–ö–£–ú–ï–ù–¢–ê\n\n"
                f"–î–æ–∫—É–º–µ–Ω—Ç: {result.doc_id}\n"
                f"–û—à–∏–±–∫–∞: {str(e)}\n\n"
                f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."
            )

    def create_summary(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—ã–∂–∏–º–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —É—á–µ—Ç–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"""
        if not self.summarizer:
            QMessageBox.warning(self, "–û—à–∏–±–∫–∞", "–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å")
            return

        file_path = self.summary_file_edit.text()
        if not file_path:
            QMessageBox.warning(self, "–û—à–∏–±–∫–∞", "–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏")
            return

        file_path = Path(file_path)
        if not file_path.exists():
            QMessageBox.warning(self, "–û—à–∏–±–∫–∞", "–§–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            return

        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç
            extractor = FileExtractor()
            text = extractor.extract_text(file_path)

            if not text:
                QMessageBox.warning(self, "–û—à–∏–±–∫–∞", "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞")
                return

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞
            text_length = len(text)
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω —Ç–µ–∫—Å—Ç –¥–ª–∏–Ω–æ–π {text_length} —Å–∏–º–≤–æ–ª–æ–≤")

            # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –¥–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
            if text_length > 2_000_000:
                reply = QMessageBox.question(
                    self,
                    "–ë–æ–ª—å—à–æ–π —Ñ–∞–π–ª",
                    f"–§–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç {text_length:,} —Å–∏–º–≤–æ–ª–æ–≤.\n"
                    "–û–±—Ä–∞–±–æ—Ç–∫–∞ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç.\n"
                    "–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å?",
                    QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No,
                )
                if reply == QMessageBox.StandardButton.No:
                    return

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç (–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø—Ä–µ–≤—å—é)
            preview_length = min(5000, text_length)
            preview = text[:preview_length]
            if text_length > preview_length:
                preview += f"\n\n... (–ø–æ–∫–∞–∑–∞–Ω—ã –ø–µ—Ä–≤—ã–µ {preview_length:,} –∏–∑ {text_length:,} —Å–∏–º–≤–æ–ª–æ–≤) ..."
            self.original_text.setPlainText(preview)

            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä–∞ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
            if self.filter_short_checkbox.isChecked():
                self.summarizer.min_summary_sentence_length = (
                    self.min_sentence_length_spin.value()
                )
                self.summarizer.min_words_in_sentence = self.min_words_spin.value()
            else:
                # –ï—Å–ª–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                self.summarizer.min_summary_sentence_length = 1
                self.summarizer.min_words_in_sentence = 1

            # –°–æ–∑–¥–∞–µ–º –≤—ã–∂–∏–º–∫—É
            self.status_bar.showMessage(
                "–°–æ–∑–¥–∞–Ω–∏–µ –≤—ã–∂–∏–º–∫–∏... –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤"
            )
            QApplication.processEvents()  # –û–±–Ω–æ–≤–ª—è–µ–º UI

            sentences_count = self.sentences_spin.value()

            # –û—Ç–∫–ª—é—á–∞–µ–º –∫–Ω–æ–ø–∫–∏ –Ω–∞ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            self.summarize_button.setEnabled(False)
            self.save_summary_button.setEnabled(False)

            try:
                # –°–æ–∑–¥–∞–µ–º –≤—ã–∂–∏–º–∫—É —Å —É—á–µ—Ç–æ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                summary = self.summarizer.summarize_text(
                    text,
                    sentences_count=sentences_count,
                    min_sentence_length=self.min_sentence_length_spin.value()
                    if self.filter_short_checkbox.isChecked()
                    else None,
                )

                if summary:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â—É—é –≤—ã–∂–∏–º–∫—É
                    self.current_summary = summary

                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—ã–∂–∏–º–∫—É
                    summary_text = "\n\n".join(
                        f"{i}. {sent.strip()}" for i, sent in enumerate(summary, 1)
                    )
                    self.summary_text.setPlainText(summary_text)

                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                    stats = self.summarizer.get_summary_statistics(text, summary)
                    self.last_summary_stats = (
                        stats  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                    )

                    stats_text = "\n\n--- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ---\n"
                    stats_text += (
                        f"–ò—Å—Ö–æ–¥–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {stats['original_sentences_count']}\n"
                    )

                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                    if (
                        self.filter_short_checkbox.isChecked()
                        and "valid_original_sentences_count" in stats
                    ):
                        filtered_count = (
                            stats["original_sentences_count"]
                            - stats["valid_original_sentences_count"]
                        )
                        stats_text += f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –∫–æ—Ä–æ—Ç–∫–∏—Ö: {filtered_count}\n"
                        stats_text += f"–í–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {stats['valid_original_sentences_count']}\n"

                    stats_text += (
                        f"–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –≤—ã–∂–∏–º–∫–µ: {stats['summary_sentences_count']}\n"
                    )
                    stats_text += f"–°–∂–∞—Ç–∏–µ: {stats['compression_ratio']:.1%}\n"
                    stats_text += (
                        f"–ò—Å—Ö–æ–¥–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤: {stats['original_chars_count']:,}\n"
                    )
                    stats_text += (
                        f"–°–∏–º–≤–æ–ª–æ–≤ –≤ –≤—ã–∂–∏–º–∫–µ: {stats['summary_chars_count']:,}\n"
                    )

                    if "avg_sentence_length" in stats:
                        stats_text += f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: {stats['avg_sentence_length']:.1f} —Å–ª–æ–≤\n"

                    self.summary_text.append(stats_text)

                    # –í–∫–ª—é—á–∞–µ–º –∫–Ω–æ–ø–∫—É —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                    self.save_summary_button.setEnabled(True)
                    self.status_bar.showMessage("–í—ã–∂–∏–º–∫–∞ —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                else:
                    QMessageBox.warning(
                        self,
                        "–û—à–∏–±–∫–∞",
                        "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≤—ã–∂–∏–º–∫—É.\n"
                        "–í–æ–∑–º–æ–∂–Ω–æ, –≤—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ.\n"
                        "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.",
                    )
                    self.status_bar.showMessage("–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã–∂–∏–º–∫–∏")

            finally:
                self.summarize_button.setEnabled(True)

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤—ã–∂–∏–º–∫–∏: {e}")
            self.summarize_button.setEnabled(True)

            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–∫–∏ SpaCy
            if "exceeds maximum" in str(e):
                QMessageBox.critical(
                    self,
                    "–û—à–∏–±–∫–∞",
                    "–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.\n"
                    "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Ñ–∞–π–ª –º–µ–Ω—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –∏–ª–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ –µ–≥–æ –Ω–∞ —á–∞—Å—Ç–∏.",
                )
            else:
                QMessageBox.critical(
                    self, "–û—à–∏–±–∫–∞", f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤—ã–∂–∏–º–∫–∏: {str(e)}"
                )

            self.status_bar.showMessage("–û—à–∏–±–∫–∞")

    def update_statistics(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        stats_text = "üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ò–°–¢–ï–ú–´\n" + "=" * 50 + "\n\n"

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–æ–¥–µ–ª–µ–π
        stats_text += "üß† –ú–û–î–ï–õ–ò:\n"
        model_files = list(MODELS_DIR.glob("*.model"))
        stats_text += f"–í—Å–µ–≥–æ –º–æ–¥–µ–ª–µ–π: {len(model_files)}\n\n"

        # –ü–æ–ª—É—á–∞–µ–º –∏–º—è —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏
        current_model_name = (
            self.model_combo.currentText() if hasattr(self, "model_combo") else None
        )
        is_current_model_shown = False

        for model_file in model_files:
            model_name = model_file.stem
            file_size_mb = model_file.stat().st_size / 1024 / 1024

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª—å—é
            is_current = (
                current_model_name == model_name and self.current_model is not None
            )

            if is_current:
                is_current_model_shown = True
                stats_text += f"üìç –¢–ï–ö–£–©–ê–Ø –ú–û–î–ï–õ–¨: {model_name}\n"
            else:
                stats_text += f"üìÅ {model_name}:\n"

            stats_text += f"   –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size_mb:.1f} –ú–ë\n"

            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
            metadata_file = MODELS_DIR / f"{model_name}_metadata.json"
            if metadata_file.exists():
                try:
                    with open(metadata_file, "r", encoding="utf-8") as f:
                        metadata = json.load(f)

                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –Ω–µ-—Ç–µ–∫—É—â–∏—Ö –º–æ–¥–µ–ª–µ–π
                    if not is_current:
                        training_time = metadata.get(
                            "training_time_formatted", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
                        )
                        training_date = metadata.get("training_date", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
                        corpus_size = metadata.get("corpus_size", 0)
                        stats_text += f"   –î–∞—Ç–∞ –æ–±—É—á–µ–Ω–∏—è: {training_date}\n"
                        stats_text += f"   –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_time}\n"
                        stats_text += f"   –†–∞–∑–º–µ—Ä –∫–æ—Ä–ø—É—Å–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {corpus_size}\n"

                    else:
                        # –î–ª—è —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                        trainer = Doc2VecTrainer()
                        trainer.model = self.current_model
                        trainer.training_metadata = metadata

                        model_info = trainer.get_model_info()

                        stats_text += (
                            f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤: {model_info['vector_size']}\n"
                        )
                        stats_text += f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {model_info['vocabulary_size']:,} —Å–ª–æ–≤\n"
                        stats_text += (
                            f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –º–æ–¥–µ–ª–∏: {model_info['documents_count']}\n"
                        )
                        stats_text += f"   –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞: {model_info['window']}\n"
                        stats_text += (
                            f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞: {model_info['min_count']}\n"
                        )
                        stats_text += f"   –≠–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è: {model_info['epochs']}\n"

                        stats_text += f"   –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {model_info['training_time_formatted']}\n"
                        stats_text += (
                            f"   –î–∞—Ç–∞ –æ–±—É—á–µ–Ω–∏—è: {model_info['training_date']}\n"
                        )

                        # –†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
                        if model_info["dm"] == 1:
                            stats_text += "   –†–µ–∂–∏–º: Distributed Memory (DM)\n"
                        else:
                            stats_text += "   –†–µ–∂–∏–º: Distributed Bag of Words (DBOW)\n"

                except Exception as e:
                    logger.debug(
                        f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è {model_name}: {e}"
                    )

            stats_text += "\n"

        # –ï—Å–ª–∏ —Ç–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å –Ω–µ –±—ã–ª–∞ –ø–æ–∫–∞–∑–∞–Ω–∞ –≤ —Å–ø–∏—Å–∫–µ (–Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ)
        if self.current_model and not is_current_model_shown:
            stats_text += "‚ö†Ô∏è –¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –Ω–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã\n\n"

        # –°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        try:
            import psutil

            stats_text += "üíª –°–ò–°–¢–ï–ú–ê:\n"
            stats_text += f"CPU: {psutil.cpu_percent()}% –∑–∞–≥—Ä—É–∑–∫–∞\n"
            stats_text += f"–ü–∞–º—è—Ç—å: {psutil.virtual_memory().percent}% –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ\n"
            stats_text += f"–°–≤–æ–±–æ–¥–Ω–æ –ø–∞–º—è—Ç–∏: {psutil.virtual_memory().available / 1024 / 1024 / 1024:.1f} –ì–ë\n"
        except ImportError:
            stats_text += "\nüíª –°–ò–°–¢–ï–ú–ê:\n"
            stats_text += "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ psutil –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\n"

        self.statistics_text.setPlainText(stats_text)

    def load_model_dialog(self):
        """–î–∏–∞–ª–æ–≥ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏"""
        file_path, _ = QFileDialog.getOpenFileName(
            self,
            "–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏",
            str(MODELS_DIR),
            "–ú–æ–¥–µ–ª–∏ (*.model);;–í—Å–µ —Ñ–∞–π–ª—ã (*.*)",
        )

        if file_path:
            model_name = Path(file_path).stem

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –º–æ–¥–µ–ª—å –≤ —Å–ø–∏—Å–∫–µ
            index = self.model_combo.findText(model_name)
            if index >= 0:
                self.model_combo.setCurrentIndex(index)
            else:
                # –ö–æ–ø–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –≤ –Ω–∞—à—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                import shutil

                try:
                    shutil.copy2(file_path, MODELS_DIR / Path(file_path).name)
                    self.load_models()
                    self.model_combo.setCurrentText(model_name)
                except Exception as e:
                    QMessageBox.critical(
                        self, "–û—à–∏–±–∫–∞", f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: {str(e)}"
                    )

    def show_about(self):
        """–ü–æ–∫–∞–∑–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–≥—Ä–∞–º–º–µ"""
        about_text = """
        <h2>Semantic Document Search</h2>
        <p>–í–µ—Ä—Å–∏—è 1.0.0</p>
        <p>–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ Doc2Vec.</p>
        <p><b>–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:</b></p>
        <ul>
        <li>–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤</li>
        <li>–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ñ–æ—Ä–º–∞—Ç–æ–≤ PDF, DOCX, DOC</li>
        <li>–°–æ–∑–¥–∞–Ω–∏–µ –≤—ã–∂–∏–º–æ–∫ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤</li>
        <li>–û–±—É—á–µ–Ω–∏–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π</li>
        </ul>
        <p><b>–ê–≤—Ç–æ—Ä:</b> Evgeny Odintsov</p>
        <p><b>Email:</b> ev1genial@gmail.com</p>
        """

        QMessageBox.about(self, "–û –ø—Ä–æ–≥—Ä–∞–º–º–µ", about_text)

    def closeEvent(self, event):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è –æ–∫–Ω–∞"""
        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Ç–æ–∫–∏ –µ—Å–ª–∏ –æ–Ω–∏ –∑–∞–ø—É—â–µ–Ω—ã
        if self.training_thread and self.training_thread.isRunning():
            self.training_thread.cancel()
            self.training_thread.wait()

        if self.search_thread and self.search_thread.isRunning():
            self.search_thread.wait()

        event.accept()


========================================
FILE: src\semantic_search\utils\__init__.py
========================================
"""–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã"""

from .cache_manager import CacheManager
from .file_utils import FileExtractor
from .logging_config import logging_manager, setup_logging
from .notification_system import (
    NotificationManager,
    ProgressTracker,
    notification_manager,
)
from .performance_monitor import PerformanceMonitor
from .task_manager import TaskManager, task_manager
from .text_utils import TextProcessor
from .validators import DataValidator, FileValidator, ValidationError

__all__ = [
    "CacheManager",
    "FileExtractor",
    "setup_logging",
    "logging_manager",
    "NotificationManager",
    "notification_manager",
    "ProgressTracker",
    "PerformanceMonitor",
    "TaskManager",
    "task_manager",
    "TextProcessor",
    "DataValidator",
    "FileValidator",
    "ValidationError",
]


========================================
FILE: src\semantic_search\utils\cache_manager.py
========================================
"""–ú–µ–Ω–µ–¥–∂–µ—Ä –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""

import hashlib
import pickle
from pathlib import Path
from typing import Any, Optional

from loguru import logger


class CacheManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""

    def __init__(self, cache_dir: Path):
        self.cache_dir = cache_dir
        self.cache_dir.mkdir(exist_ok=True, parents=True)

    def _get_cache_key(self, data: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞"""
        return hashlib.md5(data.encode()).hexdigest()

    def get(self, key: str) -> Optional[Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫—ç—à–∞"""
        cache_file = self.cache_dir / f"{self._get_cache_key(key)}.pkl"

        if cache_file.exists():
            try:
                with open(cache_file, "rb") as f:
                    return pickle.load(f)
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫—ç—à–∞: {e}")

        return None

    def set(self, key: str, value: Any) -> bool:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –∫—ç—à"""
        cache_file = self.cache_dir / f"{self._get_cache_key(key)}.pkl"

        try:
            with open(cache_file, "wb") as f:
                pickle.dump(value, f)
            return True
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ –∫—ç—à: {e}")
            return False

    def clear(self) -> bool:
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞"""
        try:
            for cache_file in self.cache_dir.glob("*.pkl"):
                cache_file.unlink()
            logger.info("–ö—ç—à –æ—á–∏—â–µ–Ω")
            return True
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞: {e}")
            return False


========================================
FILE: src\semantic_search\utils\file_utils.py
========================================
"""–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ñ–∞–π–ª–∞–º–∏"""

from collections import Counter
from pathlib import Path
from typing import List, Optional

import pymupdf
from docx import Document as DocxDocument
from loguru import logger

from semantic_search.config import SUPPORTED_EXTENSIONS

try:
    import win32com.client

    DOC_SUPPORT = True
except ImportError:
    DOC_SUPPORT = False
    logger.warning("pywin32 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ü–æ–¥–¥–µ—Ä–∂–∫–∞ .doc —Ñ–∞–π–ª–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")


class FileExtractor:
    """–ö–ª–∞—Å—Å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Ñ–∞–π–ª–æ–≤"""

    def __init__(self):
        self.word_app: Optional[object] = None
        if DOC_SUPPORT:
            try:
                self.word_app = win32com.client.Dispatch("Word.Application")
                self.word_app.Visible = False
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Word Application: {e}")

    def __del__(self):
        """–û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        if self.word_app is not None:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ–±—ä–µ–∫—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–µ—Ç–æ–¥ Quit
                if hasattr(self.word_app, "Quit"):
                    self.word_app.Quit()
            except Exception:
                pass

    def find_documents(self, root_path: Path) -> List[Path]:
        """
        –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏

        Args:
            root_path: –ü—É—Ç—å –∫ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏

        Returns:
            –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –Ω–∞–π–¥–µ–Ω–Ω—ã–º —Ñ–∞–π–ª–∞–º
        """
        if not root_path.exists():
            raise FileNotFoundError(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {root_path}")

        found_files = []
        logger.info(f"–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤: {root_path}")

        for file_path in root_path.rglob("*"):
            if file_path.is_file() and file_path.suffix.lower() in SUPPORTED_EXTENSIONS:
                found_files.append(file_path)

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(found_files)}")
        ext_counter = Counter(f.suffix.lower() for f in found_files)
        for ext in SUPPORTED_EXTENSIONS:
            count = ext_counter.get(ext, 0)
            if count > 0:
                logger.info(f"  {ext}: {count}")

        return found_files

    def extract_from_pdf(self, file_path: Path) -> str:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤"""
        try:
            doc = pymupdf.open(file_path)
            text_parts = []

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
            total_pages = len(doc)
            if total_pages > 1000:
                logger.warning(
                    f"PDF —Å–æ–¥–µ—Ä–∂–∏—Ç {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü. –û–±—Ä–∞–±–æ—Ç–∫–∞ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è"
                )

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
            page_iterator = range(len(doc))
            if total_pages > 50:
                try:
                    from tqdm import tqdm

                    page_iterator = tqdm(
                        page_iterator, desc=f"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ {file_path.name}"
                    )
                except ImportError:
                    pass

            for page_num in page_iterator:
                try:
                    page = doc[page_num]
                    page_text = page.get_text()

                    # –§–∏–ª—å—Ç—Ä—É–µ–º —è–≤–Ω–æ –º—É—Å–æ—Ä–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    if len(page_text.strip()) > 50:  # –ú–∏–Ω–∏–º—É–º 50 —Å–∏–º–≤–æ–ª–æ–≤
                        text_parts.append(page_text)

                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
                    continue

            doc.close()

            full_text = "\n".join(text_parts)
            logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(full_text)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü")

            return full_text.strip()

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF {file_path}: {e}")
            return ""

    def extract_from_docx(self, file_path: Path) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOCX —Ñ–∞–π–ª–∞"""
        try:
            doc = DocxDocument(file_path)
            text_parts = []

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    text_parts.append(paragraph.text.strip())

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Ç–∞–±–ª–∏—Ü
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        if cell.text.strip():
                            text_parts.append(cell.text.strip())

            return "\n".join(text_parts)

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOCX {file_path}: {e}")
            return ""

    def extract_from_doc(self, file_path: Path) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOC —Ñ–∞–π–ª–∞ (—Ç–æ–ª—å–∫–æ Windows)"""
        if not DOC_SUPPORT or self.word_app is None:
            logger.warning(f"–ü–æ–¥–¥–µ—Ä–∂–∫–∞ .doc —Ñ–∞–π–ª–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {file_path}")
            return ""

        try:
            doc = self.word_app.Documents.Open(str(file_path.absolute()))
            text = doc.Content.Text
            doc.Close()
            return text.strip()

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOC {file_path}: {e}")
            return ""

    def extract_text(self, file_path: Path) -> str:
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞

        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É

        Returns:
            –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        extension = file_path.suffix.lower()

        if extension == ".pdf":
            return self.extract_from_pdf(file_path)
        elif extension == ".docx":
            return self.extract_from_docx(file_path)
        elif extension == ".doc":
            return self.extract_from_doc(file_path)
        else:
            logger.warning(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {file_path}")
            return ""


========================================
FILE: src\semantic_search\utils\logging_config.py
========================================
"""–£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""

import sys
from typing import Optional

from loguru import logger

from semantic_search.config import LOGS_DIR


class LoggingManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä —Å–∏—Å—Ç–µ–º—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""

    def __init__(self):
        self.handlers = {}
        self.is_configured = False

    def setup_logging(
        self,
        level: str = "INFO",
        enable_file_logging: bool = True,
        enable_rotation: bool = True,
        custom_format: Optional[str] = None,
    ) -> None:
        """
        –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∏—Å—Ç–µ–º—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è

        Args:
            level: –£—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            enable_file_logging: –í–∫–ª—é—á–∏—Ç—å –∑–∞–ø–∏—Å—å –≤ —Ñ–∞–π–ª
            enable_rotation: –í–∫–ª—é—á–∏—Ç—å —Ä–æ—Ç–∞—Ü–∏—é –ª–æ–≥–æ–≤
            custom_format: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç –ª–æ–≥–æ–≤
        """
        if self.is_configured:
            return

        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π handler
        logger.remove()

        # –§–æ—Ä–º–∞—Ç –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏
        console_format = custom_format or (
            "<green>{time:YYYY-MM-DD HH:mm:ss}</green> | "
            "<level>{level: <8}</level> | "
            "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> | "
            "<level>{message}</level>"
        )

        # –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π –≤—ã–≤–æ–¥
        console_handler = logger.add(
            sys.stderr,
            level=level,
            colorize=True,
            format=console_format,
            diagnose=True,
            backtrace=True,
        )
        self.handlers["console"] = console_handler

        if enable_file_logging:
            # –û—Å–Ω–æ–≤–Ω–æ–π –ª–æ–≥ —Ñ–∞–π–ª
            main_log = LOGS_DIR / "semantic_search.log"
            file_format = "{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} | {message}"

            if enable_rotation:
                main_handler = logger.add(
                    main_log,
                    level=level,
                    format=file_format,
                    rotation="10 MB",
                    retention="2 weeks",
                    compression="zip",
                    serialize=False,
                )
            else:
                main_handler = logger.add(main_log, level=level, format=file_format)

            self.handlers["main"] = main_handler

            # –û—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –æ—à–∏–±–æ–∫
            error_log = LOGS_DIR / "errors.log"
            error_handler = logger.add(
                error_log,
                level="ERROR",
                format=file_format,
                rotation="5 MB",
                retention="1 month",
                compression="zip",
            )
            self.handlers["error"] = error_handler

            # –õ–æ–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            perf_log = LOGS_DIR / "performance.log"
            perf_handler = logger.add(
                perf_log,
                level="INFO",
                format="{time:YYYY-MM-DD HH:mm:ss} | {message}",
                filter=lambda record: "PERF" in record["message"],
                rotation="50 MB",
                retention="1 week",
            )
            self.handlers["performance"] = perf_handler

        self.is_configured = True
        logger.info("–°–∏—Å—Ç–µ–º–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞")

        # –õ–æ–≥–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        self._log_system_info()

    def _log_system_info(self):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–∏—Å—Ç–µ–º–µ"""
        try:
            import platform

            import psutil

            logger.info(f"–°–∏—Å—Ç–µ–º–∞: {platform.system()} {platform.release()}")
            logger.info(f"Python: {platform.python_version()}")
            logger.info(f"–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä: {platform.processor()}")
            logger.info(f"–û–ó–£: {psutil.virtual_memory().total / 1024**3:.1f} –ì–ë")
            logger.info(
                f"–°–≤–æ–±–æ–¥–Ω–æ–µ –º–µ—Å—Ç–æ: {psutil.disk_usage('/').free / 1024**3:.1f} –ì–ë"
            )

        except Exception as e:
            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é: {e}")

    def add_performance_log(self, operation: str, duration: float, **kwargs):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        perf_data = {"operation": operation, "duration": duration, **kwargs}

        logger.info(f"PERF: {perf_data}")

    def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        for handler_id in self.handlers.values():
            try:
                logger.remove(handler_id)
            except ValueError:
                pass

        self.handlers.clear()
        self.is_configured = False


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
logging_manager = LoggingManager()


def setup_logging(level: str = "INFO") -> None:
    """–û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å"""
    logging_manager.setup_logging(level)


========================================
FILE: src\semantic_search\utils\notification_system.py
========================================
"""–°–∏—Å—Ç–µ–º–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""

import time
from dataclasses import dataclass
from enum import Enum
from queue import Empty, Queue
from threading import Event, Thread
from typing import Any, Callable, Dict, Optional

from loguru import logger


class NotificationType(Enum):
    """–¢–∏–ø—ã —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π"""

    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    SUCCESS = "success"
    PROGRESS = "progress"


@dataclass
class Notification:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è"""

    type: NotificationType
    title: str
    message: str
    details: Optional[str] = None
    progress: Optional[float] = None  # 0.0 - 1.0
    timestamp: float = None

    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = time.time()


class ProgressTracker:
    """–¢—Ä–µ–∫–µ—Ä –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–ø–µ—Ä–∞—Ü–∏–π"""

    def __init__(self, total_steps: int, description: str = ""):
        self.total_steps = total_steps
        self.current_step = 0
        self.description = description
        self.start_time = time.time()
        self.callbacks = []

    def add_callback(self, callback: Callable[[Dict[str, Any]], None]):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ callback –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        self.callbacks.append(callback)

    def update(self, step: Optional[int] = None, message: str = ""):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        if step is not None:
            self.current_step = step
        else:
            self.current_step += 1

        progress = min(self.current_step / self.total_steps, 1.0)
        elapsed = time.time() - self.start_time

        if progress > 0:
            eta = elapsed / progress * (1 - progress)
        else:
            eta = 0

        progress_info = {
            "progress": progress,
            "current_step": self.current_step,
            "total_steps": self.total_steps,
            "elapsed": elapsed,
            "eta": eta,
            "message": message,
            "description": self.description,
        }

        # –í—ã–∑—ã–≤–∞–µ–º callbacks
        for callback in self.callbacks:
            try:
                callback(progress_info)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ progress callback: {e}")

    def finish(self, message: str = "–ó–∞–≤–µ—Ä—à–µ–Ω–æ"):
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏"""
        self.current_step = self.total_steps
        self.update(message=message)


class NotificationManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä —Å–∏—Å—Ç–µ–º—ã —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π"""

    def __init__(self):
        self.subscribers = []
        self.notification_queue = Queue()
        self.is_running = False
        self.worker_thread = None
        self.stop_event = Event()

    def start(self):
        """–ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º—ã —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π"""
        if self.is_running:
            return

        self.is_running = True
        self.stop_event.clear()
        self.worker_thread = Thread(target=self._worker, daemon=True)
        self.worker_thread.start()

        logger.info("–°–∏—Å—Ç–µ–º–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π –∑–∞–ø—É—â–µ–Ω–∞")

    def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–∏—Å—Ç–µ–º—ã —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π"""
        if not self.is_running:
            return

        self.is_running = False
        self.stop_event.set()

        if self.worker_thread:
            self.worker_thread.join(timeout=1.0)

        logger.info("–°–∏—Å—Ç–µ–º–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")

    def subscribe(self, callback: Callable[[Notification], None]):
        """–ü–æ–¥–ø–∏—Å–∫–∞ –Ω–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è"""
        self.subscribers.append(callback)

    def unsubscribe(self, callback: Callable[[Notification], None]):
        """–û—Ç–ø–∏—Å–∫–∞ –æ—Ç —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π"""
        if callback in self.subscribers:
            self.subscribers.remove(callback)

    def notify(self, notification: Notification):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è"""
        if self.is_running:
            self.notification_queue.put(notification)
        else:
            # –ï—Å–ª–∏ —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –∑–∞–ø—É—â–µ–Ω–∞, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å—Ä–∞–∑—É
            self._send_notification(notification)

    def _worker(self):
        """–†–∞–±–æ—á–∏–π –ø–æ—Ç–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π"""
        while not self.stop_event.is_set():
            try:
                notification = self.notification_queue.get(timeout=0.1)
                self._send_notification(notification)
                self.notification_queue.task_done()
            except Empty:
                continue
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ worker —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π: {e}")

    def _send_notification(self, notification: Notification):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –ø–æ–¥–ø–∏—Å—á–∏–∫–∞–º"""
        for callback in self.subscribers:
            try:
                callback(notification)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ callback —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è: {e}")

    # –£–¥–æ–±–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π
    def info(self, title: str, message: str, details: Optional[str] = None):
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ"""
        self.notify(Notification(NotificationType.INFO, title, message, details))

    def warning(self, title: str, message: str, details: Optional[str] = None):
        """–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ"""
        self.notify(Notification(NotificationType.WARNING, title, message, details))

    def error(self, title: str, message: str, details: Optional[str] = None):
        """–£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ"""
        self.notify(Notification(NotificationType.ERROR, title, message, details))

    def success(self, title: str, message: str, details: Optional[str] = None):
        """–£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ–± —É—Å–ø–µ—Ö–µ"""
        self.notify(Notification(NotificationType.SUCCESS, title, message, details))


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
notification_manager = NotificationManager()


========================================
FILE: src\semantic_search\utils\performance_monitor.py
========================================
"""–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""

import time
from contextlib import contextmanager
from typing import Any, Dict

import psutil
from loguru import logger


class PerformanceMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ–ø–µ—Ä–∞—Ü–∏–π"""

    def __init__(self):
        self.metrics = {}

    @contextmanager
    def measure_operation(self, operation_name: str):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –æ–ø–µ—Ä–∞—Ü–∏–∏"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB

        try:
            yield
        finally:
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB

            duration = end_time - start_time
            memory_delta = end_memory - start_memory

            self.metrics[operation_name] = {
                "duration": duration,
                "memory_start": start_memory,
                "memory_end": end_memory,
                "memory_delta": memory_delta,
                "timestamp": time.time(),
            }

            logger.info(
                f"{operation_name}: {duration:.2f}s, –ü–∞–º—è—Ç—å: {memory_delta:+.1f}MB"
            )

    def get_system_info(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–∏—Å—Ç–µ–º–µ"""
        return {
            "cpu_count": psutil.cpu_count(),
            "cpu_percent": psutil.cpu_percent(interval=1),
            "memory_total": psutil.virtual_memory().total / 1024**3,  # GB
            "memory_available": psutil.virtual_memory().available / 1024**3,  # GB
            "memory_percent": psutil.virtual_memory().percent,
            "disk_usage": psutil.disk_usage("/").percent
            if psutil.disk_usage("/")
            else 0,
        }


========================================
FILE: src\semantic_search\utils\statistics.py
========================================
"""–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""

from collections import Counter
from typing import Any, Dict, List

from semantic_search.core.document_processor import ProcessedDocument


def calculate_statistics_from_processed_docs(
    docs_data: List[ProcessedDocument],
) -> Dict[str, Any]:
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–∑ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

    Args:
        docs_data: –°–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π:
        - processed_files: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        - total_tokens: –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
        - avg_tokens_per_doc: —Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç
        - extensions_count: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º
        - largest_doc: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∞–º–æ–º –±–æ–ª—å—à–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ
        - smallest_doc: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∞–º–æ–º –º–∞–ª–µ–Ω—å–∫–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ
        - total_chars: –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤
        - avg_chars_per_doc: —Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç
    """
    if not docs_data:
        return {
            "processed_files": 0,
            "total_tokens": 0,
            "avg_tokens_per_doc": 0.0,
            "extensions_count": {},
            "largest_doc": None,
            "smallest_doc": None,
            "total_chars": 0,
            "avg_chars_per_doc": 0.0,
        }

    # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = {
        "processed_files": len(docs_data),
        "total_tokens": sum(doc.metadata["tokens_count"] for doc in docs_data),
        "total_chars": sum(doc.metadata["text_length"] for doc in docs_data),
        "extensions_count": dict(
            Counter(doc.metadata["extension"] for doc in docs_data)
        ),
    }

    # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    stats["avg_tokens_per_doc"] = stats["total_tokens"] / stats["processed_files"]
    stats["avg_chars_per_doc"] = stats["total_chars"] / stats["processed_files"]

    # –°–∞–º—ã–π –±–æ–ª—å—à–æ–π –∏ –º–∞–ª–µ–Ω—å–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç—ã
    docs_by_tokens = sorted(docs_data, key=lambda x: x.metadata["tokens_count"])
    stats["smallest_doc"] = {
        "path": docs_by_tokens[0].relative_path,
        "tokens": docs_by_tokens[0].metadata["tokens_count"],
        "chars": docs_by_tokens[0].metadata["text_length"],
    }
    stats["largest_doc"] = {
        "path": docs_by_tokens[-1].relative_path,
        "tokens": docs_by_tokens[-1].metadata["tokens_count"],
        "chars": docs_by_tokens[-1].metadata["text_length"],
    }

    return stats


def format_statistics_for_display(stats: Dict[str, Any]) -> str:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞

    Args:
        stats: –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –∏–∑ calculate_statistics_from_processed_docs

    Returns:
        –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
    """
    if stats["processed_files"] == 0:
        return "‚ùå –ù–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"

    lines = [
        "üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ—Ä–ø—É—Å–∞:",
        f"üìÅ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {stats['processed_files']}",
        f"üî§ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {stats['total_tokens']:,}",
        f"üìÑ –°—Ä–µ–¥–Ω–µ–µ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç: {stats['avg_tokens_per_doc']:.1f}",
        f"üìù –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤: {stats['total_chars']:,}",
        f"üìñ –°—Ä–µ–¥–Ω–µ–µ —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç: {stats['avg_chars_per_doc']:.1f}",
        f"üìã –§–æ—Ä–º–∞—Ç—ã —Ñ–∞–π–ª–æ–≤: {stats['extensions_count']}",
    ]

    if stats["largest_doc"]:
        lines.extend(
            [
                "üìà –°–∞–º—ã–π –±–æ–ª—å—à–æ–π –¥–æ–∫—É–º–µ–Ω—Ç:",
                f"   üìÑ {stats['largest_doc']['path']}",
                f"   üî§ {stats['largest_doc']['tokens']} —Ç–æ–∫–µ–Ω–æ–≤, {stats['largest_doc']['chars']} —Å–∏–º–≤–æ–ª–æ–≤",
            ]
        )

    if stats["smallest_doc"]:
        lines.extend(
            [
                "üìâ –°–∞–º—ã–π –º–∞–ª–µ–Ω—å–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç:",
                f"   üìÑ {stats['smallest_doc']['path']}",
                f"   üî§ {stats['smallest_doc']['tokens']} —Ç–æ–∫–µ–Ω–æ–≤, {stats['smallest_doc']['chars']} —Å–∏–º–≤–æ–ª–æ–≤",
            ]
        )

    return "\n".join(lines)


def calculate_model_statistics(model_info: Dict[str, Any]) -> str:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –º–æ–¥–µ–ª–∏ –¥–ª—è –≤—ã–≤–æ–¥–∞

    Args:
        model_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏ –∏–∑ Doc2VecTrainer.get_model_info()

    Returns:
        –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –º–æ–¥–µ–ª–∏
    """
    if model_info.get("status") != "loaded":
        return f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {model_info.get('status', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}"

    lines = [
        "üß† –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–æ–¥–µ–ª–∏:",
        f"‚úÖ –°—Ç–∞—Ç—É—Å: {model_info['status']}",
        f"üìè –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤: {model_info['vector_size']}",
        f"üìö –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {model_info['vocabulary_size']:,} —Å–ª–æ–≤",
        f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –º–æ–¥–µ–ª–∏: {model_info['documents_count']}",
        f"üîç –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {model_info['window']}",
        f"üìä –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤–∞: {model_info['min_count']}",
        f"üîÑ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è: {model_info['epochs']}",
    ]

    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è
    if "training_time_formatted" in model_info:
        lines.append(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {model_info['training_time_formatted']}")

    if "training_date" in model_info:
        lines.append(f"üìÖ –î–∞—Ç–∞ –æ–±—É—á–µ–Ω–∏—è: {model_info['training_date']}")

    if "corpus_size" in model_info and model_info["corpus_size"] > 0:
        lines.append(
            f"üìë –†–∞–∑–º–µ—Ä –∫–æ—Ä–ø—É—Å–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {model_info['corpus_size']} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
        )

    # –†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
    if model_info.get("dm") == 1:
        lines.append("üîß –†–µ–∂–∏–º: Distributed Memory (DM)")
    else:
        lines.append("üîß –†–µ–∂–∏–º: Distributed Bag of Words (DBOW)")

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    if model_info.get("negative", 0) > 0:
        lines.append(f"‚ûñ Negative sampling: {model_info['negative']}")
    if model_info.get("hs") == 1:
        lines.append("üå≥ Hierarchical Softmax: –≤–∫–ª—é—á–µ–Ω")

    return "\n".join(lines)


========================================
FILE: src\semantic_search\utils\task_manager.py
========================================
"""–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∑–∞–¥–∞—á"""

import time
import uuid
from concurrent.futures import Future, ThreadPoolExecutor
from dataclasses import dataclass, field
from enum import Enum
from threading import Lock
from typing import Any, Callable, Dict, List, Optional

from loguru import logger

from .notification_system import ProgressTracker, notification_manager


class TaskStatus(Enum):
    """–°—Ç–∞—Ç—É—Å—ã –∑–∞–¥–∞—á"""

    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


@dataclass
class Task:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∑–∞–¥–∞—á–∏"""

    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    name: str = ""
    description: str = ""
    status: TaskStatus = TaskStatus.PENDING
    progress: float = 0.0
    result: Any = None
    error: Optional[str] = None
    start_time: Optional[float] = None
    end_time: Optional[float] = None
    future: Optional[Future] = None
    progress_tracker: Optional[ProgressTracker] = None

    @property
    def duration(self) -> Optional[float]:
        """–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏"""
        if self.start_time and self.end_time:
            return self.end_time - self.start_time
        elif self.start_time:
            return time.time() - self.start_time
        return None


class TaskManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –∑–∞–¥–∞—á –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π"""

    def __init__(self, max_workers: int = 4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.tasks: Dict[str, Task] = {}
        self.lock = Lock()

        logger.info(f"TaskManager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å {max_workers} –ø–æ—Ç–æ–∫–∞–º–∏")

    def submit_task(
        self,
        func: Callable,
        args: tuple = (),
        kwargs: dict = None,
        name: str = "",
        description: str = "",
        track_progress: bool = False,
        total_steps: int = 0,
    ) -> str:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ –∑–∞–¥–∞—á–∏

        Args:
            func: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            args: –ê—Ä–≥—É–º–µ–Ω—Ç—ã —Ñ—É–Ω–∫—Ü–∏–∏
            kwargs: –ò–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã
            name: –ù–∞–∑–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
            description: –û–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
            track_progress: –í–∫–ª—é—á–∏—Ç—å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            total_steps: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ (–¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞)

        Returns:
            ID –∑–∞–¥–∞—á–∏
        """
        kwargs = kwargs or {}

        task = Task(name=name or func.__name__, description=description)

        if track_progress and total_steps > 0:
            task.progress_tracker = ProgressTracker(total_steps, name)
            # –î–æ–±–∞–≤–ª—è–µ–º tracker –≤ kwargs –µ—Å–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è –µ–≥–æ –æ–∂–∏–¥–∞–µ—Ç
            if "progress_tracker" in func.__code__.co_varnames:
                kwargs["progress_tracker"] = task.progress_tracker

        # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞
        def wrapped_func():
            task.status = TaskStatus.RUNNING
            task.start_time = time.time()

            try:
                notification_manager.info(
                    "–ó–∞–¥–∞—á–∞ –∑–∞–ø—É—â–µ–Ω–∞", f"–ù–∞—á–∞—Ç–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ: {task.name}"
                )

                result = func(*args, **kwargs)

                task.status = TaskStatus.COMPLETED
                task.result = result
                task.progress = 1.0

                notification_manager.success(
                    "–ó–∞–¥–∞—á–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞",
                    f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {task.name}",
                    f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {task.duration:.1f}—Å",
                )

                return result

            except Exception as e:
                task.status = TaskStatus.FAILED
                task.error = str(e)

                notification_manager.error(
                    "–û—à–∏–±–∫–∞ –∑–∞–¥–∞—á–∏", f"–û—à–∏–±–∫–∞ –≤ –∑–∞–¥–∞—á–µ: {task.name}", str(e)
                )

                logger.error(f"–û—à–∏–±–∫–∞ –≤ –∑–∞–¥–∞—á–µ {task.id}: {e}")
                raise

            finally:
                task.end_time = time.time()

        # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á—É
        future = self.executor.submit(wrapped_func)
        task.future = future

        with self.lock:
            self.tasks[task.id] = task

        logger.info(f"–ó–∞–¥–∞—á–∞ {task.id} ({name}) –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –≤ –æ—á–µ—Ä–µ–¥—å")
        return task.id

    def get_task(self, task_id: str) -> Optional[Task]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∑–∞–¥–∞—á–µ"""
        with self.lock:
            return self.tasks.get(task_id)

    def get_all_tasks(self) -> List[Task]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –∑–∞–¥–∞—á"""
        with self.lock:
            return list(self.tasks.values())

    def get_running_tasks(self) -> List[Task]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω—è—é—â–∏—Ö—Å—è –∑–∞–¥–∞—á"""
        return [
            task for task in self.get_all_tasks() if task.status == TaskStatus.RUNNING
        ]

    def cancel_task(self, task_id: str) -> bool:
        """–û—Ç–º–µ–Ω–∞ –∑–∞–¥–∞—á–∏"""
        task = self.get_task(task_id)
        if not task or not task.future:
            return False

        if task.future.cancel():
            task.status = TaskStatus.CANCELLED
            logger.info(f"–ó–∞–¥–∞—á–∞ {task_id} –æ—Ç–º–µ–Ω–µ–Ω–∞")
            return True

        return False

    def wait_for_task(self, task_id: str, timeout: Optional[float] = None) -> Any:
        """–û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏"""
        task = self.get_task(task_id)
        if not task or not task.future:
            raise ValueError(f"–ó–∞–¥–∞—á–∞ {task_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

        return task.future.result(timeout=timeout)

    def cleanup_finished_tasks(self, max_keep: int = 100):
        """–û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á"""
        with self.lock:
            finished_tasks = [
                (task_id, task)
                for task_id, task in self.tasks.items()
                if task.status
                in [TaskStatus.COMPLETED, TaskStatus.FAILED, TaskStatus.CANCELLED]
            ]

            if len(finished_tasks) > max_keep:
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∏ —É–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ
                finished_tasks.sort(key=lambda x: x[1].end_time or 0)
                to_remove = finished_tasks[:-max_keep]

                for task_id, _ in to_remove:
                    del self.tasks[task_id]

                logger.info(f"–£–¥–∞–ª–µ–Ω–æ {len(to_remove)} –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á")

    def shutdown(self, wait: bool = True):
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –∑–∞–¥–∞—á"""
        logger.info("–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã TaskManager...")
        self.executor.shutdown(wait=wait)


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
task_manager = TaskManager()


========================================
FILE: src\semantic_search\utils\text_utils.py
========================================
# –í —Ñ–∞–π–ª–µ src/semantic_search/utils/text_utils.py
# –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π TextProcessor —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –¥–≤—É—Ö —è–∑—ã–∫–æ–≤

import re
from typing import List, Optional, Tuple

from loguru import logger

from semantic_search.config import SPACY_MODELS, TEXT_PROCESSING_CONFIG

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –ª–µ–Ω–∏–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
_nlp_ru = None
_nlp_en = None
_spacy_available = None
_initialization_attempted = False


def check_spacy_model_availability() -> dict:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ SpaCy –º–æ–¥–µ–ª–µ–π

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π
    """
    info = {
        "spacy_installed": False,
        "ru_model_found": False,
        "en_model_found": False,
        "ru_model_loadable": False,
        "en_model_loadable": False,
        "models_info": {},
        "error": None,
    }

    try:
        import spacy

        info["spacy_installed"] = True

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä—É—Å—Å–∫–æ–π –º–æ–¥–µ–ª–∏
        ru_model = SPACY_MODELS.get("ru", "ru_core_news_sm")
        try:
            nlp_ru = spacy.load(ru_model)
            info["ru_model_found"] = True
            info["ru_model_loadable"] = True
            info["models_info"]["ru"] = ru_model
        except OSError:
            info["error"] = f"–†—É—Å—Å–∫–∞—è –º–æ–¥–µ–ª—å '{ru_model}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–π –º–æ–¥–µ–ª–∏
        en_model = SPACY_MODELS.get("en", "en_core_web_sm")
        try:
            nlp_en = spacy.load(en_model)
            info["en_model_found"] = True
            info["en_model_loadable"] = True
            info["models_info"]["en"] = en_model
        except OSError:
            if not info["error"]:
                info["error"] = f"–ê–Ω–≥–ª–∏–π—Å–∫–∞—è –º–æ–¥–µ–ª—å '{en_model}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
            else:
                info["error"] += f", –∞–Ω–≥–ª–∏–π—Å–∫–∞—è –º–æ–¥–µ–ª—å '{en_model}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"

    except ImportError as e:
        info["error"] = f"SpaCy –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {e}"

    return info


def _initialize_spacy() -> Tuple[Optional[object], Optional[object], bool]:
    """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SpaCy –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ"""
    global _nlp_ru, _nlp_en, _spacy_available, _initialization_attempted

    if _initialization_attempted:
        return _nlp_ru, _nlp_en, _spacy_available

    _initialization_attempted = True

    try:
        import spacy

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä—É—Å—Å–∫—É—é –º–æ–¥–µ–ª—å
        ru_model = SPACY_MODELS.get("ru", "ru_core_news_sm")
        try:
            _nlp_ru = spacy.load(ru_model)
            _nlp_ru.max_length = TEXT_PROCESSING_CONFIG.get(
                "spacy_max_length", 3_000_000
            )
            logger.info(f"SpaCy —Ä—É—Å—Å–∫–∞—è –º–æ–¥–µ–ª—å '{ru_model}' –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except OSError:
            logger.warning(f"SpaCy —Ä—É—Å—Å–∫–∞—è –º–æ–¥–µ–ª—å '{ru_model}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            _nlp_ru = None

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞–Ω–≥–ª–∏–π—Å–∫—É—é –º–æ–¥–µ–ª—å
        en_model = SPACY_MODELS.get("en", "en_core_web_sm")
        try:
            _nlp_en = spacy.load(en_model)
            _nlp_en.max_length = TEXT_PROCESSING_CONFIG.get(
                "spacy_max_length", 3_000_000
            )
            logger.info(f"SpaCy –∞–Ω–≥–ª–∏–π—Å–∫–∞—è –º–æ–¥–µ–ª—å '{en_model}' –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except OSError:
            logger.warning(f"SpaCy –∞–Ω–≥–ª–∏–π—Å–∫–∞—è –º–æ–¥–µ–ª—å '{en_model}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            _nlp_en = None

        _spacy_available = (_nlp_ru is not None) or (_nlp_en is not None)

    except ImportError:
        logger.warning("SpaCy –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        _nlp_ru = None
        _nlp_en = None
        _spacy_available = False

    return _nlp_ru, _nlp_en, _spacy_available


class TextProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä—É—Å—Å–∫–æ–≥–æ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤"""

    def __init__(self):
        self.config = TEXT_PROCESSING_CONFIG
        self._nlp_ru = None
        self._nlp_en = None
        self._spacy_available = None
        self.max_chunk_size = self.config.get("chunk_size", 800_000)

    def _get_nlp(self):
        """–ü–æ–ª—É—á–∏—Ç—å SpaCy –º–æ–¥–µ–ª–∏ (—Å –ª–µ–Ω–∏–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π)"""
        if self._nlp_ru is None and self._nlp_en is None:
            self._nlp_ru, self._nlp_en, self._spacy_available = _initialize_spacy()
        return self._nlp_ru, self._nlp_en, self._spacy_available

    def detect_language(self, text: str) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–æ–±–ª–∞–¥–∞—é—â–µ–≥–æ —è–∑—ã–∫–∞ —Ç–µ–∫—Å—Ç–∞

        Returns:
            'ru' - —Ä—É—Å—Å–∫–∏–π, 'en' - –∞–Ω–≥–ª–∏–π—Å–∫–∏–π, 'mixed' - —Å–º–µ—à–∞–Ω–Ω—ã–π
        """
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤
        sample = text[:1000]

        # –ü–æ–¥—Å—á–µ—Ç –∞–ª—Ñ–∞–≤–∏—Ç–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        cyrillic = sum(1 for c in sample if "\u0400" <= c <= "\u04ff")
        latin = sum(1 for c in sample if ("a" <= c <= "z") or ("A" <= c <= "Z"))

        total = cyrillic + latin
        if total == 0:
            return "unknown"

        cyrillic_ratio = cyrillic / total

        if cyrillic_ratio > 0.8:
            return "ru"
        elif cyrillic_ratio < 0.2:
            return "en"
        else:
            return "mixed"

    def clean_text(self, text: str) -> str:
        """
        –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞

        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç

        Returns:
            –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        if not text:
            return ""

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–æ–ª—å—à–µ —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        text = re.sub(r'[^\w\s\-.,!?;:()\[\]""¬´¬ª\']+', " ", text, flags=re.UNICODE)
        text = re.sub(r"\s+", " ", text)

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
        words = text.split()
        words = [word for word in words if len(word) > 1 or word.lower() in ["i", "a"]]

        return " ".join(words).strip()

    def preprocess_with_spacy(self, text: str, language: str = "auto") -> List[str]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º SpaCy

        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            language: 'ru', 'en' –∏–ª–∏ 'auto' –¥–ª—è –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è

        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        """
        nlp_ru, nlp_en, spacy_available = self._get_nlp()

        if not spacy_available:
            return self.preprocess_basic(text)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
        if language == "auto":
            language = self.detect_language(text)
            logger.debug(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω —è–∑—ã–∫: {language}")

        # –í—ã–±–∏—Ä–∞–µ–º –ø–æ–¥—Ö–æ–¥—è—â—É—é –º–æ–¥–µ–ª—å
        if language == "ru" and nlp_ru:
            nlp = nlp_ru
        elif language == "en" and nlp_en:
            nlp = nlp_en
        elif language == "mixed":
            # –î–ª—è —Å–º–µ—à–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ —á–∞—Å—Ç—è–º
            return self._process_mixed_text(text)
        else:
            # Fallback –Ω–∞ –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å
            nlp = nlp_ru or nlp_en
            if not nlp:
                return self.preprocess_basic(text)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ SpaCy
        tokens = []

        # –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ - –ø–æ —á–∞—Å—Ç—è–º
        if len(text) > self.max_chunk_size:
            for i in range(0, len(text), self.max_chunk_size):
                chunk = text[i : i + self.max_chunk_size]
                chunk_tokens = self._process_spacy_chunk(chunk, nlp)
                tokens.extend(chunk_tokens)
        else:
            tokens = self._process_spacy_chunk(text, nlp)

        return tokens

    def _process_mixed_text(self, text: str) -> List[str]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å–æ —Å–º–µ—à–∞–Ω–Ω—ã–º–∏ —è–∑—ã–∫–∞–º–∏"""
        nlp_ru, nlp_en, _ = self._get_nlp()

        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = self.split_into_sentences(text)
        all_tokens = []

        for sentence in sentences:
            lang = self.detect_language(sentence)

            if lang == "ru" and nlp_ru:
                tokens = self._process_spacy_chunk(sentence, nlp_ru)
            elif lang == "en" and nlp_en:
                tokens = self._process_spacy_chunk(sentence, nlp_en)
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–µ–π –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
                tokens = self.preprocess_basic(sentence)

            all_tokens.extend(tokens)

        return all_tokens

    def _process_spacy_chunk(self, text: str, nlp) -> List[str]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ SpaCy"""
        doc = nlp(text)
        tokens = []

        for token in doc:
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã
            if (
                not token.is_punct
                and not token.is_space
                and not token.is_stop
                and len(token.text) >= self.config["min_token_length"]
                and (token.is_alpha or token.like_num)  # –ë—É–∫–≤—ã –∏–ª–∏ —á–∏—Å–ª–∞
            ):
                # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞
                if self.config["lemmatize"]:
                    tokens.append(token.lemma_.lower())
                else:
                    tokens.append(token.text.lower())

        return tokens

    def preprocess_basic(self, text: str) -> List[str]:
        """
        –ë–∞–∑–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ SpaCy
        """
        text = text.lower()

        # –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        tokens = re.findall(r"\b\w+\b", text, re.UNICODE)

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        tokens = [
            token for token in tokens if len(token) >= self.config["min_token_length"]
        ]

        return tokens

    def preprocess_text(self, text: str) -> List[str]:
        """
        –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–∞
        """
        if not text:
            return []

        cleaned_text = self.clean_text(text)
        if not cleaned_text:
            return []

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
        tokens = self.preprocess_with_spacy(cleaned_text)

        return tokens

    def split_into_sentences(self, text: str) -> List[str]:
        """
        –†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç–∏
        """
        if not text:
            return []

        nlp_ru, nlp_en, spacy_available = self._get_nlp()
        min_sentence_length = self.config.get("min_sentence_length", 10)

        if spacy_available:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫
            lang = self.detect_language(text)

            # –í—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å
            if lang == "ru" and nlp_ru:
                nlp = nlp_ru
            elif lang == "en" and nlp_en:
                nlp = nlp_en
            else:
                # –î–ª—è —Å–º–µ—à–∞–Ω–Ω–æ–≥–æ –∏–ª–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ - –±–∞–∑–æ–≤—ã–π –º–µ—Ç–æ–¥
                return self._split_sentences_basic(text, min_sentence_length)

            try:
                doc = nlp(text)
                sentences = [sent.text.strip() for sent in doc.sents]
                sentences = [
                    sent for sent in sentences if len(sent) >= min_sentence_length
                ]
                return sentences
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–±–∏–µ–Ω–∏–∏ —á–µ—Ä–µ–∑ SpaCy: {e}")
                return self._split_sentences_basic(text, min_sentence_length)
        else:
            return self._split_sentences_basic(text, min_sentence_length)

    def _split_sentences_basic(self, text: str, min_sentence_length: int) -> List[str]:
        """
        –ë–∞–∑–æ–≤–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –±–µ–∑ SpaCy

        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            min_sentence_length: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è

        Returns:
            –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        """
        # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –∑–Ω–∞–∫–∞–º –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä—É—Å—Å–∫–∏—Ö –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π
        abbreviations = {
            "–≥.",
            "–≥–≥.",
            "—Ç.–¥.",
            "—Ç.–ø.",
            "–¥—Ä.",
            "–ø—Ä.",
            "—Å–º.",
            "—Å—Ç—Ä.",
            "Mr.",
            "Mrs.",
            "Dr.",
            "Prof.",
            "Inc.",
            "Ltd.",
            "Co.",
            "vs.",
            "etc.",
            "i.e.",
            "e.g.",
        }

        # –ó–∞–º–µ–Ω–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–∞—Ä–∫–µ—Ä–∞–º–∏
        temp_text = text
        replacements = {}
        for i, abbr in enumerate(abbreviations):
            marker = f"__ABBR{i}__"
            replacements[marker] = abbr
            temp_text = temp_text.replace(abbr, marker)

        # –†–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –æ—Å–Ω–æ–≤–Ω—ã–º –∑–Ω–∞–∫–∞–º –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        import re

        sentences = re.split(r"[.!?]+", temp_text)

        # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π
        result_sentences = []
        for sent in sentences:
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è
            for marker, abbr in replacements.items():
                sent = sent.replace(marker, abbr)

            sent = sent.strip()
            if len(sent) >= min_sentence_length:
                result_sentences.append(sent)

        return result_sentences

    def get_spacy_status(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å SpaCy –º–æ–¥–µ–ª–µ–π"""
        nlp_ru, nlp_en, _ = self._get_nlp()

        status_parts = []

        if nlp_ru:
            status_parts.append("‚úÖ –†—É—Å—Å–∫–∞—è –º–æ–¥–µ–ª—å")
        else:
            status_parts.append("‚ùå –†—É—Å—Å–∫–∞—è –º–æ–¥–µ–ª—å –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")

        if nlp_en:
            status_parts.append("‚úÖ –ê–Ω–≥–ª–∏–π—Å–∫–∞—è –º–æ–¥–µ–ª—å")
        else:
            status_parts.append("‚ùå –ê–Ω–≥–ª–∏–π—Å–∫–∞—è –º–æ–¥–µ–ª—å –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")

        return " | ".join(status_parts)


========================================
FILE: src\semantic_search\utils\validators.py
========================================
"""–í–∞–ª–∏–¥–∞—Ç–æ—Ä—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""

from pathlib import Path
from typing import Any, Dict, Optional, Union


class ValidationError(Exception):
    """–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""

    pass


class DataValidator:
    """–í–∞–ª–∏–¥–∞—Ç–æ—Ä –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""

    @staticmethod
    def validate_file_path(path: Union[str, Path], must_exist: bool = True) -> Path:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É

        Args:
            path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            must_exist: –§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤–æ–≤–∞—Ç—å

        Returns:
            –í–∞–ª–∏–¥–Ω—ã–π Path –æ–±—ä–µ–∫—Ç

        Raises:
            ValidationError: –ü—Ä–∏ –Ω–µ–≤–∞–ª–∏–¥–Ω–æ–º –ø—É–≥–µ
        """
        if isinstance(path, str):
            path = Path(path)

        if not isinstance(path, Path):
            raise ValidationError(
                f"–ü—É—Ç—å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π –∏–ª–∏ Path –æ–±—ä–µ–∫—Ç–æ–º: {type(path)}"
            )

        if must_exist and not path.exists():
            raise ValidationError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}")

        if must_exist and not path.is_file():
            raise ValidationError(f"–ü—É—Ç—å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Ñ–∞–π–ª–æ–º: {path}")

        return path

    @staticmethod
    def validate_directory_path(
        path: Union[str, Path], must_exist: bool = True
    ) -> Path:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –ø—É—Ç–∏ –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
        if isinstance(path, str):
            path = Path(path)

        if not isinstance(path, Path):
            raise ValidationError(
                f"–ü—É—Ç—å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π –∏–ª–∏ Path –æ–±—ä–µ–∫—Ç–æ–º: {type(path)}"
            )

        if must_exist and not path.exists():
            raise ValidationError(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {path}")

        if must_exist and not path.is_dir():
            raise ValidationError(f"–ü—É—Ç—å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–µ–π: {path}")

        return path

    @staticmethod
    def validate_text(
        text: str, min_length: int = 1, max_length: Optional[int] = None
    ) -> str:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        if not isinstance(text, str):
            raise ValidationError(f"–¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π: {type(text)}")

        text = text.strip()

        if len(text) < min_length:
            raise ValidationError(
                f"–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π. –ú–∏–Ω–∏–º—É–º: {min_length}, –ø–æ–ª—É—á–µ–Ω–æ: {len(text)}"
            )

        if max_length and len(text) > max_length:
            raise ValidationError(
                f"–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π. –ú–∞–∫—Å–∏–º—É–º: {max_length}, –ø–æ–ª—É—á–µ–Ω–æ: {len(text)}"
            )

        return text

    @staticmethod
    def validate_search_params(
        query: str,
        top_k: Optional[int] = None,
        similarity_threshold: Optional[float] = None,
    ) -> Dict[str, Any]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–∏—Å–∫–∞"""

        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞
        query = DataValidator.validate_text(query, min_length=2, max_length=1000)

        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if top_k is not None:
            if not isinstance(top_k, int) or top_k < 1 or top_k > 1000:
                raise ValidationError(
                    f"top_k –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º –æ—Ç 1 –¥–æ 1000: {top_k}"
                )

        # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏
        if similarity_threshold is not None:
            if not isinstance(similarity_threshold, (int, float)) or not (
                0 <= similarity_threshold <= 1
            ):
                raise ValidationError(
                    f"similarity_threshold –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–∏—Å–ª–æ–º –æ—Ç 0 –¥–æ 1: {similarity_threshold}"
                )

        return {
            "query": query,
            "top_k": top_k,
            "similarity_threshold": similarity_threshold,
        }

    @staticmethod
    def validate_model_params(**params) -> Dict[str, Any]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ Doc2Vec"""
        validated = {}

        # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤
        if "vector_size" in params:
            vs = params["vector_size"]
            if not isinstance(vs, int) or not (50 <= vs <= 1000):
                raise ValidationError(f"vector_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç 50 –¥–æ 1000: {vs}")
            validated["vector_size"] = vs

        # –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞
        if "window" in params:
            w = params["window"]
            if not isinstance(w, int) or not (1 <= w <= 50):
                raise ValidationError(f"window –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç 1 –¥–æ 50: {w}")
            validated["window"] = w

        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞
        if "min_count" in params:
            mc = params["min_count"]
            if not isinstance(mc, int) or not (1 <= mc <= 100):
                raise ValidationError(f"min_count –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç 1 –¥–æ 100: {mc}")
            validated["min_count"] = mc

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
        if "epochs" in params:
            e = params["epochs"]
            if not isinstance(e, int) or not (1 <= e <= 1000):
                raise ValidationError(f"epochs –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç 1 –¥–æ 1000: {e}")
            validated["epochs"] = e

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤
        if "workers" in params:
            w = params["workers"]
            if not isinstance(w, int) or not (1 <= w <= 32):
                raise ValidationError(f"workers –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç 1 –¥–æ 32: {w}")
            validated["workers"] = w

        return validated


class FileValidator:
    """–í–∞–ª–∏–¥–∞—Ç–æ—Ä –¥–ª—è —Ñ–∞–π–ª–æ–≤"""

    SUPPORTED_EXTENSIONS = {".pdf", ".docx", ".doc", ".txt"}
    MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB

    @classmethod
    def validate_document_file(cls, file_path: Path) -> Dict[str, Any]:
        """
        –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        result = {"valid": True, "errors": [], "warnings": [], "file_info": {}}

        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è
            if not file_path.exists():
                result["errors"].append(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
                result["valid"] = False
                return result

            # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ —ç—Ç–æ —Ñ–∞–π–ª
            if not file_path.is_file():
                result["errors"].append(f"–ü—É—Ç—å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Ñ–∞–π–ª–æ–º: {file_path}")
                result["valid"] = False
                return result

            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ
            stat = file_path.stat()
            result["file_info"] = {
                "size": stat.st_size,
                "size_mb": stat.st_size / 1024 / 1024,
                "extension": file_path.suffix.lower(),
                "name": file_path.name,
            }

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
            if file_path.suffix.lower() not in cls.SUPPORTED_EXTENSIONS:
                result["errors"].append(
                    f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ: {file_path.suffix}"
                )
                result["valid"] = False

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞
            if stat.st_size > cls.MAX_FILE_SIZE:
                result["errors"].append(
                    f"–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π: {stat.st_size / 1024 / 1024:.1f}MB"
                )
                result["valid"] = False
            elif stat.st_size > cls.MAX_FILE_SIZE * 0.8:
                result["warnings"].append(
                    f"–ë–æ–ª—å—à–æ–π —Ñ–∞–π–ª: {stat.st_size / 1024 / 1024:.1f}MB"
                )

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—É—Å—Ç–æ–≥–æ —Ñ–∞–π–ª–∞
            if stat.st_size == 0:
                result["errors"].append("–§–∞–π–ª –ø—É—Å—Ç–æ–π")
                result["valid"] = False
            elif stat.st_size < 100:  # –ú–µ–Ω—å—à–µ 100 –±–∞–π—Ç
                result["warnings"].append("–û—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–π —Ñ–∞–π–ª")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –¥–ª—è —á—Ç–µ–Ω–∏—è
            try:
                with open(file_path, "rb") as f:
                    f.read(1)
            except PermissionError:
                result["errors"].append("–ù–µ—Ç –ø—Ä–∞–≤ –Ω–∞ —á—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞")
                result["valid"] = False
            except Exception as e:
                result["errors"].append(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
                result["valid"] = False

        except Exception as e:
            result["errors"].append(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
            result["valid"] = False

        return result


========================================
FILE: src\semantic_search\evaluation\__init__.py
========================================
"""–ú–æ–¥—É–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞"""

from .baselines import BaseSearchMethod, OpenAISearchBaseline
from .comparison import SearchComparison
from .metrics import SearchMetrics

__all__ = [
    "BaseSearchMethod",
    "OpenAISearchBaseline",
    "SearchComparison",
    "SearchMetrics",
]


========================================
FILE: src\semantic_search\evaluation\baselines.py
========================================
"""–ë–∞–∑–æ–≤—ã–µ –∫–ª–∞—Å—Å—ã –∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ–∏—Å–∫–∞"""

import json
import os
import time
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
from loguru import logger

from semantic_search.core.search_engine import SearchResult
from semantic_search.utils.text_utils import TextProcessor


class BaseSearchMethod(ABC):
    """–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞"""

    def __init__(self, name: str):
        self.name = name
        self.index_time = 0.0
        self.indexed_documents = []

    @abstractmethod
    def index(self, documents: List[Tuple[str, str, str]]) -> None:
        """
        –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        Args:
            documents: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (doc_id, text, metadata)
        """
        pass

    @abstractmethod
    def search(self, query: str, top_k: int = 10) -> List[SearchResult]:
        """
        –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        """
        pass

    def get_method_name(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–∞"""
        return self.name

    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –º–µ—Ç–æ–¥–∞"""
        return {
            "method_name": self.name,
            "indexed_documents": len(self.indexed_documents),
            "index_time": self.index_time,
        }


class Doc2VecSearchAdapter(BaseSearchMethod):
    """–ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ Doc2Vec –ø–æ–∏—Å–∫–∞"""

    def __init__(self, search_engine, corpus_info):
        super().__init__("Doc2Vec")
        self.search_engine = search_engine
        self.corpus_info = corpus_info

    def index(self, documents: List[Tuple[str, str, str]]) -> None:
        """Doc2Vec —É–∂–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏"""
        self.indexed_documents = documents

    def search(self, query: str, top_k: int = 10) -> List[SearchResult]:
        """–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –ø–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫"""
        return self.search_engine.search(query, top_k=top_k)


class OpenAISearchBaseline(BaseSearchMethod):
    """–ü–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º OpenAI embeddings"""

    def __init__(
        self, api_key: Optional[str] = None, model: str = "text-embedding-ada-002"
    ):
        super().__init__(f"OpenAI ({model})")
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.model = model
        self.embeddings = {}
        self.documents = {}
        self.text_processor = TextProcessor()

        if not self.api_key:
            raise ValueError(
                "OpenAI API key –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è OPENAI_API_KEY"
            )

        # Lazy import –¥–ª—è –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
        try:
            import openai

            self.openai = openai
            self.client = openai.OpenAI(api_key=self.api_key)
        except ImportError:
            raise ImportError("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ openai: pip install openai")

    def _get_embedding(self, text: str) -> List[float]:
        """–ü–æ–ª—É—á–∏—Ç—å embedding –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        try:
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
            if len(text) > 8000:
                text = text[:8000]

            response = self.client.embeddings.create(model=self.model, input=text)
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ embedding: {e}")
            raise

    def index(self, documents: List[Tuple[str, str, str]]) -> None:
        """
        –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ OpenAI API

        Args:
            documents: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (doc_id, text, metadata)
        """
        start_time = time.time()
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ OpenAI")

        for i, (doc_id, text, metadata) in enumerate(documents):
            try:
                # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è embedding
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 2000 —Å–∏–º–≤–æ–ª–æ–≤ + –ø–æ—Å–ª–µ–¥–Ω–∏–µ 1000
                if len(text) > 3000:
                    text_sample = text[:2000] + " ... " + text[-1000:]
                else:
                    text_sample = text

                # –ü–æ–ª—É—á–∞–µ–º embedding
                embedding = self._get_embedding(text_sample)

                self.embeddings[doc_id] = np.array(embedding)
                self.documents[doc_id] = {"text": text, "metadata": metadata}

                if (i + 1) % 10 == 0:
                    logger.info(f"–ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ {i + 1}/{len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

                # –ó–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è rate limits
                time.sleep(0.1)

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_id}: {e}")
                continue

        self.indexed_documents = documents
        self.index_time = time.time() - start_time
        logger.info(f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {self.index_time:.2f} —Å–µ–∫—É–Ω–¥")

    def search(self, query: str, top_k: int = 10) -> List[SearchResult]:
        """
        –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        """
        if not self.embeddings:
            logger.error("–ò–Ω–¥–µ–∫—Å –ø—É—Å—Ç. –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä—É–π—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã")
            return []

        try:
            # –ü–æ–ª—É—á–∞–µ–º embedding –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = np.array(self._get_embedding(query))

            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            similarities = []
            for doc_id, doc_embedding in self.embeddings.items():
                # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                similarity = np.dot(query_embedding, doc_embedding) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)
                )
                similarities.append((doc_id, similarity))

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
            similarities.sort(key=lambda x: x[1], reverse=True)

            # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            results = []
            for doc_id, similarity in similarities[:top_k]:
                metadata = self.documents[doc_id].get("metadata", {})
                results.append(SearchResult(doc_id, float(similarity), metadata))

            return results

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
            return []

    def save_index(self, path: Path) -> None:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–Ω–¥–µ–∫—Å –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
        data = {
            "embeddings": {k: v.tolist() for k, v in self.embeddings.items()},
            "documents": self.documents,
            "model": self.model,
        }

        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f)

        logger.info(f"–ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {path}")

    def load_index(self, path: Path) -> None:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–Ω–¥–µ–∫—Å"""
        if not path.exists():
            raise FileNotFoundError(f"–§–∞–π–ª –∏–Ω–¥–µ–∫—Å–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}")

        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)

        self.embeddings = {k: np.array(v) for k, v in data["embeddings"].items()}
        self.documents = data["documents"]
        self.indexed_documents = list(self.documents.keys())

        logger.info(f"–ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {path}")


========================================
FILE: src\semantic_search\evaluation\comparison.py
========================================
"""–ú–æ–¥—É–ª—å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞"""

import json
import time
from pathlib import Path
from typing import Dict, List, Optional, Set

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from loguru import logger

from semantic_search.config import DATA_DIR

from .baselines import BaseSearchMethod
from .metrics import SearchMetrics


class QueryTestCase:
    """–¢–µ—Å—Ç–æ–≤—ã–π —Å–ª—É—á–∞–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–∏—Å–∫–∞"""

    def __init__(
        self,
        query: str,
        relevant_docs: Set[str],
        relevance_scores: Optional[Dict[str, float]] = None,
        description: str = "",
    ):
        self.query = query
        self.relevant_docs = relevant_docs
        self.relevance_scores = relevance_scores or {}
        self.description = description


class SearchComparison:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞"""

    def __init__(self, test_cases: Optional[List[QueryTestCase]] = None):
        self.test_cases = test_cases or []
        self.results = {}
        self.metrics = SearchMetrics()

    def add_test_case(self, test_case: QueryTestCase) -> None:
        """–î–æ–±–∞–≤–∏—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π —Å–ª—É—á–∞–π"""
        self.test_cases.append(test_case)

    def create_default_test_cases(self) -> List[QueryTestCase]:
        """–°–æ–∑–¥–∞—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
        test_cases = [
            # QueryTestCase(
            #     query="–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏",
            #     relevant_docs={
            #         "ml_basics.pdf",
            #         "neural_networks.pdf",
            #         "deep_learning.pdf",
            #     },
            #     relevance_scores={
            #         "ml_basics.pdf": 3,
            #         "neural_networks.pdf": 3,
            #         "deep_learning.pdf": 2,
            #         "ai_overview.pdf": 1,
            #     },
            #     description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ ML",
            # ),
            # QueryTestCase(
            #     query="–≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
            #     relevant_docs={
            #         "cnn_tutorial.pdf",
            #         "image_processing.pdf",
            #         "deep_learning.pdf",
            #     },
            #     relevance_scores={
            #         "cnn_tutorial.pdf": 3,
            #         "image_processing.pdf": 3,
            #         "deep_learning.pdf": 2,
            #         "computer_vision.pdf": 2,
            #     },
            #     description="–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ CV",
            # ),
            QueryTestCase(
                query="–º–µ—Ç–æ–¥—ã –≥–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏",
                relevant_docs={
                    "nlp_transformers.pdf",
                    "bert_paper.pdf",
                    "attention_mechanism.pdf",
                },
                relevance_scores={
                    "nlp_transformers.pdf": 3,
                    "bert_paper.pdf": 3,
                    "attention_mechanism.pdf": 2,
                    "nlp_basics.pdf": 1,
                },
                description="–ó–∞–ø—Ä–æ—Å –ø–æ NLP",
            ),
            # QueryTestCase(
            #     query="–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è",
            #     relevant_docs={"optimization_methods.pdf", "gradient_descent.pdf"},
            #     relevance_scores={
            #         "optimization_methods.pdf": 3,
            #         "gradient_descent.pdf": 3,
            #         "ml_basics.pdf": 1,
            #     },
            #     description="–ó–∞–ø—Ä–æ—Å –ø–æ –º–µ—Ç–æ–¥–∞–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏",
            # ),
            # QueryTestCase(
            #     query="—Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ LSTM",
            #     relevant_docs={
            #         "rnn_tutorial.pdf",
            #         "lstm_explained.pdf",
            #         "sequence_models.pdf",
            #     },
            #     relevance_scores={
            #         "rnn_tutorial.pdf": 3,
            #         "lstm_explained.pdf": 3,
            #         "sequence_models.pdf": 2,
            #     },
            #     description="–ó–∞–ø—Ä–æ—Å –ø–æ RNN",
            # ),
        ]

        return test_cases

    def evaluate_method(
        self, method: BaseSearchMethod, top_k: int = 10, verbose: bool = True
    ) -> Dict[str, any]:
        """
        –û—Ü–µ–Ω–∏—Ç—å –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ –Ω–∞ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞—è—Ö

        Args:
            method: –ú–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            verbose: –í—ã–≤–æ–¥–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ—Ü–µ–Ω–∫–∏
        """
        method_name = method.get_method_name()

        if verbose:
            logger.info(f"–û—Ü–µ–Ω–∫–∞ –º–µ—Ç–æ–¥–∞: {method_name}")

        all_metrics = []
        query_times = []
        all_results = []

        for i, test_case in enumerate(self.test_cases):
            if verbose and (i + 1) % 5 == 0:
                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {i + 1}/{len(self.test_cases)}")

            # –ò–∑–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞
            start_time = time.time()
            search_results = method.search(test_case.query, top_k=top_k)
            query_time = time.time() - start_time
            query_times.append(query_time)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            retrieved_docs = [result.doc_id for result in search_results]

            # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
            metrics = self.metrics.calculate_all_metrics(
                retrieved=retrieved_docs,
                relevant=test_case.relevant_docs,
                relevance_scores=test_case.relevance_scores,
                k_values=[1, 5, 10],
            )

            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–ø—Ä–æ—Å–µ
            metrics["query"] = test_case.query
            metrics["query_time"] = query_time

            all_metrics.append(metrics)
            all_results.append((retrieved_docs, test_case.relevant_docs))

        # –í—ã—á–∏—Å–ª—è–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        aggregated = self._aggregate_metrics(all_metrics)

        # MAP –∏ MRR
        aggregated["MAP"] = self.metrics.mean_average_precision(all_results)
        aggregated["MRR"] = self.metrics.mean_reciprocal_rank(all_results)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        aggregated["avg_query_time"] = np.mean(query_times)
        aggregated["std_query_time"] = np.std(query_times)
        aggregated["median_query_time"] = np.median(query_times)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.results[method_name] = {
            "aggregated": aggregated,
            "detailed": all_metrics,
            "method_stats": method.get_stats(),
        }

        if verbose:
            logger.info(f"–û—Ü–µ–Ω–∫–∞ {method_name} –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            logger.info(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∑–∞–ø—Ä–æ—Å–∞: {aggregated['avg_query_time']:.3f}—Å")
            logger.info(f"MAP: {aggregated['MAP']:.3f}")
            logger.info(f"MRR: {aggregated['MRR']:.3f}")

        return self.results[method_name]

    def _aggregate_metrics(self, metrics_list: List[Dict]) -> Dict[str, float]:
        """–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –≤—Å–µ–º –∑–∞–ø—Ä–æ—Å–∞–º"""
        aggregated = {}

        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∫–ª—é—á–∏ –º–µ—Ç—Ä–∏–∫ (–∏—Å–∫–ª—é—á–∞—è —Å–ª—É–∂–µ–±–Ω—ã–µ)
        metric_keys = [
            k for k in metrics_list[0].keys() if k not in ["query", "query_time"]
        ]

        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –¥–ª—è –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–∏
        for key in metric_keys:
            values = [m[key] for m in metrics_list]
            aggregated[f"avg_{key}"] = np.mean(values)
            aggregated[f"std_{key}"] = np.std(values)

        return aggregated

    def compare_methods(
        self,
        methods: List[BaseSearchMethod],
        top_k: int = 10,
        save_results: bool = True,
    ) -> pd.DataFrame:
        """
        –°—Ä–∞–≤–Ω–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Ç–æ–¥–æ–≤

        Args:
            methods: –°–ø–∏—Å–æ–∫ –º–µ—Ç–æ–¥–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            save_results: –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª

        Returns:
            DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        """
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ {len(methods)} –º–µ—Ç–æ–¥–æ–≤")

        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π –º–µ—Ç–æ–¥
        for method in methods:
            self.evaluate_method(method, top_k=top_k)

        # –°–æ–∑–¥–∞–µ–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
        comparison_data = []

        for method_name, results in self.results.items():
            row = {
                "Method": method_name,
                "MAP": results["aggregated"]["MAP"],
                "MRR": results["aggregated"]["MRR"],
                "Avg Query Time (s)": results["aggregated"]["avg_query_time"],
                "Index Time (s)": results["method_stats"]["index_time"],
            }

            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö k
            for k in [1, 5, 10]:
                row[f"P@{k}"] = results["aggregated"][f"avg_precision@{k}"]
                row[f"R@{k}"] = results["aggregated"][f"avg_recall@{k}"]
                if f"avg_ndcg@{k}" in results["aggregated"]:
                    row[f"NDCG@{k}"] = results["aggregated"][f"avg_ndcg@{k}"]

            comparison_data.append(row)

        df_comparison = pd.DataFrame(comparison_data)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if save_results:
            results_dir = DATA_DIR / "evaluation_results"
            results_dir.mkdir(exist_ok=True)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–±–ª–∏—Ü—É
            df_comparison.to_csv(results_dir / "comparison_results.csv", index=False)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            with open(
                results_dir / "detailed_results.json", "w", encoding="utf-8"
            ) as f:
                json.dump(self.results, f, indent=2)

            logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {results_dir}")

        return df_comparison

    def plot_comparison(self, save_plots: bool = True) -> None:
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        if not self.results:
            logger.error("–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
            return

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        methods = list(self.results.keys())

        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–≥—É—Ä—É —Å –ø–æ–¥–≥—Ä–∞—Ñ–∏–∫–∞–º–∏
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞", fontsize=16)

        # 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        ax1 = axes[0, 0]
        metrics_data = {
            "MAP": [self.results[m]["aggregated"]["MAP"] for m in methods],
            "MRR": [self.results[m]["aggregated"]["MRR"] for m in methods],
            "P@10": [
                self.results[m]["aggregated"]["avg_precision@10"] for m in methods
            ],
            "R@10": [self.results[m]["aggregated"]["avg_recall@10"] for m in methods],
        }

        x = np.arange(len(methods))
        width = 0.2

        for i, (metric, values) in enumerate(metrics_data.items()):
            ax1.bar(x + i * width, values, width, label=metric)

        ax1.set_xlabel("–ú–µ—Ç–æ–¥—ã")
        ax1.set_ylabel("–ó–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏")
        ax1.set_title("–û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞")
        ax1.set_xticks(x + width * 1.5)
        ax1.set_xticklabels(methods)
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # 2. Precision-Recall –¥–ª—è —Ä–∞–∑–Ω—ã—Ö k
        ax2 = axes[0, 1]
        k_values = [1, 5, 10]

        for method in methods:
            precisions = [
                self.results[method]["aggregated"][f"avg_precision@{k}"]
                for k in k_values
            ]
            recalls = [
                self.results[method]["aggregated"][f"avg_recall@{k}"] for k in k_values
            ]
            ax2.plot(recalls, precisions, marker="o", label=method)

        ax2.set_xlabel("Recall")
        ax2.set_ylabel("Precision")
        ax2.set_title("Precision-Recall –∫—Ä–∏–≤—ã–µ")
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # 3. –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        ax3 = axes[1, 0]
        query_times = [self.results[m]["aggregated"]["avg_query_time"] for m in methods]
        index_times = [self.results[m]["method_stats"]["index_time"] for m in methods]

        x = np.arange(len(methods))
        width = 0.35

        ax3.bar(x - width / 2, query_times, width, label="–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∑–∞–ø—Ä–æ—Å–∞")
        ax3.bar(x + width / 2, index_times, width, label="–í—Ä–µ–º—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")

        ax3.set_xlabel("–ú–µ—Ç–æ–¥—ã")
        ax3.set_ylabel("–í—Ä–µ–º—è (—Å–µ–∫—É–Ω–¥—ã)")
        ax3.set_title("–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–æ–≤")
        ax3.set_xticks(x)
        ax3.set_xticklabels(methods)
        ax3.legend()
        ax3.grid(True, alpha=0.3)

        # 4. NDCG –¥–ª—è —Ä–∞–∑–Ω—ã—Ö k
        ax4 = axes[1, 1]

        for method in methods:
            if "avg_ndcg@1" in self.results[method]["aggregated"]:
                ndcg_values = [
                    self.results[method]["aggregated"][f"avg_ndcg@{k}"]
                    for k in k_values
                ]
                ax4.plot(k_values, ndcg_values, marker="s", label=method)

        ax4.set_xlabel("k")
        ax4.set_ylabel("NDCG@k")
        ax4.set_title("NDCG –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö k")
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        ax4.set_xticks(k_values)

        plt.tight_layout()

        if save_plots:
            plots_dir = DATA_DIR / "evaluation_results" / "plots"
            plots_dir.mkdir(exist_ok=True, parents=True)
            plt.savefig(
                plots_dir / "comparison_plots.png", dpi=300, bbox_inches="tight"
            )
            logger.info(f"–ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {plots_dir}")

        plt.show()

    def plot_detailed_metrics(self, method_name: str, save_plot: bool = True) -> None:
        """–î–µ—Ç–∞–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞"""
        if method_name not in self.results:
            logger.error(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –º–µ—Ç–æ–¥–∞ {method_name} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return

        detailed = self.results[method_name]["detailed"]

        # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
        df = pd.DataFrame(detailed)

        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f"–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–¥–∞: {method_name}", fontsize=16)

        # 1. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Average Precision
        ax1 = axes[0, 0]
        ax1.hist(
            df["average_precision"], bins=20, alpha=0.7, color="blue", edgecolor="black"
        )
        ax1.axvline(
            df["average_precision"].mean(),
            color="red",
            linestyle="--",
            label=f"–°—Ä–µ–¥–Ω–µ–µ: {df['average_precision'].mean():.3f}",
        )
        ax1.set_xlabel("Average Precision")
        ax1.set_ylabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤")
        ax1.set_title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Average Precision")
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # 2. –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤
        ax2 = axes[0, 1]
        ax2.scatter(range(len(df)), df["query_time"], alpha=0.6)
        ax2.axhline(
            df["query_time"].mean(),
            color="red",
            linestyle="--",
            label=f"–°—Ä–µ–¥–Ω–µ–µ: {df['query_time'].mean():.3f}s",
        )
        ax2.set_xlabel("–ù–æ–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞")
        ax2.set_ylabel("–í—Ä–µ–º—è (—Å–µ–∫—É–Ω–¥—ã)")
        ax2.set_title("–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤")
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # 3. –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ k
        ax3 = axes[1, 0]
        k_values = [1, 5, 10]
        metrics = ["precision", "recall", "f1"]

        for metric in metrics:
            values = [df[f"{metric}@{k}"].mean() for k in k_values]
            ax3.plot(k_values, values, marker="o", label=metric.capitalize())

        ax3.set_xlabel("k")
        ax3.set_ylabel("–ó–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏")
        ax3.set_title("–ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö k")
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        ax3.set_xticks(k_values)

        # 4. –¢–æ–ø-10 –ª—É—á—à–∏—Ö –∏ —Ö—É–¥—à–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ AP
        ax4 = axes[1, 1]

        sorted_df = df.sort_values("average_precision")
        worst_5 = sorted_df.head(5)
        best_5 = sorted_df.tail(5)

        combined = pd.concat([worst_5, best_5])
        colors = ["red"] * 5 + ["green"] * 5

        y_pos = np.arange(len(combined))
        ax4.barh(y_pos, combined["average_precision"], color=colors, alpha=0.7)

        # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        labels = [q[:50] + "..." if len(q) > 50 else q for q in combined["query"]]
        ax4.set_yticks(y_pos)
        ax4.set_yticklabels(labels, fontsize=8)
        ax4.set_xlabel("Average Precision")
        ax4.set_title("–õ—É—á—à–∏–µ –∏ —Ö—É–¥—à–∏–µ –∑–∞–ø—Ä–æ—Å—ã –ø–æ AP")
        ax4.grid(True, alpha=0.3, axis="x")

        plt.tight_layout()

        if save_plot:
            plots_dir = DATA_DIR / "evaluation_results" / "plots"
            plots_dir.mkdir(exist_ok=True, parents=True)
            plt.savefig(
                plots_dir / f"{method_name.replace(' ', '_')}_detailed.png",
                dpi=300,
                bbox_inches="tight",
            )
            logger.info(f"–î–µ—Ç–∞–ª—å–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –¥–ª—è {method_name}")

        plt.show()

    def generate_report(self, output_path: Optional[Path] = None) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏

        Args:
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞

        Returns:
            –¢–µ–∫—Å—Ç –æ—Ç—á–µ—Ç–∞
        """
        if not self.results:
            return "–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞"

        report = []
        report.append("=" * 80)
        report.append("–û–¢–ß–ï–¢ –û –°–†–ê–í–ù–ï–ù–ò–ò –ú–ï–¢–û–î–û–í –ü–û–ò–°–ö–ê")
        report.append("=" * 80)
        report.append("")

        # –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        report.append(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {len(self.test_cases)}")
        report.append(f"–û—Ü–µ–Ω–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã: {', '.join(self.results.keys())}")
        report.append("")

        # –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
        report.append("–°–í–û–î–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
        report.append("-" * 80)

        # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞
        comparison_data = []
        for method_name, results in self.results.items():
            row = {
                "–ú–µ—Ç–æ–¥": method_name,
                "MAP": f"{results['aggregated']['MAP']:.3f}",
                "MRR": f"{results['aggregated']['MRR']:.3f}",
                "P@10": f"{results['aggregated']['avg_precision@10']:.3f}",
                "R@10": f"{results['aggregated']['avg_recall@10']:.3f}",
                "–í—Ä–µ–º—è –∑–∞–ø—Ä–æ—Å–∞ (—Å)": f"{results['aggregated']['avg_query_time']:.3f}",
                "–í—Ä–µ–º—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (—Å)": f"{results['method_stats']['index_time']:.1f}",
            }
            comparison_data.append(row)

        df = pd.DataFrame(comparison_data)
        report.append(df.to_string(index=False))
        report.append("")

        # –í—ã–≤–æ–¥—ã
        report.append("–û–°–ù–û–í–ù–´–ï –í–´–í–û–î–´")
        report.append("-" * 80)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à–∏–π –º–µ—Ç–æ–¥ –ø–æ MAP
        best_method = max(self.results.items(), key=lambda x: x[1]["aggregated"]["MAP"])
        report.append(
            f"‚úì –õ—É—á—à–∏–π –º–µ—Ç–æ–¥ –ø–æ MAP: {best_method[0]} ({best_method[1]['aggregated']['MAP']:.3f})"
        )

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π –º–µ—Ç–æ–¥
        fastest_method = min(
            self.results.items(), key=lambda x: x[1]["aggregated"]["avg_query_time"]
        )
        report.append(
            f"‚úì –°–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π –º–µ—Ç–æ–¥: {fastest_method[0]} ({fastest_method[1]['aggregated']['avg_query_time']:.3f}—Å)"
        )

        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Doc2Vec –∏ OpenAI
        if "Doc2Vec" in self.results and any(
            "OpenAI" in m for m in self.results.keys()
        ):
            doc2vec_map = self.results["Doc2Vec"]["aggregated"]["MAP"]
            openai_method = next(m for m in self.results.keys() if "OpenAI" in m)
            openai_map = self.results[openai_method]["aggregated"]["MAP"]

            improvement = ((doc2vec_map - openai_map) / openai_map) * 100

            report.append("")
            report.append("–°–†–ê–í–ù–ï–ù–ò–ï DOC2VEC –ò OPENAI")
            report.append("-" * 80)

            if doc2vec_map > openai_map:
                report.append(
                    f"‚úì Doc2Vec –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç {openai_method} –ø–æ MAP –Ω–∞ {improvement:.1f}%"
                )
            else:
                report.append(
                    f"‚úó {openai_method} –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Doc2Vec –ø–æ MAP –Ω–∞ {-improvement:.1f}%"
                )

            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏
            doc2vec_time = self.results["Doc2Vec"]["aggregated"]["avg_query_time"]
            openai_time = self.results[openai_method]["aggregated"]["avg_query_time"]
            time_ratio = openai_time / doc2vec_time

            report.append(f"‚úì Doc2Vec –±—ã—Å—Ç—Ä–µ–µ {openai_method} –≤ {time_ratio:.1f} —Ä–∞–∑")

            # –≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∞—è –≤—ã–≥–æ–¥–∞
            report.append("")
            report.append("–≠–ö–û–ù–û–ú–ò–ß–ï–°–ö–ê–Ø –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–¨")
            report.append("-" * 80)
            report.append("–ü—Ä–∏ 1000 –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –¥–µ–Ω—å:")

            # –ü—Ä–∏–º–µ—Ä–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å OpenAI embeddings
            openai_cost_per_1k_tokens = 0.0001  # $0.0001 per 1K tokens
            avg_tokens_per_query = 50  # –ø—Ä–∏–º–µ—Ä–Ω–æ
            daily_cost = (
                1000 * avg_tokens_per_query / 1000
            ) * openai_cost_per_1k_tokens
            monthly_cost = daily_cost * 30

            report.append(f"- –°—Ç–æ–∏–º–æ—Å—Ç—å OpenAI: ~${monthly_cost:.2f}/–º–µ—Å—è—Ü")
            report.append("- –°—Ç–æ–∏–º–æ—Å—Ç—å Doc2Vec: $0 (–ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è)")
            report.append(f"- –≠–∫–æ–Ω–æ–º–∏—è: ${monthly_cost:.2f}/–º–µ—Å—è—Ü")

        report.append("")
        report.append("=" * 80)
        report.append(f"–û—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {time.strftime('%Y-%m-%d %H:%M:%S')}")

        report_text = "\n".join(report)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        if output_path:
            with open(output_path, "w", encoding="utf-8") as f:
                f.write(report_text)
            logger.info(f"–û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {output_path}")

        return report_text


========================================
FILE: src\semantic_search\evaluation\metrics.py
========================================
"""–ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞"""

from typing import Dict, List, Set, Tuple

import numpy as np


class SearchMetrics:
    """–ö–ª–∞—Å—Å –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞"""

    @staticmethod
    def precision_at_k(retrieved: List[str], relevant: Set[str], k: int) -> float:
        """
        –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ k

        Args:
            retrieved: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–≤ –ø–æ—Ä—è–¥–∫–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è)
            relevant: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            k: –ü–æ–∑–∏—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏

        Returns:
            Precision@k
        """
        if k <= 0 or not retrieved:
            return 0.0

        retrieved_k = retrieved[:k]
        relevant_in_k = sum(1 for doc in retrieved_k if doc in relevant)

        return relevant_in_k / k

    @staticmethod
    def recall_at_k(retrieved: List[str], relevant: Set[str], k: int) -> float:
        """
        –ü–æ–ª–Ω–æ—Ç–∞ –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ k

        Args:
            retrieved: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            relevant: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            k: –ü–æ–∑–∏—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø–æ–ª–Ω–æ—Ç—ã

        Returns:
            Recall@k
        """
        if not relevant or k <= 0:
            return 0.0

        retrieved_k = retrieved[:k]
        relevant_in_k = sum(1 for doc in retrieved_k if doc in relevant)

        return relevant_in_k / len(relevant)

    @staticmethod
    def average_precision(retrieved: List[str], relevant: Set[str]) -> float:
        """
        –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å (AP)

        Args:
            retrieved: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            relevant: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        Returns:
            Average Precision
        """
        if not relevant or not retrieved:
            return 0.0

        precisions = []
        relevant_found = 0

        for i, doc in enumerate(retrieved):
            if doc in relevant:
                relevant_found += 1
                precision = relevant_found / (i + 1)
                precisions.append(precision)

        if not precisions:
            return 0.0

        return sum(precisions) / len(relevant)

    @staticmethod
    def mean_average_precision(results: List[Tuple[List[str], Set[str]]]) -> float:
        """
        –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ –≤—Å–µ–º –∑–∞–ø—Ä–æ—Å–∞–º (MAP)

        Args:
            results: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (retrieved, relevant) –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞

        Returns:
            Mean Average Precision
        """
        if not results:
            return 0.0

        ap_scores = [
            SearchMetrics.average_precision(retrieved, relevant)
            for retrieved, relevant in results
        ]

        return sum(ap_scores) / len(ap_scores)

    @staticmethod
    def dcg_at_k(
        retrieved: List[str], relevance_scores: Dict[str, float], k: int
    ) -> float:
        """
        Discounted Cumulative Gain –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ k

        Args:
            retrieved: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            relevance_scores: –°–ª–æ–≤–∞—Ä—å —Å –æ—Ü–µ–Ω–∫–∞–º–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (0-3)
            k: –ü–æ–∑–∏—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è DCG

        Returns:
            DCG@k
        """
        if k <= 0 or not retrieved:
            return 0.0

        dcg = 0.0
        for i, doc in enumerate(retrieved[:k]):
            rel = relevance_scores.get(doc, 0)
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º log2(i+2) —Ç–∞–∫ –∫–∞–∫ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 0
            dcg += (2**rel - 1) / np.log2(i + 2)

        return dcg

    @staticmethod
    def ndcg_at_k(
        retrieved: List[str], relevance_scores: Dict[str, float], k: int
    ) -> float:
        """
        Normalized Discounted Cumulative Gain –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ k

        Args:
            retrieved: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            relevance_scores: –°–ª–æ–≤–∞—Ä—å —Å –æ—Ü–µ–Ω–∫–∞–º–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            k: –ü–æ–∑–∏—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è NDCG

        Returns:
            NDCG@k
        """
        dcg = SearchMetrics.dcg_at_k(retrieved, relevance_scores, k)

        # –ò–¥–µ–∞–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ - —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        ideal_order = sorted(
            relevance_scores.keys(), key=lambda x: relevance_scores[x], reverse=True
        )

        idcg = SearchMetrics.dcg_at_k(ideal_order, relevance_scores, k)

        if idcg == 0:
            return 0.0

        return dcg / idcg

    @staticmethod
    def mean_reciprocal_rank(results: List[Tuple[List[str], Set[str]]]) -> float:
        """
        Mean Reciprocal Rank (MRR)

        Args:
            results: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (retrieved, relevant) –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞

        Returns:
            MRR
        """
        if not results:
            return 0.0

        reciprocal_ranks = []

        for retrieved, relevant in results:
            # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏—é –ø–µ—Ä–≤–æ–≥–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            for i, doc in enumerate(retrieved):
                if doc in relevant:
                    reciprocal_ranks.append(1 / (i + 1))
                    break
            else:
                # –ï—Å–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
                reciprocal_ranks.append(0.0)

        return sum(reciprocal_ranks) / len(reciprocal_ranks)

    @staticmethod
    def f1_at_k(retrieved: List[str], relevant: Set[str], k: int) -> float:
        """
        F1-–º–µ—Ä–∞ –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ k

        Args:
            retrieved: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            relevant: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            k: –ü–æ–∑–∏—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è F1

        Returns:
            F1@k
        """
        precision = SearchMetrics.precision_at_k(retrieved, relevant, k)
        recall = SearchMetrics.recall_at_k(retrieved, relevant, k)

        if precision + recall == 0:
            return 0.0

        return 2 * (precision * recall) / (precision + recall)

    @staticmethod
    def calculate_all_metrics(
        retrieved: List[str],
        relevant: Set[str],
        relevance_scores: Dict[str, float] = None,
        k_values: List[int] = [1, 5, 10],
    ) -> Dict[str, float]:
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞

        Args:
            retrieved: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            relevant: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            relevance_scores: –û—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–¥–ª—è NDCG)
            k_values: –ó–Ω–∞—á–µ–Ω–∏—è k –¥–ª—è –º–µ—Ç—Ä–∏–∫

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        metrics = {}

        # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö k
        for k in k_values:
            metrics[f"precision@{k}"] = SearchMetrics.precision_at_k(
                retrieved, relevant, k
            )
            metrics[f"recall@{k}"] = SearchMetrics.recall_at_k(retrieved, relevant, k)
            metrics[f"f1@{k}"] = SearchMetrics.f1_at_k(retrieved, relevant, k)

            if relevance_scores:
                metrics[f"ndcg@{k}"] = SearchMetrics.ndcg_at_k(
                    retrieved, relevance_scores, k
                )

        # –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        metrics["average_precision"] = SearchMetrics.average_precision(
            retrieved, relevant
        )

        # MRR –¥–ª—è –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        for i, doc in enumerate(retrieved):
            if doc in relevant:
                metrics["reciprocal_rank"] = 1 / (i + 1)
                break
        else:
            metrics["reciprocal_rank"] = 0.0

        return metrics


========================================
FILE: scripts\build.py
========================================
"""–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–±–æ—Ä–∫–∏ –∏—Å–ø–æ–ª–Ω—è–µ–º–æ–≥–æ —Ñ–∞–π–ª–∞ —Å –ø–æ–º–æ—â—å—é PyInstaller"""

import argparse
import shutil
import sys
from pathlib import Path


def clean_build_dirs():
    """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π —Å–±–æ—Ä–∫–∏"""
    dirs_to_clean = ["build", "dist"]
    for dir_name in dirs_to_clean:
        if Path(dir_name).exists():
            shutil.rmtree(dir_name)
            print(f"‚úì –£–¥–∞–ª–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {dir_name}")


def create_spec_file(onefile=False, windowed=True, name="SemanticSearch"):
    """–°–æ–∑–¥–∞–Ω–∏–µ spec —Ñ–∞–π–ª–∞ –¥–ª—è PyInstaller"""

    # –ü—É—Ç–∏
    src_path = Path(__file__).parent.parent / "src"
    main_script = src_path / "semantic_search" / "main.py"

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    datas = [
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        ("config", "config"),
        # –ò–∫–æ–Ω–∫–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
        # ("assets/icon.ico", "assets"),
    ]

    # –°–∫—Ä—ã—Ç—ã–µ –∏–º–ø–æ—Ä—Ç—ã
    hidden_imports = [
        "semantic_search",
        "semantic_search.core",
        "semantic_search.gui",
        "semantic_search.utils",
        "semantic_search.evaluation",
        "PyQt6.QtCore",
        "PyQt6.QtGui",
        "PyQt6.QtWidgets",
        "gensim.models.doc2vec",
        "sklearn.metrics.pairwise",
        "spacy",
        "click",
        "loguru",
        "docx",
        "pymupdf",
    ]

    # –ò—Å–∫–ª—é—á–µ–Ω–∏—è
    excludes = [
        "matplotlib",
        "notebook",
        "jupyter",
        "pytest",
        "mypy",
        "ruff",
    ]

    spec_content = f"""# -*- mode: python ; coding: utf-8 -*-

a = Analysis(
    [r'{main_script}'],
    pathex=[r'{src_path}'],
    binaries=[],
    datas={datas},
    hiddenimports={hidden_imports},
    hookspath=[],
    hooksconfig={{}},
    runtime_hooks=[],
    excludes={excludes},
    noarchive=False,
)

pyz = PYZ(a.pure)

"""

    if onefile:
        spec_content += f"""exe = EXE(
    pyz,
    a.scripts,
    a.binaries,
    a.datas,
    [],
    name='{name}',
    debug=False,
    bootloader_ignore_signals=False,
    strip=False,
    upx=True,
    upx_exclude=[],
    runtime_tmpdir=None,
    console=not {windowed},
    disable_windowed_traceback=False,
    argv_emulation=False,
    target_arch=None,
    codesign_identity=None,
    entitlements_file=None,
)"""
    else:
        spec_content += f"""exe = EXE(
    pyz,
    a.scripts,
    [],
    exclude_binaries=True,
    name='{name}',
    debug=False,
    bootloader_ignore_signals=False,
    strip=False,
    upx=True,
    console=not {windowed},
    disable_windowed_traceback=False,
    argv_emulation=False,
    target_arch=None,
    codesign_identity=None,
    entitlements_file=None,
)

coll = COLLECT(
    exe,
    a.binaries,
    a.datas,
    strip=False,
    upx=True,
    upx_exclude=[],
    name='{name}',
)"""

    spec_file = Path(f"{name}.spec")
    with open(spec_file, "w", encoding="utf-8") as f:
        f.write(spec_content)

    print(f"‚úì –°–æ–∑–¥–∞–Ω spec —Ñ–∞–π–ª: {spec_file}")
    return spec_file


def build_executable(spec_file, clean=True):
    """–ó–∞–ø—É—Å–∫ PyInstaller –¥–ª—è —Å–±–æ—Ä–∫–∏"""
    import subprocess

    cmd = ["pyinstaller"]

    if clean:
        cmd.append("--clean")

    cmd.extend(["--noconfirm", str(spec_file)])

    print("\nüî® –ó–∞–ø—É—Å–∫ PyInstaller...")
    print(f"–ö–æ–º–∞–Ω–¥–∞: {' '.join(cmd)}")

    try:
        result = subprocess.run(cmd, check=True)
        print("\n‚úÖ –°–±–æ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        return True
    except subprocess.CalledProcessError as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∫–∏: {e}")
        return False


def post_build_actions(name="SemanticSearch", onefile=False):
    """–î–µ–π—Å—Ç–≤–∏—è –ø–æ—Å–ª–µ —Å–±–æ—Ä–∫–∏"""

    if onefile:
        exe_path = Path("dist") / f"{name}.exe"
        if exe_path.exists():
            print(f"\nüì¶ –ò—Å–ø–æ–ª–Ω—è–µ–º—ã–π —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω: {exe_path}")
            print(f"   –†–∞–∑–º–µ—Ä: {exe_path.stat().st_size / 1024 / 1024:.1f} –ú–ë")
    else:
        dist_dir = Path("dist") / name
        if dist_dir.exists():
            exe_path = dist_dir / f"{name}.exe"
            if exe_path.exists():
                print(f"\nüì¶ –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —Å–æ–±—Ä–∞–Ω–æ –≤: {dist_dir}")
                print(f"   –ò—Å–ø–æ–ª–Ω—è–µ–º—ã–π —Ñ–∞–π–ª: {exe_path}")

                # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö
                for dir_name in ["data", "logs", "config"]:
                    (dist_dir / dir_name).mkdir(exist_ok=True)
                print("   ‚úì –°–æ–∑–¥–∞–Ω—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö")

                # –ö–æ–ø–∏—Ä—É–µ–º README
                readme_src = Path("README.md")
                if readme_src.exists():
                    shutil.copy2(readme_src, dist_dir)
                    print("   ‚úì –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω README.md")


def main():
    parser = argparse.ArgumentParser(
        description="–°–±–æ—Ä–∫–∞ Semantic Search –≤ –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–π —Ñ–∞–π–ª"
    )
    parser.add_argument("--onefile", action="store_true", help="–°–æ–±—Ä–∞—Ç—å –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª")
    parser.add_argument(
        "--windowed", action="store_true", help="–ë–µ–∑ –∫–æ–Ω—Å–æ–ª–∏ (GUI —Ä–µ–∂–∏–º)"
    )
    parser.add_argument(
        "--name", default="SemanticSearch", help="–ò–º—è –∏—Å–ø–æ–ª–Ω—è–µ–º–æ–≥–æ —Ñ–∞–π–ª–∞"
    )
    parser.add_argument(
        "--no-clean", action="store_true", help="–ù–µ –æ—á–∏—â–∞—Ç—å —Å—Ç–∞—Ä—ã–µ —Å–±–æ—Ä–∫–∏"
    )

    args = parser.parse_args()

    print("üöÄ Semantic Search Builder")
    print("=" * 50)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ PyInstaller
    try:
        import PyInstaller

        print(f"‚úì PyInstaller {PyInstaller.__version__} —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    except ImportError:
        print("‚ùå PyInstaller –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
        print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: poetry add --group dev pyinstaller")
        sys.exit(1)

    # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö —Å–±–æ—Ä–æ–∫
    if not args.no_clean:
        clean_build_dirs()

    # –°–æ–∑–¥–∞–Ω–∏–µ spec —Ñ–∞–π–ª–∞
    spec_file = create_spec_file(
        onefile=args.onefile, windowed=args.windowed, name=args.name
    )

    # –°–±–æ—Ä–∫–∞
    if build_executable(spec_file, clean=not args.no_clean):
        post_build_actions(args.name, args.onefile)

        print("\nüí° –ü–æ–¥—Å–∫–∞–∑–∫–∏:")
        print("   - –î–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ UPX")
        print("   - –î–æ–±–∞–≤—å—Ç–µ –∏–∫–æ–Ω–∫—É –≤ assets/icon.ico")
        print("   - –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –Ω–∞ —á–∏—Å—Ç–æ–π —Å–∏—Å—Ç–µ–º–µ")

        if args.onefile:
            print("\n‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: ")
            print("   –†–µ–∂–∏–º onefile –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –º–µ–¥–ª–µ–Ω–Ω–µ–µ")
            print("   –∏ –≤—ã–∑—ã–≤–∞—Ç—å –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è –∞–Ω—Ç–∏–≤–∏—Ä—É—Å–æ–≤")
    else:
        print("\n‚ùå –°–±–æ—Ä–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –≤—ã—à–µ.")
        sys.exit(1)


if __name__ == "__main__":
    main()


========================================
FILE: scripts\cache_clear.py
========================================
#!/usr/bin/env python
"""–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""

import sys
from pathlib import Path

from semantic_search.config import CACHE_DIR
from semantic_search.utils.cache_manager import CacheManager

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src –≤ sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))


def clear_cache():
    """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    print("üßπ –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ Semantic Search")
    print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫—ç—à–∞: {CACHE_DIR}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    if not CACHE_DIR.exists():
        print("‚ÑπÔ∏è  –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫—ç—à–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ù–µ—á–µ–≥–æ –æ—á–∏—â–∞—Ç—å.")
        return

    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
    cache_files = list(CACHE_DIR.glob("*.pkl"))
    total_size = sum(f.stat().st_size for f in cache_files)

    if not cache_files:
        print("‚ÑπÔ∏è  –ö—ç—à –ø—É—Å—Ç. –ù–µ—á–µ–≥–æ –æ—á–∏—â–∞—Ç—å.")
        return

    print(f"üìä –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(cache_files)}")
    print(f"üíæ –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size / 1024 / 1024:.2f} –ú–ë")

    # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
    response = input("\n‚ùì –í—ã —É–≤–µ—Ä–µ–Ω—ã, —á—Ç–æ —Ö–æ—Ç–∏—Ç–µ –æ—á–∏—Å—Ç–∏—Ç—å –∫—ç—à? (y/n): ")

    if response.lower() != "y":
        print("‚ùå –û—á–∏—Å—Ç–∫–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞.")
        return

    # –û—á–∏—â–∞–µ–º –∫—ç—à
    cache_manager = CacheManager(CACHE_DIR)
    if cache_manager.clear():
        print("‚úÖ –ö—ç—à —É—Å–ø–µ—à–Ω–æ –æ—á–∏—â–µ–Ω.")
        print(f"üóëÔ∏è  –£–¥–∞–ª–µ–Ω–æ {len(cache_files)} —Ñ–∞–π–ª–æ–≤")
        print(f"üí® –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–æ {total_size / 1024 / 1024:.2f} –ú–ë")
    else:
        print("‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –∫—ç—à–∞.")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    try:
        clear_cache()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  –û–ø–µ—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()


========================================
FILE: scripts\check_dependencies.py
========================================
#!/usr/bin/env python
"""–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""

import importlib
import subprocess
import sys
from pathlib import Path


def check_module(module_name: str, import_name: str = None) -> tuple[bool, str]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥—É–ª—è"""
    import_name = import_name or module_name
    try:
        module = importlib.import_module(import_name)
        version = getattr(module, "__version__", "unknown")
        return True, version
    except ImportError:
        return False, None


def check_spacy_models():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π SpaCy"""
    models = {
        "ru_core_news_sm": "–†—É—Å—Å–∫–∞—è –º–æ–¥–µ–ª—å (–º–∞–ª–µ–Ω—å–∫–∞—è)",
        "en_core_web_sm": "–ê–Ω–≥–ª–∏–π—Å–∫–∞—è –º–æ–¥–µ–ª—å (–º–∞–ª–µ–Ω—å–∫–∞—è)",
    }

    results = {}
    try:
        import spacy

        for model_name, description in models.items():
            try:
                nlp = spacy.load(model_name)
                results[model_name] = (True, description)
            except OSError:
                results[model_name] = (False, description)
    except ImportError:
        for model_name, description in models.items():
            results[model_name] = (False, description)

    return results


def check_system_dependencies():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""
    results = {}

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ Python –≤–µ—Ä—Å–∏–∏
    python_version = (
        f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
    )
    results["Python"] = (
        sys.version_info.major == 3 and 10 <= sys.version_info.minor <= 12,
        python_version,
    )

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ Poetry
    try:
        result = subprocess.run(["poetry", "--version"], capture_output=True, text=True)
        if result.returncode == 0:
            version = result.stdout.strip().split()[-1]
            results["Poetry"] = (True, version)
        else:
            results["Poetry"] = (False, None)
    except FileNotFoundError:
        results["Poetry"] = (False, None)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ Microsoft Word (—Ç–æ–ª—å–∫–æ –Ω–∞ Windows)
    if sys.platform == "win32":
        try:
            import win32com.client

            word = win32com.client.Dispatch("Word.Application")
            results["Microsoft Word"] = (True, "installed")
            word.Quit()
        except:
            results["Microsoft Word"] = (False, None)

    return results


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π Semantic Search")
    print("=" * 60)

    # –û—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
    core_dependencies = [
        ("pymupdf", "fitz", "PDF –æ–±—Ä–∞–±–æ—Ç–∫–∞"),
        ("python-docx", "docx", "DOCX –æ–±—Ä–∞–±–æ—Ç–∫–∞"),
        ("spacy", None, "NLP –æ–±—Ä–∞–±–æ—Ç–∫–∞"),
        ("gensim", None, "Doc2Vec –º–æ–¥–µ–ª—å"),
        ("PyQt6", "PyQt6.QtCore", "GUI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å"),
        ("loguru", None, "–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ"),
        ("click", None, "CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å"),
        ("scikit-learn", "sklearn", "ML —É—Ç–∏–ª–∏—Ç—ã"),
        ("psutil", None, "–°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"),
        ("numpy", None, "–ß–∏—Å–ª–µ–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è"),
        ("pandas", None, "–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"),
        ("matplotlib", None, "–ì—Ä–∞—Ñ–∏–∫–∏"),
        ("tqdm", None, "–ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä—ã"),
    ]

    # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
    optional_dependencies = [
        ("openai", None, "OpenAI –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è"),
        ("seaborn", None, "–£–ª—É—á—à–µ–Ω–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏"),
    ]

    # Windows-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ
    if sys.platform == "win32":
        core_dependencies.append(("pywin32", "win32com", "DOC –æ–±—Ä–∞–±–æ—Ç–∫–∞"))

    print("\nüì¶ –û—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:")
    print("-" * 60)

    all_ok = True
    for package_name, import_name, description in core_dependencies:
        installed, version = check_module(package_name, import_name)
        if installed:
            print(f"‚úÖ {package_name:<20} {version:<15} {description}")
        else:
            print(f"‚ùå {package_name:<20} {'–ù–ï –£–°–¢–ê–ù–û–í–õ–ï–ù':<15} {description}")
            all_ok = False

    print("\nüì¶ –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:")
    print("-" * 60)

    for package_name, import_name, description in optional_dependencies:
        installed, version = check_module(package_name, import_name)
        if installed:
            print(f"‚úÖ {package_name:<20} {version:<15} {description}")
        else:
            print(f"‚ö†Ô∏è  {package_name:<20} {'–Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω':<15} {description}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ SpaCy –º–æ–¥–µ–ª–µ–π
    print("\nüß† SpaCy –º–æ–¥–µ–ª–∏:")
    print("-" * 60)

    spacy_models = check_spacy_models()
    spacy_ok = True
    for model_name, (installed, description) in spacy_models.items():
        if installed:
            print(f"‚úÖ {model_name:<20} {description}")
        else:
            print(f"‚ùå {model_name:<20} {description} - –ù–ï –£–°–¢–ê–ù–û–í–õ–ï–ù–ê")
            spacy_ok = False

    # –°–∏—Å—Ç–µ–º–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
    print("\nüíª –°–∏—Å—Ç–µ–º–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:")
    print("-" * 60)

    system_deps = check_system_dependencies()
    for dep_name, (installed, version) in system_deps.items():
        if installed:
            print(f"‚úÖ {dep_name:<20} {version}")
        else:
            if dep_name == "Microsoft Word":
                print(f"‚ö†Ô∏è  {dep_name:<20} –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (—Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–ª—è .doc —Ñ–∞–π–ª–æ–≤)")
            else:
                print(f"‚ùå {dep_name:<20} –ù–ï –£–°–¢–ê–ù–û–í–õ–ï–ù")
                all_ok = False

    # –ò—Ç–æ–≥–æ–≤—ã–π —Å—Ç–∞—Ç—É—Å
    print("\n" + "=" * 60)

    if all_ok and spacy_ok:
        print("‚úÖ –í—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")
        print("\nüöÄ –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –≥–æ—Ç–æ–≤–æ –∫ –∑–∞–ø—É—Å–∫—É:")
        print("   GUI: poetry run semantic-search")
        print("   CLI: poetry run semantic-search-cli --help")
    else:
        print("‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã —Å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏!")

        print("\nüìã –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—é:")

        if not all_ok:
            print("\n1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:")
            print("   poetry install")

        if not spacy_ok:
            print("\n2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ SpaCy:")
            print("   poetry run python scripts/setup_spacy.py")
            print("   –∏–ª–∏ –≤—Ä—É—á–Ω—É—é:")
            print("   poetry run python -m spacy download ru_core_news_sm")
            print("   poetry run python -m spacy download en_core_web_sm")

        if not system_deps.get("Poetry", (False, None))[0]:
            print("\n3. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Poetry:")
            print("   –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏: https://python-poetry.org/docs/#installation")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—É—Ç–µ–π
    print("\nüìÅ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞:")
    print("-" * 60)

    project_root = Path(__file__).parent.parent
    important_paths = [
        (project_root / "src" / "semantic_search", "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥"),
        (project_root / "data", "–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–∞–Ω–Ω—ã—Ö"),
        (project_root / "pyproject.toml", "–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Poetry"),
    ]

    for path, description in important_paths:
        if path.exists():
            print(f"‚úÖ {str(path.relative_to(project_root)):<30} {description}")
        else:
            print(
                f"‚ùå {str(path.relative_to(project_root)):<30} {description} - –ù–ï –ù–ê–ô–î–ï–ù"
            )


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


========================================
FILE: scripts\cleanup.py
========================================
#!/usr/bin/env python
"""–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏ –ª–æ–≥–æ–≤"""

import shutil
import sys
from datetime import datetime, timedelta
from pathlib import Path

from semantic_search.config import CACHE_DIR, DATA_DIR, LOGS_DIR, MODELS_DIR, TEMP_DIR

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src –≤ sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))


def format_file_size(size_bytes):
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞"""
    for unit in ["B", "KB", "MB", "GB"]:
        if size_bytes < 1024.0:
            return f"{size_bytes:.1f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.1f} TB"


def get_directory_size(directory: Path) -> int:
    """–ü–æ–ª—É—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
    total_size = 0
    if directory.exists():
        for file_path in directory.rglob("*"):
            if file_path.is_file():
                total_size += file_path.stat().st_size
    return total_size


def clean_temp_files():
    """–û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    print("\nüóëÔ∏è  –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤...")

    if not TEMP_DIR.exists():
        print("   ‚ÑπÔ∏è  –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è temp –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return 0

    temp_files = list(TEMP_DIR.rglob("*"))
    temp_size = get_directory_size(TEMP_DIR)

    if not temp_files:
        print("   ‚úÖ –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç")
        return 0

    print(f"   üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len([f for f in temp_files if f.is_file()])}")
    print(f"   üíæ –†–∞–∑–º–µ—Ä: {format_file_size(temp_size)}")

    try:
        shutil.rmtree(TEMP_DIR)
        TEMP_DIR.mkdir(exist_ok=True)
        print(f"   ‚úÖ –û—á–∏—â–µ–Ω–æ {format_file_size(temp_size)}")
        return temp_size
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
        return 0


def clean_old_logs(days: int = 7):
    """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤"""
    print(f"\nüìú –û—á–∏—Å—Ç–∫–∞ –ª–æ–≥–æ–≤ —Å—Ç–∞—Ä—à–µ {days} –¥–Ω–µ–π...")

    if not LOGS_DIR.exists():
        print("   ‚ÑπÔ∏è  –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ª–æ–≥–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return 0

    cutoff_date = datetime.now() - timedelta(days=days)
    old_logs = []
    total_size = 0

    for log_file in LOGS_DIR.glob("*.log*"):
        if log_file.is_file():
            modified_time = datetime.fromtimestamp(log_file.stat().st_mtime)
            if modified_time < cutoff_date:
                old_logs.append(log_file)
                total_size += log_file.stat().st_size

    if not old_logs:
        print("   ‚úÖ –°—Ç–∞—Ä—ã–µ –ª–æ–≥–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç")
        return 0

    print(f"   üìÅ –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤: {len(old_logs)}")
    print(f"   üíæ –†–∞–∑–º–µ—Ä: {format_file_size(total_size)}")

    removed_size = 0
    for log_file in old_logs:
        try:
            size = log_file.stat().st_size
            log_file.unlink()
            removed_size += size
        except Exception as e:
            print(f"   ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å {log_file.name}: {e}")

    print(f"   ‚úÖ –û—á–∏—â–µ–Ω–æ {format_file_size(removed_size)}")
    return removed_size


def clean_cache():
    """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞"""
    print("\nüíæ –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞...")

    if not CACHE_DIR.exists():
        print("   ‚ÑπÔ∏è  –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫—ç—à–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return 0

    from semantic_search.utils.cache_manager import CacheManager

    cache_size = get_directory_size(CACHE_DIR)
    cache_files = list(CACHE_DIR.glob("*.pkl"))

    if not cache_files:
        print("   ‚úÖ –ö—ç—à –ø—É—Å—Ç")
        return 0

    print(f"   üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(cache_files)}")
    print(f"   üíæ –†–∞–∑–º–µ—Ä: {format_file_size(cache_size)}")

    cache_manager = CacheManager(CACHE_DIR)
    if cache_manager.clear():
        print(f"   ‚úÖ –û—á–∏—â–µ–Ω–æ {format_file_size(cache_size)}")
        return cache_size
    else:
        print("   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –∫—ç—à–∞")
        return 0


def clean_evaluation_results():
    """–û—á–∏—Å—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ü–µ–Ω–∫–∏"""
    print("\nüìä –û—á–∏—Å—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ü–µ–Ω–∫–∏...")

    eval_dir = DATA_DIR / "evaluation_results"
    if not eval_dir.exists():
        print("   ‚ÑπÔ∏è  –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return 0

    eval_size = get_directory_size(eval_dir)
    eval_files = list(eval_dir.rglob("*"))

    if not eval_files:
        print("   ‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç")
        return 0

    print(f"   üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len([f for f in eval_files if f.is_file()])}")
    print(f"   üíæ –†–∞–∑–º–µ—Ä: {format_file_size(eval_size)}")

    try:
        shutil.rmtree(eval_dir)
        eval_dir.mkdir(exist_ok=True)
        print(f"   ‚úÖ –û—á–∏—â–µ–Ω–æ {format_file_size(eval_size)}")
        return eval_size
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
        return 0


def show_disk_usage():
    """–ü–æ–∫–∞–∑–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∏—Å–∫–∞"""
    print("\nüìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∏—Å–∫–æ–≤–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞:")
    print("=" * 50)

    directories = [
        (DATA_DIR, "–î–∞–Ω–Ω—ã–µ"),
        (MODELS_DIR, "–ú–æ–¥–µ–ª–∏"),
        (CACHE_DIR, "–ö—ç—à"),
        (TEMP_DIR, "–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã"),
        (LOGS_DIR, "–õ–æ–≥–∏"),
        (DATA_DIR / "evaluation_results", "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏"),
    ]

    total_size = 0
    for directory, name in directories:
        if directory.exists():
            size = get_directory_size(directory)
            total_size += size
            print(f"{name:.<30} {format_file_size(size):>15}")

    print("-" * 50)
    print(f"{'–ò–¢–û–ì–û':.<30} {format_file_size(total_size):>15}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse

    parser = argparse.ArgumentParser(description="–û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏ –∫—ç—à–∞")
    parser.add_argument(
        "--all",
        "-a",
        action="store_true",
        help="–û—á–∏—Å—Ç–∏—Ç—å –≤—Å–µ (–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã, –∫—ç—à, —Å—Ç–∞—Ä—ã–µ –ª–æ–≥–∏)",
    )
    parser.add_argument(
        "--temp", "-t", action="store_true", help="–û—á–∏—Å—Ç–∏—Ç—å —Ç–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã"
    )
    parser.add_argument(
        "--cache", "-c", action="store_true", help="–û—á–∏—Å—Ç–∏—Ç—å —Ç–æ–ª—å–∫–æ –∫—ç—à"
    )
    parser.add_argument(
        "--logs", "-l", action="store_true", help="–û—á–∏—Å—Ç–∏—Ç—å —Å—Ç–∞—Ä—ã–µ –ª–æ–≥–∏"
    )
    parser.add_argument(
        "--eval", "-e", action="store_true", help="–û—á–∏—Å—Ç–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏"
    )
    parser.add_argument(
        "--days",
        type=int,
        default=7,
        help="–£–¥–∞–ª—è—Ç—å –ª–æ–≥–∏ —Å—Ç–∞—Ä—à–µ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–Ω–µ–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 7)",
    )
    parser.add_argument(
        "--usage", "-u", action="store_true", help="–ü–æ–∫–∞–∑–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∏—Å–∫–∞"
    )
    parser.add_argument(
        "--yes", "-y", action="store_true", help="–ù–µ –∑–∞–ø—Ä–∞—à–∏–≤–∞—Ç—å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ"
    )

    args = parser.parse_args()

    # –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã —Ñ–ª–∞–≥–∏, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∏—Å–∫–∞
    if not any([args.all, args.temp, args.cache, args.logs, args.eval, args.usage]):
        args.usage = True

    print("üßπ –£—Ç–∏–ª–∏—Ç–∞ –æ—á–∏—Å—Ç–∫–∏ Semantic Search")
    print("=" * 50)

    if args.usage:
        show_disk_usage()
        return

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —á—Ç–æ –±—É–¥–µ–º –æ—á–∏—â–∞—Ç—å
    actions = []
    if args.all or args.temp:
        actions.append(("–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã", clean_temp_files))
    if args.all or args.cache:
        actions.append(("–∫—ç—à", clean_cache))
    if args.all or args.logs:
        actions.append(
            (f"–ª–æ–≥–∏ —Å—Ç–∞—Ä—à–µ {args.days} –¥–Ω–µ–π", lambda: clean_old_logs(args.days))
        )
    if args.eval:
        actions.append(("—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏", clean_evaluation_results))

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –±—É–¥–µ—Ç –æ—á–∏—â–µ–Ω–æ
    print("\nüéØ –ë—É–¥—É—Ç –æ—á–∏—â–µ–Ω—ã:")
    for name, _ in actions:
        print(f"   ‚Ä¢ {name}")

    # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
    if not args.yes:
        response = input("\n‚ùì –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å? (y/n): ")
        if response.lower() != "y":
            print("‚ùå –û—á–∏—Å—Ç–∫–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞.")
            return

    # –í—ã–ø–æ–ª–Ω—è–µ–º –æ—á–∏—Å—Ç–∫—É
    total_cleaned = 0
    for name, action in actions:
        cleaned = action()
        total_cleaned += cleaned

    print(f"\n‚úÖ –í—Å–µ–≥–æ –æ—á–∏—â–µ–Ω–æ: {format_file_size(total_cleaned)}")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  –û–ø–µ—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞.")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


========================================
FILE: scripts\demo_evaluation.py
========================================
"""–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞"""

import os
from pathlib import Path

from semantic_search.core.doc2vec_trainer import Doc2VecTrainer
from semantic_search.core.search_engine import SemanticSearchEngine
from semantic_search.evaluation.baselines import (
    Doc2VecSearchAdapter,
    OpenAISearchBaseline,
)
from semantic_search.evaluation.comparison import QueryTestCase, SearchComparison


def create_demo_test_cases():
    """–°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤"""

    test_cases = [
        QueryTestCase(
            query="–ü–æ–Ω—è—Ç–∏–µ –≥–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–µ",
            relevant_docs={
                "–¢—Ä–∞–Ω—Å–ª–∏–≥–≤–∏–∑–º/-1.pdf",
                "SALMAN RUSHDIE/Language is assumed by many to be a stable medium of communication.docx",
                "–ì–ª–æ–±–∞–ª–∏–∑–∞—Ü–∏—è –∏ –≥–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è/glokalizatsiya-i-vozvrat-etnichnosti-v-vek-globalizatsii.pdf",
            },
            relevance_scores={
                "–¢—Ä–∞–Ω—Å–ª–∏–≥–≤–∏–∑–º/-1.pdf": 3,
                "SALMAN RUSHDIE/Language is assumed by many to be a stable medium of communication.docx": 3,
                "–ì–ª–æ–±–∞–ª–∏–∑–∞—Ü–∏—è –∏ –≥–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è/glokalizatsiya-i-vozvrat-etnichnosti-v-vek-globalizatsii.pdf": 2,
                "–õ–∏–Ω–≥–≤–æ–∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å/Linguistic_Creativity_Cognitive_And_Communicative_.pdf": 1,
            },
            description="–ü–æ–Ω—è—Ç–∏–µ –≥–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–µ",
        ),
        QueryTestCase(
            query="–¢—Ä–∞–Ω—Å–ª–∏–Ω–≥–≤–∏–∑–º –∏ —Ç—Ä–∞–Ω—Å–ª–∏–Ω–≥–≤–∞–ª—å–Ω–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞",
            relevant_docs={
                "–¢—Ä–∞–Ω—Å–ª–∏–≥–≤–∏–∑–º/-1.pdf",
                "SALMAN RUSHDIE/rushdie-1997-notes-on-writing-and-the-nation.pdf",
                "glocal_strategy.pdf",
            },
            relevance_scores={
                "–¢—Ä–∞–Ω—Å–ª–∏–≥–≤–∏–∑–º/-1.pdf": 3,
                "SALMAN RUSHDIE/rushdie-1997-notes-on-writing-and-the-nation.pdf": 3,
                "SALMAN RUSHDIE/Hybridization_Heteroglossia_and_the_engl.doc": 2,
            },
            description="–¢—Ä–∞–Ω—Å–ª–∏–Ω–≥–≤–∏–∑–º –∏ —Ç—Ä–∞–Ω—Å–ª–∏–Ω–≥–≤–∞–ª—å–Ω–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞",
        ),
        QueryTestCase(
            query="–ì–µ—Ç–µ—Ä–æ–ª–æ–≥–∏—è –∏ –¥–∏–∞–ª–æ–≥–∏–∑–º",
            relevant_docs={"cultural_marketing.pdf", "cross_cultural_comm.pdf"},
            relevance_scores={
                "–¢—Ä–∞–Ω—Å–ª–∏–≥–≤–∏–∑–º/-1.pdf": 3,
                "SALMAN RUSHDIE/12.docx": 3,
                " –ë–∞—Ö—Ç–∏–Ω/Zebroski-MikhailBakhtinQuestion-1992.pdf": 1,
            },
            description="–ì–µ—Ç–µ—Ä–æ–ª–æ–≥–∏—è –∏ –¥–∏–∞–ª–æ–≥–∏–∑–º",
        ),
    ]

    return test_cases


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    print("=" * 80)
    print("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –°–†–ê–í–ù–ï–ù–ò–Ø DOC2VEC –ò OPENAI EMBEDDINGS")
    print("=" * 80)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ API –∫–ª—é—á–∞
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("\n‚ùå –û–®–ò–ë–ö–ê: OpenAI API key –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è OPENAI_API_KEY")
        print("–ù–∞–ø—Ä–∏–º–µ—Ä: set OPENAI_API_KEY=sk-...")
        return

    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Doc2Vec
    print("\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Doc2Vec...")
    model_name = "doc2vec_model"  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –∏–º—è –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏

    trainer = Doc2VecTrainer()
    model = trainer.load_model(model_name)

    if not model:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å '{model_name}'")
        print("–°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å –∫–æ–º–∞–Ω–¥–æ–π:")
        print("poetry run semantic-search-cli train -d /path/to/documents")
        return

    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {len(model.dv)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞
    search_engine = SemanticSearchEngine(model, trainer.corpus_info)

    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤
    print("\nüß™ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤...")
    test_cases = create_demo_test_cases()
    print(f"   –°–æ–∑–¥–∞–Ω–æ {len(test_cases)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")

    # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    comparison = SearchComparison(test_cases)

    # –°–æ–∑–¥–∞–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ –º–µ—Ç–æ–¥–æ–≤
    print("\nüîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞...")

    # Doc2Vec –∞–¥–∞–ø—Ç–µ—Ä
    doc2vec_adapter = Doc2VecSearchAdapter(search_engine, trainer.corpus_info)
    print("‚úÖ Doc2Vec –∞–¥–∞–ø—Ç–µ—Ä –≥–æ—Ç–æ–≤")

    # OpenAI baseline
    try:
        openai_baseline = OpenAISearchBaseline(api_key=api_key)
        print("‚úÖ OpenAI baseline –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ OpenAI: {e}")
        return

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è OpenAI
    print("\nüìö –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è OpenAI...")
    print("   (–î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 20 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")

    # –ë–µ—Ä–µ–º –ø–æ–¥–≤—ã–±–æ—Ä–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    demo_documents = []
    for i, (tokens, doc_id, metadata) in enumerate(trainer.corpus_info[:20]):
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Ç–æ–∫–µ–Ω–æ–≤
        text = " ".join(tokens[:300])  # –ü–µ—Ä–≤—ã–µ 300 —Ç–æ–∫–µ–Ω–æ–≤
        demo_documents.append((doc_id, text, metadata))

    try:
        openai_baseline.index(demo_documents)
        print(f"‚úÖ –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ {len(demo_documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")
        return

    # –û—Ü–µ–Ω–∫–∞ –º–µ—Ç–æ–¥–æ–≤
    print("\nüìä –û–¶–ï–ù–ö–ê –ú–ï–¢–û–î–û–í")
    print("-" * 80)

    # Doc2Vec
    print("\n1Ô∏è‚É£ –û—Ü–µ–Ω–∫–∞ Doc2Vec...")
    doc2vec_results = comparison.evaluate_method(
        doc2vec_adapter, top_k=10, verbose=True
    )

    # OpenAI
    print("\n2Ô∏è‚É£ –û—Ü–µ–Ω–∫–∞ OpenAI embeddings...")
    openai_results = comparison.evaluate_method(openai_baseline, top_k=10, verbose=True)

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\nüìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–†–ê–í–ù–ï–ù–ò–Ø")
    print("=" * 80)

    # –°–æ–∑–¥–∞–µ–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
    df_comparison = comparison.compare_methods(
        [doc2vec_adapter, openai_baseline], save_results=True
    )

    print("\n–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞:")
    print(df_comparison.to_string(index=False))

    # –û—Å–Ω–æ–≤–Ω—ã–µ –≤—ã–≤–æ–¥—ã
    doc2vec_map = doc2vec_results["aggregated"]["MAP"]
    openai_map = openai_results["aggregated"]["MAP"]

    print("\nüéØ –û–°–ù–û–í–ù–´–ï –í–´–í–û–î–´:")
    print("-" * 80)

    if doc2vec_map > openai_map:
        improvement = ((doc2vec_map - openai_map) / openai_map) * 100
        print(f"‚úÖ Doc2Vec –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç OpenAI –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –ø–æ–∏—Å–∫–∞ –Ω–∞ {improvement:.1f}%")
    else:
        improvement = ((openai_map - doc2vec_map) / doc2vec_map) * 100
        print(f"‚ùå OpenAI –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Doc2Vec –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –ø–æ–∏—Å–∫–∞ –Ω–∞ {improvement:.1f}%")

    # –°–∫–æ—Ä–æ—Å—Ç—å
    doc2vec_time = doc2vec_results["aggregated"]["avg_query_time"]
    openai_time = openai_results["aggregated"]["avg_query_time"]
    speed_ratio = openai_time / doc2vec_time

    print(f"\n‚úÖ Doc2Vec —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ {speed_ratio:.1f} —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ OpenAI")
    print(f"   Doc2Vec: {doc2vec_time:.3f}—Å –Ω–∞ –∑–∞–ø—Ä–æ—Å")
    print(f"   OpenAI:  {openai_time:.3f}—Å –Ω–∞ –∑–∞–ø—Ä–æ—Å")

    # –≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
    print("\nüí∞ –≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å:")
    yearly_cost = 1000 * 50 / 1000 * 0.0001 * 365  # –ü—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞—Å—á–µ—Ç
    print(f"   –ü—Ä–∏ 1000 –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –¥–µ–Ω—å —ç–∫–æ–Ω–æ–º–∏—è —Å–æ—Å—Ç–∞–≤–∏—Ç ~${yearly_cost:.0f} –≤ –≥–æ–¥")

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
    print("\nüìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤...")
    try:
        comparison.plot_comparison(save_plots=True)
        print("‚úÖ –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ data/evaluation_results/plots/")
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏: {e}")

    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    print("\nüìÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞...")
    report_path = Path("data/evaluation_results/comparison_report.txt")
    report = comparison.generate_report(report_path)

    print("\n‚úÖ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: data/evaluation_results/")
    print("\n" + "=" * 80)


if __name__ == "__main__":
    main()


========================================
FILE: scripts\export_project.py
========================================
#!/usr/bin/env python
"""–°–∫—Ä–∏–ø—Ç –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤—Å–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ –≤ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏"""

import os
from datetime import datetime
from pathlib import Path


def should_include_file(file_path: Path, output_file: str) -> bool:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –≤–∫–ª—é—á–∞—Ç—å —Ñ–∞–π–ª"""

    # –í–ê–ñ–ù–û: –ò—Å–∫–ª—é—á–∞–µ–º —Å–∞–º —Ñ–∞–π–ª —ç–∫—Å–ø–æ—Ä—Ç–∞!
    if file_path.name == output_file or file_path.name == "project_export.txt":
        return False

    # –ò—Å–∫–ª—é—á–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —ç–∫—Å–ø–æ—Ä—Ç–∞
    if file_path.name.startswith("project_export") and file_path.suffix == ".txt":
        return False

    # –ò—Å–∫–ª—é—á–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    exclude_dirs = {
        "__pycache__",
        ".git",
        ".venv",
        "venv",
        ".env",
        "dist",
        "build",
        ".pytest_cache",
        ".mypy_cache",
        "node_modules",
        ".idea",
        ".vscode",
        "logs",
        "data/models",
        "data/cache",
        "data/temp",
        ".ruff_cache",
        "htmlcov",
        ".coverage",
    }

    # –ò—Å–∫–ª—é—á–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
    exclude_extensions = {
        ".pyc",
        ".pyo",
        ".pyd",
        ".so",
        ".dll",
        ".dylib",
        ".exe",
        ".bin",
        ".pkl",
        ".npy",
        ".model",
        ".log",
        ".tmp",
        ".cache",
        ".lock",
        ".db",
        ".zip",
        ".tar",
        ".gz",
        ".rar",
        ".7z",
    }

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ —Ñ–∞–π–ª –≤ –∏—Å–∫–ª—é—á–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    for parent in file_path.parents:
        if parent.name in exclude_dirs:
            return False

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
    if file_path.suffix in exclude_extensions:
        return False

    # –ò—Å–∫–ª—é—á–∞–µ–º –±–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã (–±–æ–ª—å—à–µ 1MB)
    try:
        if file_path.is_file() and file_path.stat().st_size > 1_000_000:
            return False
    except:
        return False

    return True


def collect_project_files(root_path: Path, output_file: str) -> list[Path]:
    """–°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã –ø—Ä–æ–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –≤–∫–ª—é—á–∏—Ç—å"""
    included_files = []
    seen_paths = set()  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ñ–∞–π–ª–æ–≤
    priority_order = [
        "pyproject.toml",
        "README.md",
        "LICENSE",
        ".gitignore",
        "src/semantic_search/__init__.py",
        "src/semantic_search/config.py",
        "src/semantic_search/main.py",
        "src/semantic_search/core",
        "src/semantic_search/gui",
        "src/semantic_search/utils",
        "src/semantic_search/evaluation",
        "scripts",
        "tests",
    ]

    # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ö–æ–¥–∏–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    for path in root_path.rglob("*"):
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        abs_path = path.absolute()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –¥–æ–±–∞–≤–ª—è–ª–∏ –ª–∏ —É–∂–µ —ç—Ç–æ—Ç —Ñ–∞–π–ª
        if abs_path in seen_paths:
            continue

        if path.is_file() and should_include_file(path, output_file):
            included_files.append(path)
            seen_paths.add(abs_path)

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
    def get_sort_key(file_path: Path):
        rel_path = str(file_path.relative_to(root_path)).replace("\\", "/")

        # –ò—â–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        for i, priority in enumerate(priority_order):
            if rel_path == priority or rel_path.startswith(priority + "/"):
                return (i, rel_path)

        # –û—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ –∫–æ–Ω–µ—Ü
        return (len(priority_order), rel_path)

    included_files.sort(key=get_sort_key)

    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
    unique_files = []
    seen_contents = set()

    for file_path in included_files:
        file_key = (file_path.name, file_path.stat().st_size)
        if file_key not in seen_contents:
            unique_files.append(file_path)
            seen_contents.add(file_key)

    return unique_files


def export_project(output_file: str = "project_export.txt", include_data: bool = False):
    """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ–µ–∫—Ç –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª"""
    project_root = Path(__file__).parent.parent

    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π —Ñ–∞–π–ª —ç–∫—Å–ø–æ—Ä—Ç–∞ –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    output_path = Path(output_file)
    if output_path.exists():
        print(f"üóëÔ∏è  –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π —Ñ–∞–π–ª —ç–∫—Å–ø–æ—Ä—Ç–∞: {output_file}")
        output_path.unlink()

    # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∞–π–ª—ã
    print("üìÇ –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞...")
    all_files = collect_project_files(project_root, output_file)

    # –§–∏–ª—å—Ç—Ä—É–µ–º data —Ñ–∞–π–ª—ã –µ—Å–ª–∏ –Ω–µ –Ω—É–∂–Ω—ã
    if not include_data:
        all_files = [
            f for f in all_files if "data" not in f.parts or f.name == ".gitkeep"
        ]

    print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞: {len(all_files)}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
    file_names = [f.name for f in all_files]
    duplicates = [name for name in file_names if file_names.count(name) > 1]
    if duplicates:
        print(f"‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ñ–∞–π–ª—ã —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ –∏–º–µ–Ω–∞–º–∏: {set(duplicates)}")

    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ —Ñ–∞–π–ª
    with open(output_file, "w", encoding="utf-8") as f:
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        f.write("=" * 80 + "\n")
        f.write("SEMANTIC SEARCH PROJECT EXPORT\n")
        f.write(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Root: {project_root}\n")
        f.write("=" * 80 + "\n\n")

        # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ (–ø—Ä–æ—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫)
        f.write("PROJECT STRUCTURE:\n")
        f.write("-" * 40 + "\n")

        current_dir = None
        for file_path in all_files:
            rel_path = file_path.relative_to(project_root)
            parent_dir = rel_path.parent

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–∏ —Å–º–µ–Ω–µ
            if parent_dir != current_dir:
                current_dir = parent_dir
                if str(parent_dir) != ".":
                    f.write(f"\n{parent_dir}/\n")
                else:
                    f.write("\n[root]\n")

            f.write(f"  - {file_path.name}\n")

        # –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–æ–≤
        f.write("\n" + "=" * 80 + "\n")
        f.write("FILE CONTENTS:\n")
        f.write("=" * 80 + "\n")

        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
        written_files = set()  # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –∑–∞–ø–∏—Å–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã

        for file_path in all_files:
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á –¥–ª—è —Ñ–∞–π–ª–∞
            file_key = str(file_path.absolute())

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∑–∞–ø–∏—Å–∞–ª–∏ –ª–∏ —É–∂–µ
            if file_key in written_files:
                print(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç: {file_path.relative_to(project_root)}")
                continue

            written_files.add(file_key)
            rel_path = file_path.relative_to(project_root)

            f.write(f"\n\n{'=' * 40}\n")
            f.write(f"FILE: {rel_path}\n")
            f.write(f"{'=' * 40}\n")

            try:
                # –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã
                if file_path.suffix in [
                    ".py",
                    ".toml",
                    ".md",
                    ".txt",
                    ".json",
                    ".yaml",
                    ".yml",
                    ".bat",
                    ".sh",
                    ".cfg",
                    ".ini",
                ]:
                    content = file_path.read_text(encoding="utf-8", errors="ignore")
                    f.write(content)
                    if not content.endswith("\n"):
                        f.write("\n")
                else:
                    # –ë–∏–Ω–∞—Ä–Ω—ã–µ —Ñ–∞–π–ª—ã
                    f.write(f"[Binary file - {file_path.stat().st_size} bytes]\n")

            except Exception as e:
                f.write(f"[Error reading file: {e}]\n")

        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        f.write("\n" + "=" * 80 + "\n")
        f.write("EXPORT SUMMARY:\n")
        f.write("-" * 40 + "\n")
        f.write(f"Total files included: {len(written_files)}\n")
        f.write(f"Export file: {output_file}\n")
        f.write(f"Export size: {os.path.getsize(output_file):,} bytes\n")
        f.write("=" * 80 + "\n")

    # –†–µ–∑—É–ª—å—Ç–∞—Ç
    file_size_mb = os.path.getsize(output_file) / 1024 / 1024
    print(f"‚úÖ –ü—Ä–æ–µ–∫—Ç —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≤: {output_file}")
    print(f"üìä –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size_mb:.2f} MB")
    print(f"üìÅ –í–∫–ª—é—á–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(written_files)}")

    if file_size_mb > 10:
        print("\n‚ö†Ô∏è  –§–∞–π–ª –±–æ–ª—å—à–µ 10 MB, –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–º –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —á–∞—Ç–æ–≤")
        print("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–∫–ª—é—á–∏—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ñ–∞–π–ª—ã –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞—Ä—Ö–∏–≤")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="–≠–∫—Å–ø–æ—Ä—Ç –ø—Ä–æ–µ–∫—Ç–∞ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏"
    )
    parser.add_argument(
        "-o",
        "--output",
        default="project_export.txt",
        help="–ò–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: project_export.txt)",
    )
    parser.add_argument(
        "--include-data", action="store_true", help="–í–∫–ª—é—á–∏—Ç—å —Ñ–∞–π–ª—ã –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ data/"
    )

    args = parser.parse_args()

    export_project(args.output, args.include_data)


========================================
FILE: scripts\inspect_corpus.py
========================================
"""scripts/inspect_corpus.py - –°–∫—Ä–∏–ø—Ç –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –º–æ–¥–µ–ª–∏"""

import sys
from pathlib import Path

from loguru import logger

from semantic_search.core.doc2vec_trainer import Doc2VecTrainer


def inspect_corpus(model_name: str = "doc2vec_model"):
    """–ü–æ–∫–∞–∑–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –≤ –º–æ–¥–µ–ª–∏"""

    trainer = Doc2VecTrainer()
    model = trainer.load_model(model_name)

    if not model:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å {model_name}")
        return

    if not trainer.corpus_info:
        logger.error("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ—Ä–ø—É—Å–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return

    print(f"\nüìö –î–û–ö–£–ú–ï–ù–¢–´ –í –ú–û–î–ï–õ–ò '{model_name}':")
    print("=" * 80)
    print(f"–í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(trainer.corpus_info)}\n")

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º
    by_extension = {}
    for tokens, doc_id, metadata in trainer.corpus_info:
        ext = Path(doc_id).suffix
        if ext not in by_extension:
            by_extension[ext] = []
        by_extension[ext].append(doc_id)

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–∏–ø–∞–º
    print("üìä –ü–æ —Ç–∏–ø–∞–º —Ñ–∞–π–ª–æ–≤:")
    for ext, docs in sorted(by_extension.items()):
        print(f"  {ext}: {len(docs)} —Ñ–∞–π–ª–æ–≤")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    print("\nüìÑ –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:")
    for i, (tokens, doc_id, metadata) in enumerate(trainer.corpus_info, 1):
        tokens_count = metadata.get("tokens_count", len(tokens))
        print(f"{i:3d}. {doc_id:<50} ({tokens_count} —Ç–æ–∫–µ–Ω–æ–≤)")

    # –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ—Å—Ç–æ–≤
    print("\nüí° –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤:")
    print("relevant_docs = {")
    for i, (_, doc_id, _) in enumerate(trainer.corpus_info[:5]):
        print(f'    "{doc_id}",')
    print("}")

    print("\n‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç—Ç–∏ –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤ –≤ QueryTestCase!")


if __name__ == "__main__":
    import sys

    model_name = sys.argv[1] if len(sys.argv) > 1 else "doc2vec_model"
    inspect_corpus(model_name)


========================================
FILE: scripts\list_models.py
========================================
#!/usr/bin/env python
"""–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""

import json
import sys
from datetime import datetime
from pathlib import Path

from semantic_search.config import MODELS_DIR
from semantic_search.core.doc2vec_trainer import Doc2VecTrainer

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src –≤ sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))


def format_file_size(size_bytes):
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞"""
    for unit in ["B", "KB", "MB", "GB"]:
        if size_bytes < 1024.0:
            return f"{size_bytes:.1f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.1f} TB"


def list_models():
    """–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    print("üìö –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ Doc2Vec")
    print("=" * 80)

    if not MODELS_DIR.exists():
        print("‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –º–æ–¥–µ–ª–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
        return

    model_files = list(MODELS_DIR.glob("*.model"))

    if not model_files:
        print("‚ÑπÔ∏è  –ú–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        print(f"   –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –º–æ–¥–µ–ª–µ–π: {MODELS_DIR}")
        print("\nüí° –°–æ–∑–¥–∞–π—Ç–µ –º–æ–¥–µ–ª—å –∫–æ–º–∞–Ω–¥–æ–π:")
        print("   poetry run semantic-search-cli train -d /path/to/documents")
        return

    print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –º–æ–¥–µ–ª–µ–π: {MODELS_DIR}")
    print(f"üî¢ –ù–∞–π–¥–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(model_files)}\n")

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–µ)
    model_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)

    for i, model_file in enumerate(model_files, 1):
        model_name = model_file.stem
        file_size = model_file.stat().st_size
        modified_time = datetime.fromtimestamp(model_file.stat().st_mtime)

        print(f"{i}. üß† {model_name}")
        print(f"   üìè –†–∞–∑–º–µ—Ä: {format_file_size(file_size)}")
        print(f"   üìÖ –ò–∑–º–µ–Ω–µ–Ω: {modified_time.strftime('%Y-%m-%d %H:%M:%S')}")

        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata_file = MODELS_DIR / f"{model_name}_metadata.json"
        if metadata_file.exists():
            try:
                with open(metadata_file, "r", encoding="utf-8") as f:
                    metadata = json.load(f)

                if "corpus_size" in metadata:
                    print(f"   üìÑ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {metadata['corpus_size']}")
                if "vector_size" in metadata:
                    print(f"   üéØ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {metadata['vector_size']}")
                if "epochs" in metadata:
                    print(f"   üîÑ –≠–ø–æ—Ö: {metadata['epochs']}")
                if "training_time_formatted" in metadata:
                    print(
                        f"   ‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {metadata['training_time_formatted']}"
                    )
                if "documents_base_path" in metadata:
                    base_path = Path(metadata["documents_base_path"])
                    print(f"   üìÇ –ë–∞–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {base_path.name}")

            except Exception as e:
                print(f"   ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {e}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ corpus_info
        corpus_info_file = MODELS_DIR / f"{model_name}_corpus_info.pkl"
        if corpus_info_file.exists():
            corpus_size = format_file_size(corpus_info_file.stat().st_size)
            print(f"   üíæ Corpus info: {corpus_size}")

        print()  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏


def show_model_details(model_name: str):
    """–ü–æ–∫–∞–∑–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏"""
    print(f"\nüìã –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏: {model_name}")
    print("=" * 80)

    trainer = Doc2VecTrainer()
    model = trainer.load_model(model_name)

    if model is None:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å '{model_name}'")
        return

    info = trainer.get_model_info()

    print(f"‚úÖ –°—Ç–∞—Ç—É—Å: {info['status']}")
    print(f"üìè –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤: {info['vector_size']}")
    print(f"üìö –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {info['vocabulary_size']:,} —Å–ª–æ–≤")
    print(f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –º–æ–¥–µ–ª–∏: {info['documents_count']}")
    print(f"üîç –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞: {info['window']}")
    print(f"üìä –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞: {info['min_count']}")
    print(f"üîÑ –≠–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è: {info['epochs']}")

    if info["dm"] == 1:
        print("üîß –†–µ–∂–∏–º: Distributed Memory (DM)")
    else:
        print("üîß –†–µ–∂–∏–º: Distributed Bag of Words (DBOW)")

    print(f"‚ûñ Negative sampling: {info['negative']}")
    print(f"üå≥ Hierarchical Softmax: {'–î–∞' if info['hs'] == 1 else '–ù–µ—Ç'}")
    print(f"üìâ Sample threshold: {info['sample']}")

    if "training_time_formatted" in info:
        print(f"\n‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {info['training_time_formatted']}")
    if "training_date" in info:
        print(f"üìÖ –î–∞—Ç–∞ –æ–±—É—á–µ–Ω–∏—è: {info['training_date']}")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    if trainer.corpus_info:
        print("\nüìë –ü—Ä–∏–º–µ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –º–æ–¥–µ–ª–∏:")
        for i, (tokens, doc_id, metadata) in enumerate(trainer.corpus_info[:5]):
            print(f"   {i + 1}. {doc_id}")
            if "tokens_count" in metadata:
                print(f"      –¢–æ–∫–µ–Ω–æ–≤: {metadata['tokens_count']}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse

    parser = argparse.ArgumentParser(description="–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—è–º–∏ Doc2Vec")
    parser.add_argument(
        "--details", "-d", help="–ü–æ–∫–∞–∑–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏"
    )

    args = parser.parse_args()

    if args.details:
        show_model_details(args.details)
    else:
        list_models()


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  –û–ø–µ—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞.")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        sys.exit(1)


========================================
FILE: scripts\print_project_tree.py
========================================
import fnmatch
import os
from pathlib import Path


def print_tree(start_path, prefix="", ignore=None, leaf_dirs=None):
    if ignore is None:
        ignore = set()
    if leaf_dirs is None:
        leaf_dirs = {"cache", "models", "logs"}

    # –°–æ–±–∏—Ä–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å —É—á–µ—Ç–æ–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è
    contents = []
    for item in os.listdir(start_path):
        skip = False
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —à–∞–±–ª–æ–Ω—ã –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è
        for pattern in ignore:
            if fnmatch.fnmatch(item, pattern):
                skip = True
                break
        if skip:
            continue
        contents.append(item)

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    contents.sort()

    for i, item in enumerate(contents):
        path = Path(start_path) / item
        is_last = i == len(contents) - 1

        # –î–æ–±–∞–≤–ª—è–µ–º '/' –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è–º
        display_name = f"{item}/" if os.path.isdir(path) else item
        print(f"{prefix}{'‚îî‚îÄ‚îÄ ' if is_last else '‚îú‚îÄ‚îÄ '}{display_name}")

        # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –æ–±—Ö–æ–¥ —Ç–æ–ª—å–∫–æ –¥–ª—è –ù–ï–ª–∏—Å—Ç–æ–≤—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        if os.path.isdir(path) and item not in leaf_dirs:
            ext = "    " if is_last else "‚îÇ   "
            print_tree(path, prefix + ext, ignore, leaf_dirs)


if __name__ == "__main__":
    print(".")
    ignore_set = {
        "__pycache__",
        ".git",
        ".venv",
        "*.pyc",
        ".pytest_cache",
        "tree.py",
        ".benchmarks",
        ".coverage",
        "*__pycache__*",
        "poetry.lock",
        "__init__.py",
        ".gitignore",
    }
    print_tree(".", ignore=ignore_set)


========================================
FILE: scripts\run_semantic_search.bat
========================================
@echo off
REM –ë–∞—Ç–Ω–∏–∫ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ Semantic Search –Ω–∞ Windows

echo ====================================
echo    Semantic Search Launcher
echo ====================================
echo.

REM –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ Poetry
where poetry >nul 2>nul
if %errorlevel% neq 0 (
    echo [ERROR] Poetry –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ PATH!
    echo.
    echo –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Poetry: https://python-poetry.org/docs/#installation
    echo.
    pause
    exit /b 1
)

REM –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è
if not exist ".venv" (
    echo [INFO] –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è...
    poetry install
    if %errorlevel% neq 0 (
        echo [ERROR] –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π!
        pause
        exit /b 1
    )
)

REM –ú–µ–Ω—é –≤—ã–±–æ—Ä–∞
:menu
echo –í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º –∑–∞–ø—É—Å–∫–∞:
echo.
echo 1. GUI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
echo 2. CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
echo 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
echo 4. –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
echo 5. –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞
echo 6. –í—ã—Ö–æ–¥
echo.

set /p choice="–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä (1-6): "

if "%choice%"=="1" goto gui
if "%choice%"=="2" goto cli
if "%choice%"=="3" goto check
if "%choice%"=="4" goto models
if "%choice%"=="5" goto cache
if "%choice%"=="6" goto end

echo –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä!
echo.
goto menu

:gui
echo.
echo –ó–∞–ø—É—Å–∫ GUI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è...
poetry run semantic-search
goto end

:cli
echo.
echo –ó–∞–ø—É—Å–∫ CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞...
echo.
poetry run semantic-search-cli --help
echo.
pause
goto menu

:check
echo.
echo –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π...
echo.
poetry run python scripts/check_dependencies.py
echo.
pause
goto menu

:models
echo.
echo –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π...
echo.
poetry run python scripts/list_models.py
echo.
pause
goto menu

:cache
echo.
echo –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞...
echo.
poetry run python scripts/cache_clear.py
echo.
pause
goto menu

:end
echo.
echo –°–ø–∞—Å–∏–±–æ –∑–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Semantic Search!
timeout /t 3 >nul


========================================
FILE: scripts\run_semantic_search.sh
========================================
#!/bin/bash
# –°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ Semantic Search –Ω–∞ Linux/Mac

echo "===================================="
echo "    Semantic Search Launcher"
echo "===================================="
echo

# –¶–≤–µ—Ç–∞ –¥–ª—è –≤—ã–≤–æ–¥–∞
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ Poetry
if ! command -v poetry &> /dev/null; then
    echo -e "${RED}[ERROR]${NC} Poetry –Ω–µ –Ω–∞–π–¥–µ–Ω!"
    echo
    echo "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Poetry: https://python-poetry.org/docs/#installation"
    echo
    exit 1
fi

# –ü—Ä–æ–≤–µ—Ä—è–µ–º/—Å–æ–∑–¥–∞–µ–º –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ
if [ ! -d ".venv" ]; then
    echo -e "${YELLOW}[INFO]${NC} –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è..."
    poetry install
    if [ $? -ne 0 ]; then
        echo -e "${RED}[ERROR]${NC} –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π!"
        exit 1
    fi
fi

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∫–∞–∑–∞ –º–µ–Ω—é
show_menu() {
    echo
    echo "–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º –∑–∞–ø—É—Å–∫–∞:"
    echo
    echo "1. GUI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ"
    echo "2. CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å"
    echo "3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"
    echo "4. –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π"
    echo "5. –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞"
    echo "6. –í—ã—Ö–æ–¥"
    echo
}

# –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª
while true; do
    show_menu
    read -p "–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä (1-6): " choice
    
    case $choice in
        1)
            echo
            echo -e "${GREEN}–ó–∞–ø—É—Å–∫ GUI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è...${NC}"
            poetry run semantic-search
            break
            ;;
        2)
            echo
            echo -e "${GREEN}–ó–∞–ø—É—Å–∫ CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞...${NC}"
            echo
            poetry run semantic-search-cli --help
            echo
            read -p "–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è..."
            ;;
        3)
            echo
            echo -e "${GREEN}–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π...${NC}"
            echo
            poetry run python scripts/check_dependencies.py
            echo
            read -p "–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è..."
            ;;
        4)
            echo
            echo -e "${GREEN}–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π...${NC}"
            echo
            poetry run python scripts/list_models.py
            echo
            read -p "–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è..."
            ;;
        5)
            echo
            echo -e "${GREEN}–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞...${NC}"
            echo
            poetry run python scripts/cache_clear.py
            echo
            read -p "–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è..."
            ;;
        6)
            echo
            echo -e "${GREEN}–°–ø–∞—Å–∏–±–æ –∑–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Semantic Search!${NC}"
            exit 0
            ;;
        *)
            echo -e "${RED}–ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä!${NC}"
            ;;
    esac
done


========================================
FILE: scripts\setup_spacy.py
========================================
# –û–±–Ω–æ–≤–∏—Ç–µ —Ñ–∞–π–ª scripts/setup_spacy.py:

"""–°–∫—Ä–∏–ø—Ç –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∏ SpaCy –º–æ–¥–µ–ª–µ–π"""

import subprocess
import sys

import click
import spacy
from loguru import logger


def check_spacy_model(model_name: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –º–æ–¥–µ–ª–∏ SpaCy"""
    try:
        spacy.load(model_name)
        return True
    except OSError:
        return False


def download_spacy_model(model_name: str) -> bool:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ SpaCy"""
    try:
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")
        subprocess.check_call(
            [sys.executable, "-m", "spacy", "download", model_name],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
        )
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        return False


@click.command()
@click.option(
    "--russian/--no-russian",
    default=True,
    help="–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ä—É—Å—Å–∫—É—é –º–æ–¥–µ–ª—å",
)
@click.option(
    "--english/--no-english",
    default=True,
    help="–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∞–Ω–≥–ª–∏–π—Å–∫—É—é –º–æ–¥–µ–ª—å",
)
def main(russian: bool, english: bool):
    """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ SpaCy –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤"""

    logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ SpaCy...")

    try:
        import spacy

        logger.info(f"SpaCy –≤–µ—Ä—Å–∏–∏ {spacy.__version__} —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    except ImportError:
        logger.error("SpaCy –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
        logger.info("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ SpaCy –∫–æ–º–∞–Ω–¥–æ–π: pip install spacy")
        sys.exit(1)

    models_to_install = []

    if russian:
        models_to_install.append(("ru_core_news_sm", "–†—É—Å—Å–∫–∞—è"))
    if english:
        models_to_install.append(("en_core_web_sm", "–ê–Ω–≥–ª–∏–π—Å–∫–∞—è"))

    if not models_to_install:
        logger.warning("–ù–µ –≤—ã–±—Ä–∞–Ω–∞ –Ω–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏")
        return

    installed_count = 0

    for model_name, model_desc in models_to_install:
        logger.info(f"\n{'=' * 50}")
        logger.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ {model_desc} –º–æ–¥–µ–ª–∏ ({model_name})...")

        if check_spacy_model(model_name):
            logger.success(f"{model_desc} –º–æ–¥–µ–ª—å —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞!")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–±–æ—Ç—É –º–æ–¥–µ–ª–∏
            try:
                nlp = spacy.load(model_name)
                test_text = (
                    "–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ."
                    if "ru" in model_name
                    else "This is a test sentence."
                )
                doc = nlp(test_text)
                logger.info(f"–ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ. –¢–æ–∫–µ–Ω–æ–≤: {len(doc)}")
                logger.info(f"–Ø–∑—ã–∫: {nlp.lang}")
                logger.info(f"–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã: {nlp.pipe_names}")
                installed_count += 1

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")

        else:
            logger.warning(f"{model_desc} –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

            response = click.confirm(
                f"–•–æ—Ç–∏—Ç–µ –∑–∞–≥—Ä—É–∑–∏—Ç—å {model_desc} –º–æ–¥–µ–ª—å?", default=True
            )
            if response:
                if download_spacy_model(model_name):
                    logger.success(f"{model_desc} –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
                    if check_spacy_model(model_name):
                        logger.success("–ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
                        installed_count += 1
                    else:
                        logger.error("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –Ω–æ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                else:
                    logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å")
                    logger.info(
                        f"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤—Ä—É—á–Ω—É—é: python -m spacy download {model_name}"
                    )
            else:
                logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞")

    # –ò—Ç–æ–≥–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    logger.info(f"\n{'=' * 50}")
    logger.info("–ò–¢–û–ì–ò –£–°–¢–ê–ù–û–í–ö–ò:")
    logger.info(f"–£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {installed_count} –∏–∑ {len(models_to_install)}")

    if installed_count == len(models_to_install):
        logger.success("‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã!")
    elif installed_count > 0:
        logger.warning("‚ö†Ô∏è –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –Ω–µ –≤—Å–µ –º–æ–¥–µ–ª–∏")
    else:
        logger.error("‚ùå –ù–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    logger.info("\nüìö –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ SpaCy:")
    logger.info("–†—É—Å—Å–∫–∏–µ:")
    logger.info("  - ru_core_news_sm (–º–∞–ª–µ–Ω—å–∫–∞—è, ~15 –ú–ë)")
    logger.info("  - ru_core_news_md (—Å—Ä–µ–¥–Ω—è—è, ~45 –ú–ë)")
    logger.info("  - ru_core_news_lg (–±–æ–ª—å—à–∞—è, ~500 –ú–ë)")
    logger.info("\n–ê–Ω–≥–ª–∏–π—Å–∫–∏–µ:")
    logger.info("  - en_core_web_sm (–º–∞–ª–µ–Ω—å–∫–∞—è, ~15 –ú–ë)")
    logger.info("  - en_core_web_md (—Å—Ä–µ–¥–Ω—è—è, ~40 –ú–ë)")
    logger.info("  - en_core_web_lg (–±–æ–ª—å—à–∞—è, ~400 –ú–ë)")
    logger.info("  - en_core_web_trf (transformer, ~450 –ú–ë)")


if __name__ == "__main__":
    main()


========================================
FILE: tests\__init__.py
========================================



========================================
FILE: tests\test_core_functionality.py
========================================
"""–¢–µ—Å—Ç—ã –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)"""

import tempfile
from pathlib import Path
from unittest.mock import Mock, patch

import pytest

from semantic_search.core.doc2vec_trainer import Doc2VecTrainer
from semantic_search.core.document_processor import DocumentProcessor
from semantic_search.core.search_engine import SemanticSearchEngine
from semantic_search.utils.validators import DataValidator, ValidationError


class TestDocumentProcessor:
    """–¢–µ—Å—Ç—ã –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""

    def test_processor_initialization(self):
        """–¢–µ—Å—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞"""
        processor = DocumentProcessor()
        assert processor is not None
        assert processor.file_extractor is not None
        assert processor.text_processor is not None

    def test_empty_directory_processing(self):
        """–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—É—Å—Ç–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
        with tempfile.TemporaryDirectory() as temp_dir:
            processor = DocumentProcessor()
            docs = list(processor.process_documents(Path(temp_dir)))
            assert len(docs) == 0

    @patch("semantic_search.core.document_processor.FileExtractor")
    def test_document_processing_with_mock(self, mock_extractor):
        """–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –º–æ–∫–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–∫–∞
        mock_instance = Mock()
        mock_extractor.return_value = mock_instance

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –≤–º–µ—Å—Ç–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ–≥–æ
        test_path = Path("test_documents")
        mock_instance.find_documents.return_value = [test_path / "test_doc.pdf"]
        mock_instance.extract_text.return_value = "Test document content " * 20

        processor = DocumentProcessor()
        processor.file_extractor = mock_instance

        docs = list(processor.process_documents(test_path))
        assert len(docs) > 0
        assert docs[0].relative_path == "test_doc.pdf"


class TestValidators:
    """–¢–µ—Å—Ç—ã –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–æ–≤"""

    def test_text_validation_success(self):
        """–¢–µ—Å—Ç —É—Å–ø–µ—à–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"""
        text = DataValidator.validate_text("Hello world", min_length=5)
        assert text == "Hello world"

    def test_text_validation_failure(self):
        """–¢–µ—Å—Ç –Ω–µ—É–¥–∞—á–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"""
        with pytest.raises(ValidationError):
            DataValidator.validate_text("Hi", min_length=10)

    def test_search_params_validation(self):
        """–¢–µ—Å—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–∏—Å–∫–∞"""
        params = DataValidator.validate_search_params(
            query="test query", top_k=10, similarity_threshold=0.5
        )

        assert params["query"] == "test query"
        assert params["top_k"] == 10
        assert params["similarity_threshold"] == 0.5

    def test_model_params_validation(self):
        """–¢–µ—Å—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏"""
        params = DataValidator.validate_model_params(vector_size=100, epochs=20)

        assert params["vector_size"] == 100
        assert params["epochs"] == 20


@pytest.fixture
def sample_corpus():
    """–§–∏–∫—Å—Ç—É—Ä–∞ —Å –æ–±—Ä–∞–∑—Ü–æ–º –∫–æ—Ä–ø—É—Å–∞"""
    return [
        (["–º–∞—à–∏–Ω–Ω–æ–µ", "–æ–±—É—á–µ–Ω–∏–µ", "–∞–ª–≥–æ—Ä–∏—Ç–º"], "doc1.pdf", {"tokens_count": 3}),
        (
            ["–Ω–µ–π—Ä–æ–Ω–Ω–∞—è", "—Å–µ—Ç—å", "–≥–ª—É–±–æ–∫–æ–µ", "–æ–±—É—á–µ–Ω–∏–µ"],
            "doc2.pdf",
            {"tokens_count": 4},
        ),
        (["–∞–Ω–∞–ª–∏–∑", "–¥–∞–Ω–Ω—ã—Ö", "—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"], "doc3.pdf", {"tokens_count": 3}),
    ]


class TestDoc2VecTrainer:
    """–¢–µ—Å—Ç—ã —Ç—Ä–µ–Ω–µ—Ä–∞ Doc2Vec"""

    def test_trainer_initialization(self):
        """–¢–µ—Å—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç—Ä–µ–Ω–µ—Ä–∞"""
        trainer = Doc2VecTrainer()
        assert trainer is not None
        assert trainer.model is None

    def test_tagged_documents_creation(self, sample_corpus):
        """–¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è TaggedDocument –æ–±—ä–µ–∫—Ç–æ–≤"""
        trainer = Doc2VecTrainer()

        if not hasattr(trainer, "create_tagged_documents"):
            pytest.skip("Gensim –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")

        tagged_docs = trainer.create_tagged_documents(sample_corpus)
        assert len(tagged_docs) == 3
        assert tagged_docs[0].tags == ["doc1.pdf"]


class TestSearchEngine:
    """–¢–µ—Å—Ç—ã –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞"""

    def test_search_engine_initialization(self):
        """–¢–µ—Å—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞"""
        engine = SemanticSearchEngine()
        assert engine is not None
        assert engine.model is None

    def test_empty_query_handling(self):
        """–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—É—Å—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        engine = SemanticSearchEngine()
        results = engine.search("")
        assert len(results) == 0

    def test_search_without_model(self):
        """–¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞ –±–µ–∑ –º–æ–¥–µ–ª–∏"""
        engine = SemanticSearchEngine()
        results = engine.search("test query")
        assert len(results) == 0


# –ë–µ–Ω—á–º–∞—Ä–∫ —Ç–µ—Å—Ç—ã
class TestPerformance:
    """–¢–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""

    def test_text_processing_speed(self, benchmark):
        """–ë–µ–Ω—á–º–∞—Ä–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞"""
        from semantic_search.utils.text_utils import TextProcessor

        processor = TextProcessor()
        test_text = "–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏. " * 100

        result = benchmark(processor.preprocess_text, test_text)
        assert isinstance(result, list)
        assert len(result) > 0


========================================
FILE: config\app_config.json
========================================
{
  "text_processing": {
    "min_text_length": 100,
    "max_text_length": 5000000,
    "min_tokens_count": 10,
    "min_token_length": 2,
    "min_sentence_length": 10,
    "remove_stop_words": true,
    "lemmatize": true,
    "max_file_size_mb": 100,
    "chunk_size": 800000,
    "spacy_max_length": 3000000
  },
  "doc2vec": {
    "vector_size": 150,
    "window": 10,
    "min_count": 2,
    "epochs": 40,
    "workers": 15,
    "seed": 42,
    "dm": 1,
    "negative": 5,
    "hs": 0,
    "sample": 0.0001
  },
  "search": {
    "default_top_k": 10,
    "max_top_k": 100,
    "similarity_threshold": 0.1,
    "enable_caching": true,
    "cache_size": 1000,
    "enable_filtering": true
  },
  "gui": {
    "window_title": "Semantic Document Search",
    "window_size": [
      1400,
      900
    ],
    "theme": "default",
    "font_size": 10,
    "enable_dark_theme": false,
    "auto_save_settings": true
  },
  "summarization": {
    "default_sentences_count": 5,
    "max_sentences_count": 20,
    "min_sentence_length": 15,
    "use_textrank": true,
    "damping_factor": 0.85,
    "max_iterations": 100
  },
  "performance": {
    "enable_monitoring": true,
    "log_slow_operations": true,
    "slow_operation_threshold": 5.0,
    "memory_warning_threshold": 80,
    "enable_profiling": false
  }
}

================================================================================
EXPORT SUMMARY:
----------------------------------------
Total files included: 44
Export file: project_export.txt
Export size: 422,671 bytes
================================================================================
