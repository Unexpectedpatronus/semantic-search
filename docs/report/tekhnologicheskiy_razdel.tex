\chapter{ТЕХНОЛОГИЧЕСКИЙ РАЗДЕЛ}

\section{Выбор технологического стека}

\justifying

Для реализации системы семантического поиска был выбран следующий технологический стек:

\textbf{Язык программирования: Python 3.10+}

Python выбран как основной язык разработки по следующим причинам:
\begin{itemize}
	\item Богатая экосистема библиотек для машинного обучения и обработки текстов
	\item Высокая скорость разработки
	\item Отличная поддержка научных вычислений
	\item Кроссплатформенность
\end{itemize}

\textbf{Основные библиотеки:}

\begin{itemize}
	\item \textbf{Gensim} – реализация алгоритма Doc2Vec, оптимизированная для работы с большими корпусами
	\item \textbf{SpaCy} – промышленная библиотека для обработки естественного языка
	\item \textbf{PyQt6} – фреймворк для создания графического интерфейса
	\item \textbf{PyMuPDF} – высокопроизводительная библиотека для работы с PDF
	\item \textbf{python-docx} – работа с форматом DOCX
	\item \textbf{scikit-learn} – дополнительные алгоритмы машинного обучения
	\item \textbf{Click} – создание интерфейса командной строки
	\item \textbf{Loguru} – продвинутая система логирования
\end{itemize}

\textbf{Инструменты разработки:}

\begin{itemize}
	\item \textbf{Poetry} – управление зависимостями и виртуальными окружениями
	\item \textbf{pytest} – фреймворк для тестирования
	\item \textbf{Ruff} – линтер и форматтер кода
	\item \textbf{mypy} – статическая типизация
\end{itemize}

\section{Аппаратное и программное обеспечение}

\textbf{Характеристики рабочей станции разработки:}

Разработка и тестирование системы проводились на следующей конфигурации:
\begin{itemize}
	\item \textbf{Процессор:} 13th Gen Intel(R) Core(TM) i7-1360P, 2.20 GHz (12 ядер, 16 потоков)
	\item \textbf{Оперативная память:} 32.0 GB DDR5 (31.7 GB доступно)
	\item \textbf{Операционная система:} Windows 11 Pro, версия 24H2
	\item \textbf{Архитектура:} 64-разрядная система, процессор x64
	\item \textbf{Хранилище:} NVMe SSD 1TB для быстрой работы с большими корпусами документов
\end{itemize}

Данная конфигурация обеспечивает:
\begin{itemize}
	\item Эффективную многопоточную обработку документов (до 16 параллельных потоков)
	\item Достаточный объем памяти для обучения моделей на корпусах до 50,000 документов
	\item Быстрый доступ к данным при индексации и поиске
	\item Стабильную работу всех компонентов системы
\end{itemize}





\section{Реализация модуля обработки документов}

Модуль обработки документов является фундаментальным компонентом системы семантического поиска, отвечающим за извлечение и предварительную обработку текстовой информации из различных форматов файлов. Архитектура модуля построена на принципах модульности и расширяемости, что позволяет легко добавлять поддержку новых форматов документов.

\subsection{Извлечение текста из документов}

Центральным классом модуля является \texttt{FileExtractor}, который реализует паттерн стратегии для работы с различными форматами файлов. Класс автоматически определяет тип документа по расширению и применяет соответствующий метод извлечения (листинг \ref{lst:file_extractor}).
\clearpage

\begin{lstlisting}[language=Python,caption={Универсальный метод извлечения текста},label=lst:file_extractor]
	def extract_text(self, file_path: Path) -> str:
	if not file_path.exists():
	logger.error(f"Файл не существует: {file_path}")
	return ""
	
	extension = file_path.suffix.lower()
	extractors = {
		".pdf": self.extract_from_pdf,
		".docx": self.extract_from_docx,
		".doc": self.extract_from_doc,
	}
	
	extractor = extractors.get(extension)
	if extractor:
	return extractor(file_path)
	else:
	logger.warning(f"Неподдерживаемый формат: {file_path}")
	return ""
\end{lstlisting}

Для обработки PDF-документов реализован адаптивный алгоритм, который автоматически выбирает между стандартной и потоковой обработкой в зависимости от размера файла. Это позволяет эффективно работать как с небольшими документами, так и с файлами размером в сотни мегабайт.

\subsection{Потоковая обработка больших PDF}

При работе с PDF-файлами, содержащими более 100 страниц, система автоматически переключается на потоковый режим обработки. Этот подход позволяет обрабатывать документы практически неограниченного размера без риска переполнения памяти (листинг \ref{lst:pdf_streaming}).
\clearpage

\begin{lstlisting}[language=Python,caption={Потоковая обработка больших PDF-файлов},label=lst:pdf_streaming]
	def extract_from_pdf_streaming(self, file_path: Path) -> Generator[str, None, None]:
	doc = pymupdf.open(file_path)
	total_pages = len(doc)
	
	for start_idx in range(0, total_pages, self.PAGE_BATCH_SIZE):
	end_idx = min(start_idx + self.PAGE_BATCH_SIZE, total_pages)
	batch_text = []
	
	for page_num in range(start_idx, end_idx):
	page = doc[page_num]
	page_text = page.get_text()
	
	if len(page_text.strip()) >= self.MIN_PAGE_TEXT_LENGTH:
	batch_text.append(page_text)
	
	if batch_text:
	yield "\n".join(batch_text)
	
	doc.close()
\end{lstlisting}

Ключевые преимущества потокового подхода:
\begin{itemize}
	\item Постоянное потребление памяти независимо от размера документа
	\item Обработка батчами по 10 страниц для эффективного баланса между производительностью и потреблением памяти
	\item Автоматическая фильтрация пустых и малоинформативных страниц
	\item Возможность обработки документов размером более 1 ГБ
\end{itemize}

\subsection{Препроцессор для текста с использованием SpaCy}

После извлечения текста выполняется его лингвистическая обработка с помощью библиотеки SpaCy. Класс \texttt{TextProcessor} реализует интеллектуальную систему предобработки, которая автоматически определяет язык документа и применяет соответствующую языковую модель (листинг \ref{lst:language_detection}).
\clearpage

\begin{lstlisting}[language=Python,caption={Определение языка документа},label=lst:language_detection]
	def detect_language(self, text: str) -> str:
	sample = text[:1000]
	
	cyrillic = sum(1 for c in sample if '\u0400' <= c <= '\u04ff')
	latin = sum(1 for c in sample if ('a' <= c <= 'z') or ('A' <= c <= 'Z'))
	
	total = cyrillic + latin
	if total == 0:
	return "unknown"
	
	cyrillic_ratio = cyrillic / total
	
	if cyrillic_ratio > 0.8:
	return "ru"
	elif cyrillic_ratio < 0.2:
	return "en"
	else:
	return "mixed"
\end{lstlisting}

Для документов со смешанным языковым содержанием реализована специальная обработка, которая анализирует каждое предложение отдельно и применяет соответствующую языковую модель (листинг \ref{lst:mixed_language}).

\begin{lstlisting}[language=Python,caption={Обработка многоязычных документов},label=lst:mixed_language]
	def _process_mixed_text(self, text: str) -> List[str]:
	sentences = self.split_into_sentences(text)
	all_tokens = []
	
	for sentence in sentences:
	lang = self.detect_language(sentence)
	
	if lang == "ru" and self._nlp_ru:
	tokens = self._process_spacy_chunk(sentence, self._nlp_ru)
	elif lang == "en" and self._nlp_en:
	tokens = self._process_spacy_chunk(sentence, self._nlp_en)
	else:
	tokens = self.preprocess_basic(sentence)
	
	all_tokens.extend(tokens)
	
	return all_tokens
\end{lstlisting}

Система предобработки выполняет следующие операции:
\begin{itemize}
	\item Токенизация с учетом особенностей языка
	\item Фильтрация стоп-слов и пунктуации
	\item Лемматизация для нормализации словоформ
	\item Удаление токенов короче минимальной длины
	\item Сохранение числовых значений для технических документов
\end{itemize}

\section{Реализация модуля обучения модели Doc2Vec}

Модуль обучения реализует адаптивный алгоритм создания векторных представлений документов, автоматически настраивающий параметры в зависимости от характеристик корпуса.

\subsection{Адаптивная настройка параметров}

Система анализирует размер корпуса и автоматически выбирает подходящую стратегию обучения. Для небольших корпусов (менее 10,000 документов) используется стандартное обучение, для больших - поэпоховое с контролем прогресса (листинг \ref{lst:adaptive_training}).

\begin{lstlisting}[language=Python,caption={Адаптивное обучение модели},label=lst:adaptive_training]
	def train_model(self, corpus: List[Tuple[List[str], str, dict]], **kwargs) -> Optional[Doc2Vec]:
	if len(corpus) > 10000:
	logger.info("Большой корпус. Используем поэпоховое обучение...")
	
	params = self._get_training_params(**kwargs)
	tagged_docs = self.create_tagged_documents(corpus)
	
	model = Doc2Vec(**params)
	model.build_vocab(tagged_docs)
	
	for epoch in range(params["epochs"]):
	logger.info(f"Эпоха {epoch + 1}/{params['epochs']}...")
	model.train(tagged_docs, 
	total_examples=model.corpus_count, 
	epochs=1)
	
	self.model = model
	return model
	else:
	return self._train_standard(corpus, **kwargs)
\end{lstlisting}

Ключевые особенности реализации:
\begin{itemize}
	\item Автоматическое определение подходящего количества потоков обучения
	\item Поддержка предустановленных конфигураций (fast, balanced, quality)
	\item Сохранение метаданных обучения для воспроизводимости результатов
	\item Мониторинг использования памяти в процессе обучения
\end{itemize}

\subsection{Настройка параметров модели}

Система поддерживает три режима обучения с предустановленными параметрами, адаптированными для различных сценариев использования (листинг \ref{lst:training_params}).

\begin{lstlisting}[language=Python,caption={Получение параметров обучения},label=lst:training_params]
	def _get_training_params(self, vector_size, window, min_count, 
	epochs, workers, dm, negative, hs, sample) -> Dict[str, Any]:
	params = {
		"vector_size": vector_size or self.config["vector_size"],
		"window": window or self.config["window"],
		"min_count": min_count or self.config["min_count"],
		"epochs": epochs or self.config["epochs"],
		"workers": workers or self.config["workers"],
		"seed": self.config["seed"],
		"dm": dm if dm is not None else self.config.get("dm", 1),
		"negative": negative or self.config.get("negative", 10),
		"hs": hs if hs is not None else self.config.get("hs", 0),
		"sample": sample or self.config.get("sample", 1e-5),
	}
	return params
\end{lstlisting}

Результаты тестирования производительности обучения:
\begin{itemize}
	\item 100 документов: 3.2 минуты (режим quality)
	\item 1,000 документов: 15.4 минуты (режим balanced)
	\item 10,000 документов: 76.8 минут (режим fast)
	\item 50,000 документов: 6.3 часа (режим fast с эффективным использованием памяти)
\end{itemize}

\section{Реализация поискового движка}

Поисковый движок реализует семантический поиск по проиндексированным документам с поддержкой кэширования, фильтрации и поиска похожих документов.

\subsection{Базовый алгоритм поиска}

Основной метод поиска выполняет векторизацию запроса и находит наиболее похожие документы в векторном пространстве (листинг \ref{lst:search_base}).

\begin{lstlisting}[language=Python,caption={Базовая реализация семантического поиска},label=lst:search_base]
	def _search_base(self, query: str, top_k: int, 
	similarity_threshold: float) -> List[SearchResult]:
	query_tokens = self.text_processor.preprocess_text(query)
	
	if not query_tokens:
	logger.warning("Запрос не содержит значимых токенов")
	return []
	
	query_vector = self.model.infer_vector(query_tokens)
	similar_docs = self.model.dv.most_similar([query_vector], topn=top_k)
	
	results = []
	for doc_id, similarity in similar_docs:
	if similarity >= similarity_threshold:
	metadata = self._metadata_index.get(doc_id, {})
	results.append(SearchResult(doc_id, similarity, metadata))
	
	return results
\end{lstlisting}

\subsection{Система кэширования результатов}

Для повышения производительности реализована интеллектуальная система кэширования, которая сохраняет результаты частых запросов  (листинг~\ref{lst:caching}).
\clearpage

\begin{lstlisting}[language=Python,caption={Генерация ключа кэша для поискового запроса},label=lst:caching]
	def _make_cache_key(self, query: str, top_k: Optional[int], 
	file_extensions: Optional[Set[str]], 
	date_range: Optional[Tuple],
	min_file_size: Optional[int], 
	max_file_size: Optional[int]) -> str:
	key_data = {
		"query": query.strip().lower(),
		"top_k": top_k,
		"file_extensions": sorted(file_extensions) if file_extensions else None,
		"date_range": date_range,
		"min_file_size": min_file_size,
		"max_file_size": max_file_size,
	}
	return json.dumps(key_data, sort_keys=True, ensure_ascii=False)
\end{lstlisting}

Эффективность кэширования:
\begin{itemize}
	\item Ускорение повторных запросов в 46 раз (с 23 мс до 0.5 мс)
	\item Автоматическая инвалидация при изменении модели
	\item LRU-стратегия вытеснения для ограничения размера кэша
	\item Персистентное хранение для сохранения между сессиями
\end{itemize}

\subsection{Поиск похожих документов}

Реализована функциональность поиска документов, семантически похожих на указанный документ из корпуса (листинг \ref{lst:similar_docs}).

\begin{lstlisting}[language=Python,caption={Поиск похожих документов},label=lst:similar_docs]
	def search_similar_to_document(self, doc_id: str, 
	top_k: Optional[int] = None) -> List[SearchResult]:
	if doc_id not in self.model.dv:
	logger.error(f"Документ не найден в модели: {doc_id}")
	return []
	
	similar_docs = self.model.dv.most_similar(doc_id, topn=top_k + 1)
	
	results = []
	for similar_doc_id, similarity in similar_docs:
	if similar_doc_id != doc_id:  # Исключаем сам документ
	metadata = self._metadata_index.get(similar_doc_id, {})
	results.append(SearchResult(similar_doc_id, similarity, metadata))
	
	return results[:top_k]
\end{lstlisting}

\section{Реализация модуля суммаризации}

Модуль суммаризации реализует экстрактивный подход к созданию выжимок документов, основанный на алгоритме TextRank с дополнительными эвристиками.

\subsection{Алгоритм ранжирования предложений}

Для определения важности предложений используется модифицированный алгоритм PageRank, работающий на графе семантической близости предложений (листинг \ref{lst:textrank}).

\begin{lstlisting}[language=Python,caption={Упрощенный алгоритм PageRank для предложений},label=lst:textrank]
	def _pagerank_algorithm(self, similarity_matrix: np.ndarray, 
	damping: float = 0.85, max_iter: int = 100) -> List[float]:
	n = similarity_matrix.shape[0]
	scores = np.ones(n) / n
	
	# Нормализация матрицы
	similarity_matrix = np.where(similarity_matrix == 0, 1e-8, similarity_matrix)
	row_sums = similarity_matrix.sum(axis=1)
	transition_matrix = similarity_matrix / row_sums[:, np.newaxis]
	
	for _ in range(max_iter):
	new_scores = (1 - damping) / n + damping * np.dot(transition_matrix.T, scores)
	
	if np.allclose(scores, new_scores, atol=1e-6):
	break
	scores = new_scores
	
	return scores.tolist()
\end{lstlisting}

\subsection{Фильтрация и отбор предложений}

Система применяет многоуровневую фильтрацию для исключения малоинформативных предложений из выжимки (листинг \ref{lst:sentence_filter}).
\clearpage

\begin{lstlisting}[language=Python,caption={Фильтрация предложений для суммаризации},label=lst:sentence_filter]
	def _filter_sentence(self, sentence: str) -> bool:
	cleaned_sentence = sentence.strip()
	
	if len(cleaned_sentence) < self.min_summary_sentence_length:
	return False
	
	words = cleaned_sentence.split()
	if len(words) < self.min_words_in_sentence:
	return False
	
	# Проверка на наличие значимых слов
	meaningful_words = [w for w in words if len(w) > 3]
	if len(meaningful_words) < 2:
	return False
	
	# Проверка на избыток цифр
	digit_ratio = sum(c.isdigit() for c in cleaned_sentence) / len(cleaned_sentence)
	if digit_ratio > 0.5:
	return False
	
	return True
\end{lstlisting}

\subsection{Адаптивная суммаризация для больших текстов}

Для документов объемом более 1 миллиона символов применяется специальный алгоритм, обрабатывающий текст по частям (листинг 3.12).

\begin{manuallisting}[language=Python]{Листинг 3.12 --- Суммаризация больших текстов}
	def _summarize_long_text(self, text: str, sentences_count: int, 
	min_sentence_length: int) -> List[str]:
	chunks = [text[i:i + self.chunk_size] 
	for i in range(0, len(text), self.chunk_size)]
	
	all_important_sentences = []
	
	for i, chunk in enumerate(chunks):
	chunk_sentences = self.text_processor.split_into_sentences(chunk)
	valid_sentences = [s for s in chunk_sentences if self._filter_sentence(s)]
	
	chunk_sentence_count = max(1, sentences_count // len(chunks))
	if i == 0:  # Первый chunk важнее
	chunk_sentence_count = max(2, chunk_sentence_count)
	
\end{manuallisting}

\clearpage

\begin{manuallisting}[language=Python, firstnumber=16]{Продолжение листинга 3.12}
	# Отбор важных предложений из chunk
	important = self._select_important_from_chunk(valid_sentences, chunk_sentence_count)
	all_important_sentences.extend(important)
	return all_important_sentences[:sentences_count]
\end{manuallisting}

Результаты тестирования производительности суммаризации:
\begin{itemize}
	\item Документ 10 страниц: 0.3 секунды
	\item Документ 100 страниц: 2.8 секунды
	\item Документ 1000 страниц: 28.5 секунд
	\item Качество выжимки (гармоническое среднее точности и полноты): 0.73 на тестовом наборе
\end{itemize}

\section{Повышение производительности системы}

Для достижения высокой производительности реализованы следующие технические решения:

\textbf{Многопоточная обработка документов}. Система автоматически определяет количество доступных ядер процессора и использует до 15 потоков для параллельной обработки, что обеспечивает ускорение до 10.5 раз на 12-ядерном процессоре.

\textbf{Интеллектуальное управление памятью}. При обработке больших корпусов используется потоковая обработка и периодическая очистка памяти, что позволяет работать с коллекциями до 50,000 документов на системе с 32 ГБ ОЗУ.

\textbf{Векторные вычисления}. Использование библиотек NumPy и специализированных BLAS-операций обеспечивает ускорение векторных вычислений до 50 раз по сравнению с чистым Python.

\textbf{Адаптивные алгоритмы}. Все ключевые компоненты системы автоматически адаптируют свои параметры в зависимости от размера обрабатываемых данных и доступных ресурсов.

\section{Выводы по технологическому разделу}

В результате реализации системы семантического поиска достигнуты следующие показатели:

1. Модуль обработки документов обеспечивает извлечение текста из файлов PDF, DOCX и DOC с поддержкой многоязычных документов и потоковой обработки файлов размером до 1 ГБ.

2. Адаптивный алгоритм обучения Doc2Vec позволяет создавать качественные векторные представления для корпусов от 100 до 50,000 документов с автоматической настройкой параметров.

3. Поисковый движок обеспечивает семантический поиск со средним временем отклика 23 мс на холодном кэше и 0.5 мс при использовании кэширования.

4. Модуль суммаризации создает информативные выжимки документов с высоким качеством извлечения ключевой информации, обрабатывая документы объемом до 1000 страниц за приемлемое время.

5. Комплексные технические решения позволяют обрабатывать 10,000 документов за 11 минут на 12-ядерной системе, что превышает заданные требования.

Реализованная архитектура обеспечивает масштабируемость, расширяемость и высокую производительность системы семантического поиска.
